
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>针对移动设备进行优化 | 智子</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="zhizi">
    

    
    <meta name="description" content="针对移动设备进行优化有一些特殊的问题需要在你尝试时处理 在手机或嵌入式设备上发货，你需要考虑这些 你正在开发你的模型。 这些问题是： 模型和二进制大小 应用速度和模型加载速度 性能和线程 我们将在下面讨论其中的一些。 TensorFlow的最低设备要求是什么？您至少需要一兆字节的程序存储器和几兆字节的RAM 运行基本的TensorFlow运行时，所以它不适合DSP或者 微控制器。除此之外，最大的限">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="针对移动设备进行优化">
<meta property="og:url" content="https://www.tracholar.top/2018/01/01/optimizing/index.html">
<meta property="og:site_name" content="智子">
<meta property="og:description" content="针对移动设备进行优化有一些特殊的问题需要在你尝试时处理 在手机或嵌入式设备上发货，你需要考虑这些 你正在开发你的模型。 这些问题是： 模型和二进制大小 应用速度和模型加载速度 性能和线程 我们将在下面讨论其中的一些。 TensorFlow的最低设备要求是什么？您至少需要一兆字节的程序存储器和几兆字节的RAM 运行基本的TensorFlow运行时，所以它不适合DSP或者 微控制器。除此之外，最大的限">
<meta property="og:updated_time" content="2018-01-16T14:34:50.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="针对移动设备进行优化">
<meta name="twitter:description" content="针对移动设备进行优化有一些特殊的问题需要在你尝试时处理 在手机或嵌入式设备上发货，你需要考虑这些 你正在开发你的模型。 这些问题是： 模型和二进制大小 应用速度和模型加载速度 性能和线程 我们将在下面讨论其中的一些。 TensorFlow的最低设备要求是什么？您至少需要一兆字节的程序存储器和几兆字节的RAM 运行基本的TensorFlow运行时，所以它不适合DSP或者 微控制器。除此之外，最大的限">

    
    <link rel="alternative" href="/atom.xml" title="智子" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">

    <!-- ad start -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6300557868920774",
    enable_page_level_ads: true
  });
</script>

    <!-- ad end -->

    <!--  stat -->
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4036f580b1119e720db871571faa68cc";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-78529611-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-78529611-1');
</script>

    <!-- end stat -->
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="智子">智子</a></h1>
				<h2 class="blog-motto">智子之家</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:www.tracholar.top">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody">
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/optimizing/" title="针对移动设备进行优化" itemprop="url">针对移动设备进行优化</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">针对移动设备进行优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.1.</span> <span class="toc-text">TensorFlow的最低设备要求是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.</span> <span class="toc-text">速度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.3.</span> <span class="toc-text">模型大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.4.</span> <span class="toc-text">二进制大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.5.</span> <span class="toc-text">如何配置你的模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.6.</span> <span class="toc-text">在自己的应用程序中进行分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.7.</span> <span class="toc-text">可视化模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.8.</span> <span class="toc-text">穿线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.9.</span> <span class="toc-text">用移动数据重新调整</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.10.</span> <span class="toc-text">减少模型加载时间和/或内存占用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.11.</span> <span class="toc-text">保护模型文件免于复制</span></a></li></ol></li></ol>
		
		</div>
		

        <ins class="adsbygoogle"
     style="display:block; text-align:center; overflow:hidden;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


		<h1><span id="针对移动设备进行优化">针对移动设备进行优化</span></h1><p>有一些特殊的问题需要在你尝试时处理 在手机或嵌入式设备上发货，你需要考虑这些 你正在开发你的模型。</p>
<p>这些问题是：</p>
<p>模型和二进制大小 应用速度和模型加载速度 性能和线程</p>
<p>我们将在下面讨论其中的一些。</p>
<h2><span id="tensorflow的最低设备要求是什么">TensorFlow的最低设备要求是什么？</span></h2><p>您至少需要一兆字节的程序存储器和几兆字节的RAM 运行基本的TensorFlow运行时，所以它不适合DSP或者 微控制器。除此之外，最大的限制通常是<br>设备的计算速度，以及是否可以运行所需的模型 您的应用程序的延迟时间足够短。您可以使用基准测试工具 在如何剖析你的模型，以了解如何<br>模型需要许多FLOP，然后使用它来制定经验法则 估计他们在不同设备上运行的速度。例如，一个现代的 智能手机可能会运行10<br>GFLOPs每秒，所以最好的，你可以希望 从5 GFLOP模型是每秒两帧，但你可能会做得更差 取决于确切的计算模式是什么。</p>
<p>这种依赖模式意味着甚至可以运行TensorFlow 旧的或受限的手机，只要你优化你的网络，以适应内 延迟预算，也可能在有限的RAM内。对于内存使用，你<br>大多需要确定TensorFlow创建的中间缓冲区 不是太大，你也可以在基准输出中查看。</p>
<h2><span id="速度">速度</span></h2><p>大多数模型部署的最高优先级之一是弄清楚如何 运行推理足够快，以提供良好的用户体验。第一个地方 开始是通过查看浮点操作的总数<br>执行该图所需的。你可以通过一个非常粗略的估计 使用<code>benchmark_model</code>工具：</p>
<pre><code>bazel build -c opt tensorflow/tools/benchmark:benchmark_model &amp;&amp; \
bazel-bin/tensorflow/tools/benchmark/benchmark_model \
--graph=/tmp/inception_graph.pb --input_layer=&quot;Mul:0&quot; \
--input_layer_shape=&quot;1,299,299,3&quot; --input_layer_type=&quot;float&quot; \
--output_layer=&quot;softmax:0&quot; --show_run_order=false --show_time=false \
--show_memory=false --show_summary=true --show_flops=true --logtostderr
</code></pre><p>这应该显示您需要运行多少个操作的估计 图形。然后，您可以使用该信息来确定模型的可行性 将在您定位的设备上运行。举个例子，来自高端手机<br>2016年可能能够做到200亿FLOPs每秒，所以最好的速度 可能希望从一个需要100亿FLOPs的模型是500毫秒左右。在一个 像Raspberry Pi<br>3那样可以做大约50亿FLOP的设备，你可能只有 每两秒得一个推理。</p>
<p>有了这个估计值可以帮助你计划你将能够实际的 在设备上实现。如果模型使用太多的操作，那么有很多 优化体系结构以减少数量的机会。</p>
<p>先进的技术包括SqueezeNet 和MobileNet，这是架构 旨在生产移动模型 - 精益和快速，但具有一个小的准确性<br>成本。你也可以看看替代模型，甚至更旧的，可能 更小。例如，Inception v1只有大约700万个参数， 与Inception<br>v3的2400万相比，只需要30亿FLOP v3超过90亿。</p>
<h2><span id="模型大小">模型大小</span></h2><p>在设备上运行的模型需要存储在设备上的某个地方 大的神经网络可能是几百兆字节。大多数用户不愿意 从应用程序商店下载非常大的应用程序包，所以你想让你的模型<br>尽可能小。此外，更小的神经网络可以坚持和 移动设备的内存更快。</p>
<p>要了解您的网络将在磁盘上有多大，请先看看 在运行<code>GraphDef</code>后<code>freeze_graph</code>文件的磁盘大小和<br><code>strip_unused_nodes</code>（请参阅准备型号 关于这些工具的更多细节），因为它应该只包含 推理相关节点。要仔细检查您的结果是否符合预期，请运行<br><code>summarize_graph</code>工具查看常量中有多少个参数：</p>
<pre><code>bazel build tensorflow/tools/graph_transforms:summarize_graph &amp;&amp; \
bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \
--in_graph=/tmp/tensorflow_inception_graph.pb
</code></pre><p>该命令应该给你看起来像这样的输出：</p>
<pre><code>No inputs spotted.
Found 1 possible outputs: (name=softmax, op=Softmax)
Found 23885411 (23.89M) const parameters, 0 (0) variable parameters,
and 99 control_edges
Op types used: 489 Const, 99 CheckNumerics, 99 Identity, 94
BatchNormWithGlobalNormalization, 94 Conv2D, 94 Relu, 11 Concat, 9 AvgPool,
5 MaxPool, 1 Sub, 1 Softmax, 1 ResizeBilinear, 1 Reshape, 1 Mul, 1 MatMul,
1 ExpandDims, 1 DecodeJpeg, 1 Cast, 1 BiasAdd
</code></pre><p>我们目前的目的的重要部分是const数 参数。在大多数模型中，这些将被存储为32位浮点数，所以如果 你把const参数的数量乘以四，你应该得到一些东西<br>这接近于磁盘上文件的大小。你只能经常逃脱 每个参数8比特，最终结果的精度损失很小， 所以如果你的文件太大，你可以尝试使用<br>quantize_weights将参数向下变换。</p>
<pre><code>bazel build tensorflow/tools/graph_transforms:transform_graph &amp;&amp; \
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=/tmp/tensorflow_inception_optimized.pb \
--out_graph=/tmp/tensorflow_inception_quantized.pb \
--inputs=&apos;Mul:0&apos; --outputs=&apos;softmax:0&apos; --transforms=&apos;quantize_weights&apos;
</code></pre><p>如果你看看所得到的文件大小，你应该看到它大约是四分之一 的原始在23MB。</p>
<p>另一个转换是<code>round_weights</code>，它不会使文件变小，但它 使文件压缩到与<code>quantize_weights</code>相同的尺寸<br>用过的。这对于移动开发来说特别有用，利用了 应用程序包在被消费者下载之前被压缩。</p>
<p>原始文件不能用标准算法压缩，因为 甚至非常相似的数字的位模式可能会非常不同。该 <code>round_weights</code>变换保持重量参数存储为浮动，但<br>将它们四舍五入成一定数量的步进值。这意味着还有更多 在存储的模型中重复字节模式，所以经常会带来压缩 尺寸急剧下降，在许多情况下接近它们的尺寸 被存储为八位。</p>
<p><code>round_weights</code>的另一个优点是框架不必 分配一个临时缓冲区来解压参数，就像我们到的时候一样<br>我们只使用<code>quantize_weights</code>。这节省了一点点的延迟（虽然 结果应该被缓存，所以它只是在第一次运行成本高昂），并使其成为可能<br>如后面所述，可以使用内存映射。</p>
<h2><span id="二进制大小">二进制大小</span></h2><p>移动和服务器开发最大的区别之一是 二进制大小的重要性。在台式机上，并不罕见 数百兆字节的磁盘上的可执行文件，但移动和嵌入式<br>应用程序是至关重要的，保持二进制尽可能小，以便用户下载 很容易。如上所述，TensorFlow只包含op的一个子集<br>默认情况下是实现，但是这仍然导致12MB的最终结果 可执行文件。为了减少这一点，你可以设置库只包括 实际需要的操作的实现，基于自动 分析你的模型。要使用它：</p>
<p>在您的<code>tools/print_required_ops/print_selective_registration_header.py</code>上运行<br>模型生成一个头文件，只启用它使用的操作。 把<code>ops_to_register.h</code>文件放在编译器可以找到的地方<br>它。这可以在您的TensorFlow源文件夹的根目录下。 使用定义的<code>SELECTIVE_REGISTRATION</code>构建TensorFlow，例如通过<br>在<code>--copts=&quot;-DSELECTIVE_REGISTRATION&quot;</code>到您的Bazel生成命令。</p>
<p>这个过程重新编译库，以便只有需要的操作和类型 包括，这可以大大减少可执行文件的大小。例如，用 入侵v3，新的大小只有1.5MB。</p>
<h2><span id="如何配置你的模型">如何配置你的模型</span></h2><p>一旦您了解了您的设备的最佳性能范围，那就是 值得关注其当前的实际表现。使用独立的TensorFlow 基准，而不是在一个更大的应用程序内运行，有助于隔离只是<br>Tensorflow对贡献的贡献 潜伏。该 张量流量/工具/基准工具 旨在帮助你做到这一点。在桌面上的Inception v3上运行它<br>机器，建立这个基准模型：</p>
<pre><code>bazel build -c opt tensorflow/tools/benchmark:benchmark_model &amp;&amp; \
bazel-bin/tensorflow/tools/benchmark/benchmark_model \
--graph=/tmp/tensorflow_inception_graph.pb --input_layer=&quot;Mul&quot; \
--input_layer_shape=&quot;1,299,299,3&quot; --input_layer_type=&quot;float&quot; \
--output_layer=&quot;softmax:0&quot; --show_run_order=false --show_time=false \
--show_memory=false --show_summary=true --show_flops=true --logtostderr
</code></pre><p>你应该看到如下所示的输出：</p>
<pre><code>============================== Top by Computation Time ==============================
[node
 type]  [start]  [first] [avg ms]     [%]  [cdf%]  [mem KB]  [Name]
Conv2D   22.859   14.212   13.700  4.972%  4.972%  3871.488  conv_4/Conv2D
Conv2D    8.116    8.964   11.315  4.106%  9.078%  5531.904  conv_2/Conv2D
Conv2D   62.066   16.504    7.274  2.640% 11.717%   443.904  mixed_3/conv/Conv2D
Conv2D    2.530    6.226    4.939  1.792% 13.510%  2765.952  conv_1/Conv2D
Conv2D   55.585    4.605    4.665  1.693% 15.203%   313.600  mixed_2/tower/conv_1/Conv2D
Conv2D  127.114    5.469    4.630  1.680% 16.883%    81.920  mixed_10/conv/Conv2D
Conv2D   47.391    6.994    4.588  1.665% 18.548%   313.600  mixed_1/tower/conv_1/Conv2D
Conv2D   39.463    7.878    4.336  1.574% 20.122%   313.600  mixed/tower/conv_1/Conv2D
Conv2D  127.113    4.192    3.894  1.413% 21.535%   114.688  mixed_10/tower_1/conv/Conv2D
Conv2D   70.188    5.205    3.626  1.316% 22.850%   221.952  mixed_4/conv/Conv2D

============================== Summary by node type ==============================
[Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB]
Conv2D            94   244.899    88.952%    88.952% 35869.953
BiasAdd           95     9.664     3.510%    92.462% 35873.984
AvgPool            9     7.990     2.902%    95.364%  7493.504
Relu              94     5.727     2.080%    97.444% 35869.953
MaxPool            5     3.485     1.266%    98.710%  3358.848
Const            192     1.727     0.627%    99.337%     0.000
Concat            11     1.081     0.393%    99.730%  9892.096
MatMul             1     0.665     0.242%    99.971%     4.032
Softmax            1     0.040     0.015%    99.986%     4.032
&lt;&gt;                 1     0.032     0.012%    99.997%     0.000
Reshape            1     0.007     0.003%   100.000%     0.000

Timings (microseconds): count=50 first=330849 curr=274803 min=232354 max=415352 avg=275563 std=44193
Memory (bytes): count=50 curr=128366400(all same)
514 nodes defined 504 nodes observed
</code></pre><p>这是由show_summary标志启用的摘要视图。至 解释它，第一个表格是花费最多时间的节点的列表， 订购他们花了多长时间。从左到右，列是：</p>
<p>节点类型，这是什么样的操作。 操作的开始时间，显示操作顺序的落点。 第一次以毫秒为单位。这是第一次手术的时间<br>基准的运行，因为默认情况下执行20次运行得到更多   可靠的统计。第一次是有用的发现哪些操作正在做   昂贵的计算在第一次运行，然后缓存结果。<br>所有运行的平均操作时间，以毫秒为单位。 一次运行所花费的总时间的百分比。这是有用的   了解热点在哪里。 本表和前面的操作的累积总时间。这是<br>方便了解各层面的工作分配情况   看看是否只有几个节点占用大部分时间。 节点的名称。</p>
<p>第二个表是类似的，但不是按时间分解 特定的命名节点，它将它们按操作类型分组。这是非常有用的 了解您可能想要优化或消除哪些操作<br>你的图。这张桌子在开始的时候是安排成本最高的， 并且只显示前十个条目，并且具有其他节点的占位符。该 从左到右的列是：</p>
<p>正在分析的节点的类型。 此类型的所有节点所花费的平均时间（以毫秒为单位）。 这种类型的操作占总时间的百分比。 这个和op类型在表中的累积时间较高，所以你可以<br>了解工作量的分布。 这个op类型的输出占用了多少内存。</p>
<p>这两个表都设置好，以便您可以轻松地复制和粘贴它们 结果导入电子表格文档，因为它们与标签一起输出 列之间的分隔符。节点类型的摘要可能是最有用的<br>当寻找优化机会时，因为它是一个指向代码的指针 这是花费最多的时间。在这种情况下，您可以看到Conv2D操作符是<br>几乎90％的执行时间。这是一个图表很漂亮的标志 最佳的，因为卷积和矩阵乘法预计是大部分 一个神经网络的计算工作量。</p>
<p>作为一个经验法则，如果你看到很多其他的操作，则更加令人担忧 占用了一小部分时间。对于神经网络，操作 不涉及大矩阵乘法的情况通常应该被相对矮化<br>那些做，所以如果你看到很多时间进入这些，这是一个迹象 要么你的网络是非最优构造的，要么是实现这些的代码 ops没有尽可能优化 是。性能错误或<br>如果遇到这种情况，补丁总是受欢迎的，特别是如果 它们包括一个展示此行为和命令行的附加模型 用来运行基准测试工具。</p>
<p>上面的运行是在您的桌面上，但该工具也适用于Android，这是 它对移动开发最有用。这里是一个示例命令行 在64位ARM设备上运行它：</p>
<pre><code>bazel build -c opt --config=android_arm64 \ 
tensorflow/tools/benchmark:benchmark_model
adb push bazel-bin/tensorflow/tools/benchmark/benchmark_model /data/local/tmp
adb push /tmp/tensorflow_inception_graph.pb /data/local/tmp/
adb shell &apos;/data/local/tmp/benchmark_model \
--graph=/data/local/tmp/tensorflow_inception_graph.pb --input_layer=&quot;Mul&quot; \
--input_layer_shape=&quot;1,299,299,3&quot; --input_layer_type=&quot;float&quot; \
--output_layer=&quot;softmax:0&quot; --show_run_order=false --show_time=false \
--show_memory=false --show_summary=true&apos;
</code></pre><p>你可以用与桌面版本完全相同的方式来解释结果 以上。如果你有什么困难搞清楚什么是正确的输入和输出 名字和类型是，看看准备<br>模型页面的细节检测这些为您的模型，并看看 <code>summarize_graph</code>工具可能会给你 有用的信息。</p>
<p>对于iOS上的命令行工具没有很好的支持，所以没有 单独的例子 在 tensorflow / examples / ios / benchmark<br>在独立的应用程序中打包相同的功能。这输出 统计到设备的屏幕和调试日志。如果你想 Android示例应用程序的屏幕统计信息，您可以打开它们 按下音量增加按钮。</p>
<h2><span id="在自己的应用程序中进行分析">在自己的应用程序中进行分析</span></h2><p>您从基准测试工具看到的输出是从模块生成的 作为标准TensorFlow运行时的一部分，这意味着您有权访问<br>在你自己的应用程序中也是如此。你可以看到一个如何做的例子 在这里。</p>
<p>基本步骤是：</p>
<p>创建一个StatSummarizer对象： tensorflow :: StatSummarizer<br>stat_summarizer（tensorflow_graph）; 设置选项： tensorflow :: RunOptions run_options;<br>run_options.set_trace_level（tensorflow :: RunOptions :: FULL_TRACE）;<br>tensorflow :: RunMetadata run_metadata; 运行图： run_status = session-&gt;<br>Run（run_options，inputs，output_layer_names，{}，<br>output_layers，＆run_metadata）; 计算结果并打印出来： 断言（run_metadata.has_step_stats（））;<br>const tensorflow :: StepStats＆step_stats = run_metadata.step_stats（）;<br>stat_summarizer-&gt; ProcessStepStats（step_stats）; stat_summarizer-&gt;<br>PrintStepStats（）;</p>
<h2><span id="可视化模型">可视化模型</span></h2><p>加速你的代码最有效的方法就是改变你的模型 做更少的工作。要做到这一点，你需要了解你的模型在做什么，以及 想象这是一个好的第一步。要高度概括您的图表，<br>使用TensorBoard。</p>
<h2><span id="穿线">穿线</span></h2><p>TensorFlow的桌面版本有一个复杂的线程模型，并将 如果可以的话，尝试并行运行多个操作。在我们的术语中是这样的<br>所谓的“内部操作并行”（尽管为了避免与“内部操作”混淆，你 可以把它看作是“之间的”），并可以通过指定来设置<br><code>inter_op_parallelism_threads</code>中的会话选项。</p>
<p>默认情况下，移动设备连续运行操作;那是， <code>inter_op_parallelism_threads</code>设置为1.移动处理器通常很少<br>核心和一个小缓存，所以运行多个访问不相交部分的操作 内存通常不利于性能。 “内部操作并行”（或 “内部操作”）可以是非常有用的，特别是对于计算限制<br>像不同的线程可以馈送同样小的卷积操作 内存集。</p>
<p>在移动设备上，一个操作系统将使用多少个线程被设置为核心数量 默认值，或者当核心数量不能确定时为2。你可以覆盖 默认的ops正在使用的线程数量<br><code>intra_op_parallelism_threads</code>中的会话选项。这是一个好主意 如果你的应用程序有自己的线程做重处理，那么减少默认值<br>他们不互相干扰。</p>
<p>要查看会话选项的更多细节，请查看ConfigProto。</p>
<h2><span id="用移动数据重新调整">用移动数据重新调整</span></h2><p>在移动应用上运行模型时，精度问题的最大原因是 非代表性的训练数据。例如，大多数Imagenet照片都是 精心设计，使对象在图片的中心，光线充足，<br>用普通镜头拍摄。来自移动设备的照片通常很差， 灯光不好，可能会有鱼眼变形，特别是自拍。</p>
<p>解决方案是用实际捕获的数据扩展您的训练集 你的申请。这一步可能需要额外的工作，因为你必须标注 你自己的例子，但即使你只是用它来扩大你的原创<br>训练数据，可以大大帮助训练。改善培训 设置这样做，并通过修复其他质量问题，如重复或严重 标记示例是提高准确性的最佳方法。通常是一个<br>更大的帮助比改变你的模型架构或使用不同的技术。</p>
<h2><span id="减少模型加载时间和或内存占用">减少模型加载时间和/或内存占用</span></h2><p>大多数操作系统允许您使用内存映射加载文件 比通过通常的I / O API。而不是分配一个内存区域 在堆上，然后从磁盘复制字节到它，你只需告诉<br>操作系统使文件的全部内容直接出现在 记忆。这有几个好处：</p>
<p>加载速度 减少分页（提高性能） 不计入您的应用程序的RAM预算</p>
<p>TensorFlow支持内存映射构成大部分的权重 模型文件。由于<code>ProtoBuf</code>系列化格式的限制，我们 必须对我们的模型加载和处理代码进行一些更改。该<br>双向内存映射的工作原理是我们有一个单独的文件，其中第一部分是 正常<code>GraphDef</code>串行化成协议缓冲线的格式，不过接下来就是了<br>权重以可以直接映射的形式附加。</p>
<p>要创建这个文件，运行 <code>tensorflow/contrib/util:convert_graphdef_memmapped_format</code>工具。这需要<br>通过<code>GraphDef</code>运行的<code>freeze_graph</code>文件并将其转换为 格式，在最后附加权重。由于该文件不再是一个 标准<code>GraphDef</code><br>protobuf，则需要对加载进行一些更改 码。你可以看到这个例子 该 iOS相机演示程序， 在<code>LoadMemoryMappedModel()</code>功能中。</p>
<p>相同的代码（用Objective C调用替换文件名） 也可以在其他平台上使用。因为我们正在使用内存映射，所以我们需要<br>首先创建一个特殊的TensorFlow环境对象 我们将使用的文件：</p>
<pre><code>std::unique_ptr&lt;tensorflow::MemmappedEnv&gt; memmapped_env;
memmapped_env-&gt;reset(
      new tensorflow::MemmappedEnv(tensorflow::Env::Default()));
tensorflow::Status mmap_status =
      (memmapped_env-&gt;get())-&gt;InitializeFromFile(file_path);
</code></pre><p>然后，您需要将此环境传递给随后的调用，如下所示 加载图表：</p>
<pre><code>tensorflow::GraphDef tensorflow_graph;
tensorflow::Status load_graph_status = ReadBinaryProto(
    memmapped_env-&gt;get(),
    tensorflow::MemmappedFileSystem::kMemmappedPackageDefaultGraphDef,
    &amp;tensorflow_graph);
</code></pre><p>您还需要使用指向您所在环境的指针来创建会话 创建：</p>
<pre><code>tensorflow::SessionOptions options;
options.config.mutable_graph_options()
    -&gt;mutable_optimizer_options()
    -&gt;set_opt_level(::tensorflow::OptimizerOptions::L0);
options.env = memmapped_env-&gt;get();

tensorflow::Session* session_pointer = nullptr;
tensorflow::Status session_status =
    tensorflow::NewSession(options, &amp;session_pointer);
</code></pre><p>有一点要注意的是，我们也禁用自动优化， 因为在某些情况下，这些将折叠不变的子树，并创建副本 张量值，我们不想要和用尽更多的RAM。</p>
<p>一旦你完成了这些步骤，你可以使用会话和图表 正常，你会看到加载时间和内存使用量的减少。</p>
<h2><span id="保护模型文件免于复制">保护模型文件免于复制</span></h2><p>默认情况下，你的模型将被存储在标准的序列化protobuf中 磁盘格式。理论上这意味着任何人都可以复制你的模型，你自己<br>可能不想要。但实际上，大多数模型都是特定于应用程序的 被优化所迷惑，风险与竞争对手相似 反汇编和重复使用你的代码，但是如果你确实想让它变得更加困难的话<br>临时用户访问您的文件是可以采取一些基本的步骤。</p>
<p>我们大多数的例子都使用 该 ReadBinaryProto（）方便 调用从磁盘加载<code>GraphDef</code>。这确实需要一个未加密的protobuf<br>磁盘。幸运的是，调用的实现非常简单 编写一个可以在内存中解密的等价物应该很容易。这里的 一些代码展示了如何使用你自己的方式读取和解密protobuf<br>解密程序：</p>
<pre><code>Status ReadEncryptedProto(Env* env, const string&amp; fname,
                          ::tensorflow::protobuf::MessageLite* proto) {
  string data;
  TF_RETURN_IF_ERROR(ReadFileToString(env, fname, &amp;data));

  DecryptData(&amp;data);  // Your own function here.

  if (!proto-&gt;ParseFromString(&amp;data)) {
    TF_RETURN_IF_ERROR(stream-&gt;status());
    return errors::DataLoss(&quot;Can&apos;t parse &quot;, fname, &quot; as binary proto&quot;);
  }
  return Status::OK();
}
</code></pre><p>要使用这个，你需要自己定义DecryptData（）函数。它可以 就像这样简单：</p>
<pre><code>void DecryptData(string* data) {
  for (int i = 0; i &lt; data.size(); ++i) {
    data[i] = data[i] ^ 0x23;
  }
}
</code></pre><p>你可能想要一些更复杂的东西，但是你需要的东西就在外面 当前范围在这里。</p>


        <p style="margin-top:2em; text-align:left; font-weight:bold; font-style: italic;">未经作者同意，本文严禁转载，违者必究！</p>
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://www.tracholar.top/2018/01/01/optimizing/" data-title="针对移动设备进行优化 | 智子" data-tsina="" class="share clearfix">
	  </div>
	
	</div>


</footer>


	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/01/01/prepare_models/" title="为移动部署准备模型">
  <strong>上一篇：</strong><br/>
  <span>
  为移动部署准备模型</span>
</a>
</div>


<div class="next">
<a href="/2018/01/01/linear/"  title="具有张量流的大规模线性模型">
 <strong>下一篇：</strong><br/> 
 <span>具有张量流的大规模线性模型
</span>
</a>
</div>

</nav>

	



</div>

      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">针对移动设备进行优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.1.</span> <span class="toc-text">TensorFlow的最低设备要求是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.</span> <span class="toc-text">速度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.3.</span> <span class="toc-text">模型大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.4.</span> <span class="toc-text">二进制大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.5.</span> <span class="toc-text">如何配置你的模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.6.</span> <span class="toc-text">在自己的应用程序中进行分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.7.</span> <span class="toc-text">可视化模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.8.</span> <span class="toc-text">穿线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.9.</span> <span class="toc-text">用移动数据重新调整</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.10.</span> <span class="toc-text">减少模型加载时间和/或内存占用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.11.</span> <span class="toc-text">保护模型文件免于复制</span></a></li></ol></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- side-bar-ad -->
<ins class="adsbygoogle"
     style="display:block; overflow:hidden;"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="2232545787"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


  


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/javascript/" title="javascript">javascript<sup>207</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>205</sup></a></li>
			
		
			
				<li><a href="/tags/html/" title="html">html<sup>203</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>199</sup></a></li>
			
		
			
				<li><a href="/tags/bash/" title="bash">bash<sup>198</sup></a></li>
			
		
			
				<li><a href="/tags/php/" title="php">php<sup>197</sup></a></li>
			
		
			
				<li><a href="/tags/css/" title="css">css<sup>88</sup></a></li>
			
		
			
				<li><a href="/tags/shell/" title="shell">shell<sup>78</sup></a></li>
			
		
			
				<li><a href="/tags/jquery/" title="jquery">jquery<sup>61</sup></a></li>
			
		
			
				<li><a href="/tags/linux/" title="linux">linux<sup>57</sup></a></li>
			
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>41</sup></a></li>
			
		
			
				<li><a href="/tags/unix/" title="unix">unix<sup>30</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/html5/" title="html5">html5<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/xml/" title="xml">xml<sup>13</sup></a></li>
			
		
			
				<li><a href="/tags/http/" title="http">http<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/区块链/" title="区块链">区块链<sup>1</sup></a></li>
			
		
			
		
			
		
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="https://tracholar.github.io" target="_blank" title="个人博客">个人博客</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>

    </div>
    <footer><div id="footer" >
	
	
	<section class="info">
		<p> To be or not to be, that is a question. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		版权所有 © 2018 本站文章未经同意，禁止转载！作者：
		
		<a href="/about" target="_blank" title="zhizi">zhizi</a>
		


		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>











<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
