
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>神经机器翻译（seq2seq）教程 | 智子</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="zhizi">
    

    
    <meta name="description" content="神经机器翻译（seq2seq）教程作者：Thang Luong，Eugene Brevdo，赵锐 本教程的这个版本需要TensorFlow版本1.4+。 警告：Beam Search的错误修复不在TensorFlow 1.4中。 介绍序列 - 序列（seq2seq）模型 （Sutskever等，2014， Cho等，2014） 在机器翻译，演讲等各种工作中取得了巨大的成功识别和文本摘要。本教程给读">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="神经机器翻译（seq2seq）教程">
<meta property="og:url" content="https://www.tracholar.top/2018/01/01/seq2seq/index.html">
<meta property="og:site_name" content="智子">
<meta property="og:description" content="神经机器翻译（seq2seq）教程作者：Thang Luong，Eugene Brevdo，赵锐 本教程的这个版本需要TensorFlow版本1.4+。 警告：Beam Search的错误修复不在TensorFlow 1.4中。 介绍序列 - 序列（seq2seq）模型 （Sutskever等，2014， Cho等，2014） 在机器翻译，演讲等各种工作中取得了巨大的成功识别和文本摘要。本教程给读">
<meta property="og:image" content="https://www.tensorflow.org/images/seq2seq/encdec.jpg">
<meta property="og:image" content="https://www.tensorflow.org/images/seq2seq/seq2seq.jpg">
<meta property="og:image" content="https://www.tensorflow.org/images/seq2seq/greedy_dec.jpg">
<meta property="og:image" content="https://www.tensorflow.org/images/seq2seq/attention_vis.jpg">
<meta property="og:image" content="https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg">
<meta property="og:image" content="https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg">
<meta property="og:image" content="https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg">
<meta property="og:updated_time" content="2018-01-16T12:42:41.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经机器翻译（seq2seq）教程">
<meta name="twitter:description" content="神经机器翻译（seq2seq）教程作者：Thang Luong，Eugene Brevdo，赵锐 本教程的这个版本需要TensorFlow版本1.4+。 警告：Beam Search的错误修复不在TensorFlow 1.4中。 介绍序列 - 序列（seq2seq）模型 （Sutskever等，2014， Cho等，2014） 在机器翻译，演讲等各种工作中取得了巨大的成功识别和文本摘要。本教程给读">
<meta name="twitter:image" content="https://www.tensorflow.org/images/seq2seq/encdec.jpg">

    
    <link rel="alternative" href="/atom.xml" title="智子" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">

    <!-- ad start -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6300557868920774",
    enable_page_level_ads: true
  });
</script>

    <!-- ad end -->

    <!--  stat -->
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4036f580b1119e720db871571faa68cc";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-78529611-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-78529611-1');
</script>

    <!-- end stat -->
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="智子">智子</a></h1>
				<h2 class="blog-motto">智子之家</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:www.tracholar.top">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody">
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/seq2seq/" title="神经机器翻译（seq2seq）教程" itemprop="url">神经机器翻译（seq2seq）教程</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">神经机器翻译（seq2seq）教程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">基本</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.1.</span> <span class="toc-text">神经机器翻译的背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.2.</span> <span class="toc-text">安装教程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.</span> <span class="toc-text">培训 - 如何建立我们的第一个NMT系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.1.</span> <span class="toc-text">嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.2.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.3.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.4.</span> <span class="toc-text">失利</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.5.</span> <span class="toc-text">梯度计算和优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.4.</span> <span class="toc-text">动手 - 让我们训练一个NMT模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.5.</span> <span class="toc-text">推论 - 如何生成翻译</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">中间</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.1.</span> <span class="toc-text">注意机制的背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.2.</span> <span class="toc-text">注意包装API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.3.</span> <span class="toc-text">动手 - 建立一个基于注意力的NMT模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">提示与技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.1.</span> <span class="toc-text">构建训练，评估和推理图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.2.</span> <span class="toc-text">数据输入管道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.</span> <span class="toc-text">更好的NMT模型的其他细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.1.</span> <span class="toc-text">双向RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.2.</span> <span class="toc-text">梁搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.3.</span> <span class="toc-text">超参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.4.</span> <span class="toc-text">多GPU训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">6.</span> <span class="toc-text">基准</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.1.</span> <span class="toc-text">IWSLT英语 - 越南语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.2.</span> <span class="toc-text">WMT德语 - 英语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.3.</span> <span class="toc-text">WMT英语 - 德语 - 全面比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.4.</span> <span class="toc-text">标准HParams</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">7.</span> <span class="toc-text">其他资源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">8.</span> <span class="toc-text">承认</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">9.</span> <span class="toc-text">参考</span></a></li></ol>
		
		</div>
		

        <ins class="adsbygoogle"
     style="display:block; text-align:center; overflow:hidden;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


		<h1><span id="神经机器翻译seq2seq教程">神经机器翻译（seq2seq）教程</span></h1><p>作者：Thang Luong，Eugene Brevdo，赵锐</p>
<p>本教程的这个版本需要TensorFlow版本1.4+。 警告：Beam Search的错误修复不在TensorFlow 1.4中。</p>
<h1><span id="介绍">介绍</span></h1><p>序列 - 序列（seq2seq）模型 （Sutskever等，2014， Cho等，2014） 在机器翻译，演讲等各种工作中取得了巨大的成功<br>识别和文本摘要。本教程给读者一个完整的 了解seq2seq模型，并展示如何建立一个有竞争力的seq2seq<br>模型从头开始。我们专注于神经机器翻译（NMT）的任务， 这是seq2seq模型的第一个测试平台 野生 成功。该 包括代码是轻量级，高质量，生产就绪，并入<br>有最新的研究思路。我们通过以下方式达到这个</p>
<p>使用最近的解码器/注意力    包装纸    API，    TensorFlow 1.2数据迭代器 结合我们强大的专业知识，建立经常性和seq2seq模型<br>提供建立最好的NMT模型和复制的技巧和窍门    Google的NMT（GNMT）系统。</p>
<p>我们相信提供人们可以轻松掌握的基准是非常重要的 复制。因此，我们提供了全面的实验结果 在以下公开可用的数据集上对模型进行预训练：</p>
<p>小规模：英语 - 越南语TED会话平行语料库（133K句子    双）由提供    该    IWSLT评估活动。<br>大规模：提供德英平行语料库（4.5M句子对）    由WMT评估运动。</p>
<p>我们首先建立一些关于NMT seq2seq模型的基本知识，解释 如何建立和训练一个香草NMT模型。第二部分将详细介绍<br>建立具有注意机制的竞争性NMT模型。我们然后讨论 提示和技巧，以建立最好的NMT模型（速度和速度）<br>翻译质量），如TensorFlow最佳实践（配料，桶装）， 双向RNN，波束搜索以及使用GNMT注意力扩展到多个GPU。</p>
<h1><span id="基本">基本</span></h1><h2><span id="神经机器翻译的背景">神经机器翻译的背景</span></h2><p>早在过去，传统的基于短语的翻译系统就被执行了 他们的任务是把源语句分解成多个块然后 把它们翻译成短语。这导致翻译不流利<br>产出，并不像我们人类的翻译。我们阅读整个 源句，理解它的意思，然后产生一个翻译。神经 机器翻译（NMT）模仿！</p>
<p><img src="https://www.tensorflow.org/images/seq2seq/encdec.jpg" alt=""> 图1.编码器 - 解码器架构 -<br>一个通用方法的例子 NMT。编码器将源语句转换为“含义”向量 通过解码器产生翻译。</p>
<p>具体而言，NMT系统首先使用编码器读取源句子 建立 一个 “思想”载体， 代表句子意思的数字序列;一个解码器，那么， 处理句子向量发出翻译，如图所示<br>图1.这通常被称为编码器 - 解码器架构。在 这种方式，NMT解决了传统的本地翻译问题 基于短语的方法：它可以捕捉语言的远程依赖关系，<br>例如性别协议;语法结构;等等，而且生产得更流畅 翻译如展示 通过 谷歌神经机器翻译系统。</p>
<p>NMT模型根据其确切的体系结构而有所不同。自然的选择 时序数据是大多数NMT模型使用的递归神经网络（RNN）。 编码器和解码器通常使用RNN。 RNN模型，<br>然而，不同的方面：（一）方向性 - 单向或 双向的; （b）深度 - 单层或多层;和（三）类型 - 往往 香草RNN，长期短期记忆（LSTM）或门控复发单位<br>（GRU）。有兴趣的读者可以在上找到关于RNN和LSTM的更多信息 这篇博文。</p>
<p>在本教程中，我们将深入多层RNN作为示例 单向，并使用LSTM作为经常性单位。我们展示一个这样的例子<br>模型如图2所示。在这个例子中，我们建立了一个模型来翻译源代码 把“我是学生”这句话变成目标句子“Je suisétudiant”。在一个很高的<br>NMT模型由两个递归神经网络组成：编码器 RNN仅仅消耗输入的源词而不作任何预测;该 另一方面，解码器在预测目标语句的同时对其进行处理 接下来的话。</p>
<p>有关更多信息，请参阅读者 到本教程的Luong（2016） 基于。</p>
<p><img src="https://www.tensorflow.org/images/seq2seq/seq2seq.jpg" alt=""> 图2.神经机器翻译 -<br>一个经常性的例子 由“我是学生”这个源语句翻译成的建筑 目标句子“Jesuisétudiant”。在这里，“＆lts＆gt”标志着 解码过程，而“＆lt;<br>s＆gt;”告诉解码器停止。</p>
<h2><span id="安装教程">安装教程</span></h2><p>要安装本教程，您需要在系统上安装TensorFlow。 本教程需要TensorFlow Nightly。要安装TensorFlow，请按照 安装说明在这里。</p>
<p>一旦安装了TensorFlow，您可以下载本教程的源代码 通过运行：</p>
<pre><code>git clone https://github.com/tensorflow/nmt/
</code></pre><h2><span id="培训-如何建立我们的第一个nmt系统">培训 - 如何建立我们的第一个NMT系统</span></h2><p>我们先来看看用具体代码构建NMT模型的核心 通过我们将更详细地解释图2的片段。我们推迟了数据 准备和后面的完整代码。这部分是指 文件 model.py。</p>
<p>在底层，编码器和解码器RNNs接收作为输入 如下：首先，源句子，然后是一个边界标记“\ ~~”其中 指示从编码到解码模式的过渡以及目标<br>句子。为了训练，我们将给系统提供以下张量， 它们在时间上是主要的格式，并包含单词索引：</p>
<p>encoder_inputs [max_encoder_time，batch_size]：源输入字。 decoder_inputs<br>[max_decoder_time，batch_size]：目标输入字。 decoder_outputs<br>[max_decoder_time，batch_size]：目标输出字，    这些是decode_inputs向左移一个时间步与一个<br>在右侧附加句末标签。</p>
<p>这里为了提高效率，我们用多个句子（batch_size）进行训练 一旦。测试稍有不同，所以我们稍后再讨论。</p>
<h3><span id="嵌入">嵌入</span></h3><p>鉴于单词的分类性质，该模型必须先查找来源 和目标嵌入来检索相应的词表示。对于 这个嵌入层的工作，首先为每种语言选择一个词汇。<br>通常，选择一个词汇量V，只有最常见的V个词是 视为独特。所有其他单词都转换为“未知”令牌和所有 得到相同的嵌入。嵌入权重，每种语言一组 通常在训练中学习。</p>
<pre><code># Embedding
embedding_encoder = variable_scope.get_variable(
    &quot;embedding_encoder&quot;, [src_vocab_size, embedding_size], ...)
# Look up embedding:
#   encoder_inputs: [max_time, batch_size]
#   encoder_emb_inp: [max_time, batch_size, embedding_size]
encoder_emb_inp = embedding_ops.embedding_lookup(
    embedding_encoder, encoder_inputs)
</code></pre><p>同样，我们可以构建embedding_decoder和decoder_emb_inp。注意一个 可以选择用预训练词表示来初始化嵌入权重<br>如word2vec或Glove矢量。一般来说，给予大量的培训 数据我们可以从头学习这些嵌入。</p>
<h3><span id="编码器">编码器</span></h3><p>一旦检索到，嵌入字就作为输入被馈送到主网络中， 它由两个多层RNN组成 - 一个用于源语言的编码器 目标语言的解码器。这两个RNN原则上可以共享<br>相同的重量;然而，在实践中，我们经常使用两个不同的RNN参数 （当拟合大量的训练数据集时，这样的模型做得更好）。该<br>编码器RNN使用零矢量作为其起始状态，并且构建如下：</p>
<pre><code># Build RNN cell
encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

# Run Dynamic RNN
#   encoder_outpus: [max_time, batch_size, num_units]
#   encoder_state: [batch_size, num_units]
encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
</code></pre><p>请注意，句子有不同的长度，以避免浪费计算，我们告诉 dynamic_rnn确切的源句子的长度<br>source_sequence_length。由于我们的投入是主要的时间，我们设置 time_major<br>=真。在这里，我们只建立一个单层的LSTM，encoder_cell。我们 将描述如何构建多层LSTM，添加丢失，并引起注意 稍后的部分。</p>
<h3><span id="解码器">解码器</span></h3><p>解码器还需要访问源信息，还有一个 简单的方法来实现，就是用最后一个隐藏状态来初始化它<br>编码器，encoder_state。在图2中，我们从源代码中传递隐藏状态 单词“学生”到解码器端。</p>
<pre><code># Build RNN cell
decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)



# Helper
helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_emb_inp, decoder_lengths, time_major=True)
# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
logits = outputs.rnn_output
</code></pre><p>在这里，这个代码的核心部分是BasicDecoder对象，解码器，这个<br>接收decoder_cell（类似于encoder_cell），helper和previous<br>encoder_state作为输入。通过分离解码器和帮助器，我们可以重用 不同的代码库，例如，TrainingHelper可以被替换<br>GreedyEmbeddingHelper做贪心解码。查看更多 在 helper.py。</p>
<p>最后，我们还没有提到projection_layer这是一个密集的矩阵转向 我们说明了这一点 过程在图2的顶部。</p>
<pre><code>projection_layer = layers_core.Dense(
    tgt_vocab_size, use_bias=False)
</code></pre><h3><span id="失利">失利</span></h3><p>鉴于上面的逻辑，我们现在准备计算我们的训练损失：</p>
<pre><code>crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=decoder_outputs, logits=logits)
train_loss = (tf.reduce_sum(crossent * target_weights) /
    batch_size)
</code></pre><p>这里，target_weights是一个大小与之相同的零一个矩阵 decoder_outputs。它掩盖了目标序列之外的填充位置 长度值为0。</p>
<p>重要提示：值得指出的是，我们将损失除以 batch_size，所以我们的超参数对batch_size是“不变的”。有些人 用（batch_size *<br>num_time_steps）来划分损失，这就淡化了 短句错误。更微妙的是，我们的超参数（应用于 以前的方式）不能用于后一种方式。例如，如果两者都接近<br>使用SGD学习1.0，后一种方法有效地使用了很多 1 / num_time_steps较小的学习率。</p>
<h3><span id="梯度计算和优化">梯度计算和优化</span></h3><p>现在我们已经定义了NMT模型的正向通过。计算 反向传播只是几行代码的问题：</p>
<pre><code># Calculate and clip gradients
params = tf.trainable_variables()
gradients = tf.gradients(train_loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(
    gradients, max_gradient_norm)
</code></pre><p>训练RNNs的重要步骤之一是渐变裁剪。在这里，我们剪辑 由全球规范。最大值max_gradient_norm通常设置为一个值<br>像5或1.最后一步是选择优化器。 Adam优化器是一个 共同的选择。我们也选择一个学习率。 learning_rate的值<br>通常可以在0.0001至0.001的范围内;并可以设置为减少 培训进展。</p>
<pre><code># Optimization
optimizer = tf.train.AdamOptimizer(learning_rate)
update_step = optimizer.apply_gradients(
    zip(clipped_gradients, params))
</code></pre><p>在我们自己的实验中，我们使用标准的SGD（tf.train.GradientDescentOptimizer） 随着学习速度的降低，这会产生更好的表现。看到<br>基准。</p>
<h2><span id="动手-让我们训练一个nmt模型">动手 - 让我们训练一个NMT模型</span></h2><p>让我们训练我们第一个NMT模型，从越南翻译成英文！ 我们的代码的入口点 是 nmt.py.</p>
<p>我们将使用一个小规模的TED演讲语料库（133K培训 例子）这个练习。我们在这里使用的所有数据都可以找到 在：<br><a href="https://nlp.stanford.edu/projects/nmt/。我们" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/nmt/。我们</a><br>将使用tst2012作为我们的开发数据集，tst2013作为我们的测试数据集。</p>
<p>运行以下命令下载用于训练NMT模型的数据：\     <code>nmt/scripts/download_iwslt15.sh /tmp/nmt_data</code></p>
<p>运行以下命令开始训练：</p>
<pre><code>mkdir /tmp/nmt_model
python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
</code></pre><p>以上命令训练一个128层隐藏单元的2层LSTM seq2seq模型 并嵌入12个时期。我们使用0.2的退出值（保持概率<br>0.8）。如果没有错误，我们应该看到类似于下面的日志递减 我们训练时的困惑价值。</p>
<pre><code># First evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017
  sample train data:
    src_reverse: &lt;/s&gt; &lt;/s&gt; Điều đo , dĩ nhien , la cau chuyện trich ra từ học thuyết của Karl Marx .
    ref: That , of course , was the &lt;unk&gt; distilled from the theories of Karl Marx . &lt;/s&gt; &lt;/s&gt; &lt;/s&gt;
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00
</code></pre><p>有关更多详细信息，请参阅train.py。</p>
<p>我们可以启动Tensorboard来查看训练过程中的模型摘要：</p>
<pre><code>tensorboard --port 22222 --logdir /tmp/nmt_model/
</code></pre><p>从英语和越南语的相反方向训练可以简单地通过改变：\     <code>--src=en --tgt=vi</code></p>
<h2><span id="推论-如何生成翻译">推论 - 如何生成翻译</span></h2><p>你正在训练你的NMT模型（一旦你有训练有素的模型），你 可以获得以前看不见的源句子的翻译。这个流程 被称为推理。训练和推理之间有明确的区别<br>（测试）：在推理的时候，我们只能访问源句子， 即encoder_inputs。有很多方法来执行解码。解码 方法包括贪婪，采样和波束搜索解码。在这里，我们会的<br>讨论贪婪的解码策略。</p>
<p>这个想法很简单，我们在图3中进行说明：</p>
<p>我们仍然按照与训练期间相同的方式对源句进行编码    获得一个encoder_state，并且这个encoder_state被用来初始化    解码器。<br>一旦解码器接收到解码（翻译）过程即开始    一个起始符号“\ <del>”（在我们的代码中称为tgt_sos_id）;<br>对于解码器端的每个时间步，我们将RNN的输出视为一组    logits。我们选择最可能的单词，与最大的相关联的ID<br>逻辑值，作为发出的词（这是“贪婪的”行为）。对于    在图3中的例子中，词“moi”具有最高的翻译概率<br>在第一个解码步骤中。然后，我们把这个单词作为下一个的输入    时间步长。 这个过程一直持续到产生结束标记“\</del> ”为止<br>一个输出符号（在我们的代码中称为tgt_eos_id）。</p>
<p><img src="https://www.tensorflow.org/images/seq2seq/greedy_dec.jpg" alt=""> 图3.贪婪解码 -<br>一个训练NMT模型如何产生一个例子 翻译源句子“Je suisétudiant”使用贪婪的搜索。</p>
<p>第三步是使得推论与训练不同。而不是总是 喂养正确的目标词作为输入，推理使用预测的词 该模型。这是实现贪婪解码的代码。这是非常相似的 训练解码器。</p>
<pre><code># Helper
helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
    embedding_decoder,
    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)

# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder, maximum_iterations=maximum_iterations)
translations = outputs.sample_id
</code></pre><p>在这里，我们使用GreedyEmbeddingHelper而不是TrainingHelper。因为我们这样做<br>事先不知道目标序列长度，我们使用maximum_iterations来 限制翻译长度。一个启发式就是解码两倍的 源句子长度。</p>
<pre><code>maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)
</code></pre><p>在训练完模型后，我们现在可以创建一个推理文件并翻译一些 句子：</p>
<pre><code>cat &gt; /tmp/my_infer_file.vi
# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)

python -m nmt.nmt \
    --out_dir=/tmp/nmt_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_model/output_infer

cat /tmp/nmt_model/output_infer # To view the inference as output
</code></pre><p>请注意，上述命令也可以在模型仍在训练时运行 只要有一个培训 检查点。有关更多详细信息，请参阅inference.py。</p>
<h1><span id="中间">中间</span></h1><p>经历了最基本的seq2seq模型，让我们更先进！至 建立最先进的神经机器翻译系统，我们将需要更多 “秘诀”：首先介绍的注意机制<br>由Bahdanau等人，2015年，然后再提炼 由Luong等人，2015等人提供。钥匙 关注机制的构想是建立直接的快捷连接<br>通过对相关来源的“关注”，在目标和来源之间 我们翻译的内容。关注机制的一个很好的副产品是一个 易于可视化源句子和目标句子之间的对齐矩阵（如 如图4所示）。</p>
<p><img src="https://www.tensorflow.org/images/seq2seq/attention_vis.jpg" alt=""> 图4.注意可视化 -<br>来源之间对齐的示例 和目标句子。图片取自（Bahdanau等，2015）。</p>
<p>请记住，在香草seq2seq模型中，我们传递最后的源状态 当开始解码过程时编码器到解码器。这很好 中短句子;然而，对于长句，单身<br>固定大小的隐藏状态成为信息瓶颈。而不是丢弃 在源RNN中计算的所有隐藏状态，关注机制 提供了一个方法，使解码器偷看他们（把他们当作一个<br>动态存储源信息）。通过这样做，注意机制 改善了较长句子的翻译。目前，关注机制是 事实上的标准，并已成功应用于许多其他任务 （包括图像标题生成，语音识别和文本<br>摘要）。</p>
<h2><span id="注意机制的背景">注意机制的背景</span></h2><p>我们现在描述（Luong et al 2015年），已被用于几个最先进的系统，包括 开放源代码工具包，如OpenNMT和TF 本教程中的seq2seq<br>API。我们还将提供到其他变体的连接 的关注机制。</p>
<p><img src="https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg" alt=""> 图5.注意机制</p>
<ul>
<li>基于注意的NMT系统的例子 如（Luong等，2015）中所述。我们详细地强调了第一步 关注计算。为了清楚起见，我们不显示嵌入和 图（2）中的投影层。</li>
</ul>
<p>如图5所示，关注计算发生在每个解码器 时间步。它由以下几个阶段组成：</p>
<p>将当前目标隐藏状态与所有源状态进行比较以导出    注意力权重（可以如图4所示）。 基于注意力权重，我们计算一个上下文向量作为加权    平均来源国。<br>将上下文向量与当前目标隐藏状态相结合以产生    最后关注矢量 注意向量作为下一个时间步的输入（输入    馈送）。前三个步骤可以用下面的等式来总结：</p>
<p><img src="https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg" alt=""></p>
<p>在这里，功能<code>score</code>用来比较目标隐藏状态\（h_t \） 与每个源隐藏状态\（\ overline {h} _s \），并将结果标准化为<br>产生注意力权重（一个分配源头位置）。有 评分功能的各种选择;流行的得分功能包括 乘法和加法形式在方程（4）。一旦计算，注意 矢量\（a_t<br>\）被用来导出softmax logit和损失。这与之类似 目标隐藏状态在香草seq2seq模型的顶层。功能 <code>f</code>也可以采取其他形式。</p>
<p><img src="https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg" alt=""></p>
<p>关注机制的各种实现可以找到 在 attention_wrapper.py。</p>
<p>关注机制中有什么关系？</p>
<p>正如上面的方程式所暗示的，有很多不同的注意变量。 这些变体取决于得分功能的形式和注意力 函数，以及是否使用之前的状态\（h_ {t-1} \）<br>（Bahdanau et al。， 2015年）。经验上，我们发现只有某些选择很重要。首先，基本 注意的形式，即目标与源之间的直接联系<br>出席。其次，将注意力向量输入下一个是很重要的 时间步骤通知网络关于过去的关注决定如在展示 （Luong等，2015）。最后，评分功能的选择常常会导致<br>在不同的表现。在基准测试结果中看到更多 部分。</p>
<h2><span id="注意包装api">注意包装API</span></h2><p>在我们的执行 该 AttentionWrapper， 我们借用一些术语 来自（Weston等人，2015）的工作 内存网络。而不是有可读可写的记忆，注意力<br>本教程中介绍的机制是只读内存。具体来说， 源隐藏状态集合（或它们的转换版本，例如， \（W \ overline {h} _s<br>\）在Luong的得分风格或\（W_2 \ overline {h} _s \）中 Bahdanau的得分风格）被称为“记忆”。在每个时间步骤，<br>我们使用当前目标隐藏状态作为“查询”来决定哪些部分 的内存来读取。通常，查询需要与键进行比较 对应于单独的内存插槽。在上面的介绍中<br>注意机制，我们碰巧使用了一组源隐藏状态（或它们的隐藏状态） 例如Bahdanau得分风格的\（W_1h_t \））<br>“键”。人们可以从这个记忆网络术语中得到启发 注意的形式！</p>
<p>感谢关注包装，扩展我们的香草seq2seq代码 注意力是微不足道的。这部分是指 文件attention_model.py</p>
<p>首先，我们需要定义一个注意机制，例如（Luong et al。， 2015年）：</p>
<pre><code># attention_states: [batch_size, max_time, num_units]
attention_states = tf.transpose(encoder_outputs, [1, 0, 2])

# Create an attention mechanism
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units, attention_states,
    memory_sequence_length=source_sequence_length)
</code></pre><p>在之前的编码器部分，encoder_outputs是全部的集合 源顶层隐藏状态，形状为[max_time，<br>batch_size，num_units]（因为我们用time_major设置为dynamic_rnn 真正的效率）。对于注意机制，我们需要确保<br>传入的“内存”是批量专业，所以我们需要转置 attention_states。我们将source_sequence_length传递给关注机制<br>以确保注意力权重得到正确的标准化（非填充 职位）。</p>
<p>定义了关注机制后，我们使用AttentionWrapper来包装 解码单元格：</p>
<pre><code>decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    decoder_cell, attention_mechanism,
    attention_layer_size=num_units)
</code></pre><p>其余的代码和Section Decoder几乎一样！</p>
<h2><span id="动手-建立一个基于注意力的nmt模型">动手 - 建立一个基于注意力的NMT模型</span></h2><p>为了引起注意，我们需要使用<code>luong</code>，<code>scaled_luong</code>，<code>bahdanau</code><br>或<code>normed_bahdanau</code>作为训练期间<code>attention</code>标志的值。该 标志指定我们将使用哪种注意机制。另外，我们<br>需要为注意模型创建一个新的目录，所以我们不重用 以前训练过的基本NMT模型。</p>
<p>运行以下命令开始训练：</p>
<pre><code>mkdir /tmp/nmt_attention_model

python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_attention_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
</code></pre><p>训练结束后，我们可以使用与新的out_dir相同的推理命令 推理：</p>
<pre><code>python -m nmt.nmt \
    --out_dir=/tmp/nmt_attention_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_attention_model/output_infer
</code></pre><h1><span id="提示与技巧">提示与技巧</span></h1><h2><span id="构建训练评估和推理图">构建训练，评估和推理图</span></h2><p>在TensorFlow中构建机器学习模型时，通常最好进行构建 三个单独的图表：</p>
<p>培训图表，其中： 批次，桶和可能的子样本从一组输入数据    文件/外部输入。 包括forward和backprop操作。 构造优化器，并添加训练操作。<br>评估图，其中： 批处理和桶从一组文件/外部输入中输入数据。 包括培训前沿操作，以及额外的评估操作    不用于训练。 推理图，其中： 可能不批量输入数据。<br>不抽取或抽取输入数据。 从占位符中读取输入数据（数据可以直接输入到图形中    通过feed_dict或C ++ TensorFlow服务二进制文件）。<br>包括模型转发操作的一个子集，可能还有其他的    用于在session.run调用之间存储状态的特殊输入/输出。</p>
<p>构建独立的图有几个好处：</p>
<p>推理图通常与其他两个非常不同，所以它是成立的    感觉分开建造它。 评估图变得更简单，因为它不再有所有的附加    反向操作。<br>数据馈送可以针对每个图分别实施。 变量重用要简单得多。例如，在评估图中没有    需要重新打开变量范围重用= True只是因为培训<br>模型已经创建了这些变量。所以相同的代码可以重用    而不是随处可见地重复使用参数。 在分布式培训中，单独的员工表现是司空见惯的<br>训练，评估和推理。无论如何，这些都需要建立自己的图形。    所以通过这种方式构建系统可以为分布式培训做好准备。</p>
<p>复杂性的主要来源是如何在三者之间共享变量 图表在一台机器设置。这是通过使用单独的会话来解决的 为每个图。培训会定期保存检查点，<br>eval会话和推断会话从检查点恢复参数。该 下面的例子显示了两种方法的主要区别。</p>
<p>之前：三个模型在一个图中共享一个Session</p>
<pre><code>with tf.variable_scope(&apos;root&apos;):
  train_inputs = tf.placeholder()
  train_op, loss = BuildTrainModel(train_inputs)
  initializer = tf.global_variables_initializer()

with tf.variable_scope(&apos;root&apos;, reuse=True):
  eval_inputs = tf.placeholder()
  eval_loss = BuildEvalModel(eval_inputs)

with tf.variable_scope(&apos;root&apos;, reuse=True):
  infer_inputs = tf.placeholder()
  inference_output = BuildInferenceModel(infer_inputs)

sess = tf.Session()

sess.run(initializer)

for i in itertools.count():
  train_input_data = ...
  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})

  if i % EVAL_STEPS == 0:
    while data_to_eval:
      eval_input_data = ...
      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})

  if i % INFER_STEPS == 0:
    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})
</code></pre><p>之后：三个图中的三个模型，三个会话共享相同的变量</p>
<pre><code>train_graph = tf.Graph()
eval_graph = tf.Graph()
infer_graph = tf.Graph()

with train_graph.as_default():
  train_iterator = ...
  train_model = BuildTrainModel(train_iterator)
  initializer = tf.global_variables_initializer()

with eval_graph.as_default():
  eval_iterator = ...
  eval_model = BuildEvalModel(eval_iterator)

with infer_graph.as_default():
  infer_iterator, infer_inputs = ...
  infer_model = BuildInferenceModel(infer_iterator)

checkpoints_path = &quot;/tmp/model/checkpoints&quot;

train_sess = tf.Session(graph=train_graph)
eval_sess = tf.Session(graph=eval_graph)
infer_sess = tf.Session(graph=infer_graph)

train_sess.run(initializer)
train_sess.run(train_iterator.initializer)

for i in itertools.count():

  train_model.train(train_sess)

  if i % EVAL_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    eval_model.saver.restore(eval_sess, checkpoint_path)
    eval_sess.run(eval_iterator.initializer)
    while data_to_eval:
      eval_model.eval(eval_sess)

  if i % INFER_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    infer_model.saver.restore(infer_sess, checkpoint_path)
    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})
    while data_to_infer:
      infer_model.infer(infer_sess)
</code></pre><p>注意后一种方法是如何“准备好”转换为分布式的 版。</p>
<p>新方法的另一个区别是不使用feed_dicts 在每个session.run调用提供数据（从而执行我们自己的 批处理，数据处理和操作），我们使用有状态迭代器<br>对象。这些迭代器使输入流水线在两者中都变得更容易 单机和分布式设置。我们将覆盖新的输入数据 流水线（在TensorFlow 1.2中介绍）。</p>
<h2><span id="数据输入管道">数据输入管道</span></h2><p>在TensorFlow 1.2之前，用户有两种选择向数据提供数据 TensorFlow培训和评估管道：</p>
<p>在每个培训session.run调用直接通过feed_dict提供数据。 使用tf.train中的排队机制（例如tf.train.batch）和<br>tf.contrib.train。 使用更高级别的框架，如tf.contrib.learn或    tf.contrib.slim（有效地使用＃2）。</p>
<p>第一种方法对于不熟悉TensorFlow或者 需要做异国情调的输入修改（即他们自己的小批量排队） 只能在Python中完成。第二种和第三种方法更为标准<br>但有点不灵活;他们还需要启动多个python线程 （排队跑步者）。而且，如果使用不正确的队列会导致死锁 或不透明的错误信息。尽管如此，排队更有效率<br>比使用feed_dict和单机和标准 分布式培训。</p>
<p>从TensorFlow 1.2开始，有一个新的系统可以读取数据 进入TensorFlow模型：数据集迭代器，在tf.contrib.data中找到<br>模块。数据迭代器是灵活的，易于推理和操作，并且 通过利用TensorFlow C ++运行时提供效率和多线程。</p>
<p>数据集可以从批量数据张量，文件名或张量创建 包含多个文件名。一些例子：</p>
<pre><code># Training dataset consists of multiple files.
train_dataset = tf.contrib.data.TextLineDataset(train_files)

# Evaluation dataset uses a single file, but we may
# point to a different file for each evaluation round.
eval_file = tf.placeholder(tf.string, shape=())
eval_dataset = tf.contrib.data.TextLineDataset(eval_file)

# For inference, feed input data to the dataset directly via feed_dict.
infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))
infer_dataset = tf.contrib.data.Dataset.from_tensor_slices(infer_batch)
</code></pre><p>所有的数据集可以通过输入处理类似的处理。这包括 阅读和清理数据，分类（在训练和评估的情况下） 过滤和配料。</p>
<p>为了将每个句子转换成单词串的向量，例如，我们使用 数据集图转换：</p>
<pre><code>dataset = dataset.map(lambda string: tf.string_split([string]).values)
</code></pre><p>然后，我们可以将每个句子向量切换成包含两个向量的元组 和它的动态长度：</p>
<pre><code>dataset = dataset.map(lambda words: (words, tf.size(words))
</code></pre><p>最后，我们可以对每个句子进行词汇查询。给一个查询 表格对象表，这个映射将从一个向量中转换出第一个元组元素 字符串到整数的向量。</p>
<pre><code>dataset = dataset.map(lambda words, size: (table.lookup(words), size))
</code></pre><p>加入两个数据集也很容易。如果两个文件包含一行一行 互相翻译，每一个都被读入自己的数据集，然后一个新的 包含压缩行的元组的数据集可以通过以下方式创建：</p>
<pre><code>source_target_dataset = tf.contrib.data.Dataset.zip((source_dataset, target_dataset))
</code></pre><p>可变长度句子的分组很简单。下列 转换批量从source_target_dataset批处理batch_size元素，和<br>分别将源向量和目标向量填充到最长的长度 源和目标载体在每批。</p>
<pre><code>batched_dataset = source_target_dataset.padded_batch(
        batch_size,
        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size
                        tf.TensorShape([])),     # size(source)
                       (tf.TensorShape([None]),  # target vectors of unknown size
                        tf.TensorShape([]))),    # size(target)
        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id
                         0),          # size(source) -- unused
                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id
                         0)))         # size(target) -- unused
</code></pre><p>从这个数据集发出的值将是张量有张量的嵌套元组 大小为batch_size的最左边的维度。结构将是：</p>
<p>迭代器[0] [0]具有批处理和填充的源句子矩阵。 迭代器[0] [1]具有成批的源大小向量。 迭代器[1] [0]具有批处理和填充的目标语句矩阵。<br>迭代器[1] [1]具有成批的目标大小向量。</p>
<p>最后，将批量相似大小的源语句一起分组 也有可能。请看 文件 utils / iterator_utils.py for 更多细节和全面实施。</p>
<p>从数据集中读取数据需要三行代码：创建迭代器， 获取它的值，并初始化它。</p>
<pre><code>batched_iterator = batched_dataset.make_initializable_iterator()

((source, source_lengths), (target, target_lenghts)) = batched_iterator.get_next()

# At initialization time.
session.run(batched_iterator.initializer, feed_dict={...})
</code></pre><p>一旦迭代器被初始化，每个访问源的session.run调用 或目标张量将要求来自底层数据集的下一个小批次。</p>
<h2><span id="更好的nmt模型的其他细节">更好的NMT模型的其他细节</span></h2><h3><span id="双向rnn">双向RNN</span></h3><p>编码器方面的双向性通常会带来更好的性能（使用 速度随着更多层的使用而降低）。在这里，我们给一个简化 如何建立一个单一的双向层编码器的例子：</p>
<pre><code># Construct forward and backward cells
forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(
    forward_cell, backward_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
encoder_outputs = tf.concat(bi_outputs, -1)
</code></pre><p>可以以相同的方式使用变量encoder_outputs和encoder_state 如在部分编码器中。请注意，对于多个双向层，我们需要<br>操纵一下encoder_state，看model.py，方法 _build_bidirectional_rnn（）获取更多细节。</p>
<h3><span id="梁搜索">梁搜索</span></h3><p>而贪婪的解码可以给我们相当合理的翻译质量，一个梁 搜索解码器可进一步提升性能。梁搜索的想法是 更好地探索所有可能的翻译搜索空间，保持周围<br>我们翻译的一小组顶级候选人。梁的大小被称为 光束宽度;例如尺寸10的最小光束宽度通常就足够了。对于 更多的信息，请参阅第7.2.3节<br>Neubig，（2017）。这是一个例子 光束搜索可以做到：</p>
<pre><code># Replicate encoder infos beam_width times
decoder_initial_state = tf.contrib.seq2seq.tile_batch(
    encoder_state, multiplier=hparams.beam_width)

# Define a beam-search decoder
decoder = tf.contrib.seq2seq.BeamSearchDecoder(
        cell=decoder_cell,
        embedding=embedding_decoder,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=decoder_initial_state,
        beam_width=beam_width,
        output_layer=projection_layer,
        length_penalty_weight=0.0)

# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
</code></pre><p>请注意，使用相同的dynamic_decode（）API调用，类似于 部分解码器。一旦解码，我们可以访问翻译为 如下：</p>
<pre><code>translations = outputs.predicted_ids
# Make sure translations shape is [batch_size, beam_width, time]
if self.time_major:
   translations = tf.transpose(translations, perm=[1, 2, 0])
</code></pre><p>有关更多详细信息，请参阅model.py，_build_decoder（）方法。</p>
<h3><span id="超参数">超参数</span></h3><p>有几个超参数可以导致额外的 表演。在这里，我们根据自己的经验列出一些[免责声明： 别人可能不同意我们写的东西！ ]。</p>
<p>优化：亚当可以导致“陌生”的合理结果， 体系结构，带调度的SGD通常会导致更好的性能 你可以用SGD来训练。</p>
<p>注意：Bahdanau式的注意往往需要双向的 编码器端工作正常;而Luong式的注意力往往适用于 不同的设置。对于本教程的代码，我们建议使用这两个改进<br>Luong＆Bahdanau式的注意变体：scaled_luong＆normed bahdanau。</p>
<h3><span id="多gpu训练">多GPU训练</span></h3><p>培训一个NMT模型可能需要几天的时间。放置不同的RNN图层 不同的GPU可以提高训练速度。这是一个创建的例子 RNN层在多个GPU上。</p>
<pre><code>cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      &quot;/gpu:%d&quot; % (num_layers % num_gpus)))
cell = tf.contrib.rnn.MultiRNNCell(cells)
</code></pre><p>另外，我们需要启用<code>colocate_gradients_with_ops</code>选项 <code>tf.gradients</code>并行化梯度计算。</p>
<p>您可能会注意到基于注意力的NMT模型的速度提高非常快 随着GPU数量的增加而变小。标准的一个主要缺点 注意体系结构正在使用顶层（最终）层的输出来查询<br>注意在每个时间步骤。这意味着每个解码步骤都必须等待 上一步完全完成;因此，我们不能并行解码 通过简单地在多个GPU上放置RNN层进行处理。</p>
<p>GNMT关注架构 通过使用底层（第一层）来并行解码器的计算 输出查询关注。因此，每个解码步骤可以尽快开始 其前一步的第一层和关注计算完成。我们<br>在中实现了架构 GNMTAttentionMultiCell， tf.contrib.rnn.MultiRNNCell的子类。这里是一个如何创建的例子<br>具有GNMTAttentionMultiCell的解码器单元。</p>
<pre><code>cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      &quot;/gpu:%d&quot; % (num_layers % num_gpus)))
attention_cell = cells.pop(0)
attention_cell = tf.contrib.seq2seq.AttentionWrapper(
    attention_cell,
    attention_mechanism,
    attention_layer_size=None,  # don&apos;t add an additional dense layer.
    output_attention=False,)
cell = GNMTAttentionMultiCell(attention_cell, cells)
</code></pre><h1><span id="基准">基准</span></h1><h2><span id="iwslt英语-越南语">IWSLT英语 - 越南语</span></h2><p>训练：133K例子，vocab = vocab。（vi | en），train = train。（vi | en） 开发= tst2012（六| EN）。，<br>test = tst2013。（vi | en），下载脚本。</p>
<p>培训细节。我们训练双向512个单元的2层LSTM 编码器（即，用于编码器的1个双向层），嵌入暗淡 是512. LuongAttention（scale =<br>True）和drop_out_prob一起使用 0.8。所有的参数是统一的。学习率1.0的SGD使用如下： 列车12K步（约12个时代）;<br>8K步后，我们开始减半学习 每步1K。</p>
<p>结果。 TODO（rzhao）：添加英语 - 越南语训练模型的URL。</p>
<p>以下是2个模型的平均结果 （型号1， 模型2）。\ 我们用BLEU得分来衡量翻译质量（Papineni et al。，2002）。</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>tst2012 (dev)</th>
<th>test2013 (test)  </th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (greedy)</td>
<td>23.2</td>
<td>25.5  </td>
</tr>
<tr>
<td>NMT (beam=10)</td>
<td>23.8</td>
<td><strong>26.1</strong>  </td>
</tr>
</tbody>
</table>
<p><a href="https://nlp.stanford.edu/pubs/luong-manning-
iwslt15.pdf" target="_blank" rel="noopener">(Luong &amp; Manning, 2015)</a> |  - | 23.3  </p>
<p>训练速度：在TitanX上的K40m和（0.17s步进时间，32.2K wps）上（0.37s步进时间，15.3K wps）<br>在这里，步进时间是指运行一个小批量（128号）的时间。对于wps，我们计算来源和目标上的字数。</p>
<h2><span id="wmt德语-英语">WMT德语 - 英语</span></h2><p>训练：4.5M例子，vocab = vocab.bpe.32000。（de | en）， train =<br>train.tok.clean.bpe.32000。（de | en），dev = newstest2013.tok.bpe.32000。（de |<br>en）， 测试= newstest2015.tok.bpe.32000。（DE | EN） 下载脚本</p>
<p>培训细节。我们的训练超参数类似于 英语 - 越南语的实验除了以下细节。数据是 使用BPE拆分为子字单位<br>（32K操作）。我们训练双向1024个单元的4层LSTMs 编码器（即，用于编码器的2个双向层），嵌入暗淡<br>是1024.我们训练了350K步（〜10个纪元）。经过170K步，我们开始 每17K步减半学习率。</p>
<p>结果。 TODO（rzhao）：为德英培训模型添加网址。</p>
<p>前两行是2个模型的平均结果 （型号1， 模型2）。 第三行的结果是GNMT的关注 （模型） ;使用4个GPU进行训练。</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>newstest2013 (dev)</th>
<th>newstest2015  </th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (greedy)</td>
<td>27.1</td>
<td>27.6  </td>
</tr>
<tr>
<td>NMT (beam=10)</td>
<td>28.0</td>
<td>28.9  </td>
</tr>
<tr>
<td>NMT + GNMT attention (beam=10)</td>
<td>29.0</td>
<td><strong>29.9</strong>  </td>
</tr>
<tr>
<td><a href="http://matrix.statmt.org/" target="_blank" rel="noopener">WMT SOTA</a></td>
<td>-</td>
<td>29.3  </td>
</tr>
</tbody>
</table>
<p>这些结果表明我们的代码为NMT构建了强大的基线系统。 （请注意，WMT系统通常使用大量的单语数据，而目前我们没有。</p>
<p>Nvidia K40m和（0.7s step-time，8.7K wps）在Nvidia TitanX上的训练速度（2.1s step-time，3.4K<br>wps） 为了看到GNMT关注的加速，我们仅以K40m为基准：</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>1 gpu</th>
<th>4 gpus</th>
<th>8 gpus  </th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (4 layers)</td>
<td>2.2s, 3.4K</td>
<td>1.9s, 3.9K</td>
<td>-  </td>
</tr>
<tr>
<td>NMT (8 layers)</td>
<td>3.5s, 2.0K</td>
<td>-</td>
<td>2.9s, 2.4K  </td>
</tr>
<tr>
<td>NMT + GNMT attention (4 layers)</td>
<td>2.6s, 2.8K</td>
<td>1.7s, 4.3K</td>
<td>-  </td>
</tr>
<tr>
<td>NMT + GNMT attention (8 layers)</td>
<td>4.2s, 1.7K</td>
<td>-</td>
<td>1.9s, 3.8K  </td>
</tr>
</tbody>
</table>
<p>这些结果表明，没有GNMT的关注，使用多个gpus的收益是最小的。 在GNMT的关注下，我们从多个gpus中获得了50％-100％的加速。</p>
<h2><span id="wmt英语-德语-全面比较">WMT英语 - 德语 - 全面比较</span></h2><p>前两行是我们的GNMT模型 注意： 模型1（4层）， 模型2（8层）。</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>newstest2014</th>
<th>newstest2015  </th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Ours</em> – NMT + GNMT attention (4 layers)</td>
<td>23.7</td>
<td>26.5  </td>
</tr>
<tr>
<td><em>Ours</em> – NMT + GNMT attention (8 layers)</td>
<td>24.4</td>
<td><strong>27.6</strong>  </td>
</tr>
<tr>
<td><a href="http://matrix.statmt.org/" target="_blank" rel="noopener">WMT SOTA</a></td>
<td>20.6</td>
<td>24.9  </td>
</tr>
<tr>
<td>OpenNMT <a href="https://arxiv.org/abs/1701.02810" target="_blank" rel="noopener">(Klein et al., 2017)</a></td>
<td>19.3</td>
<td>-  </td>
</tr>
<tr>
<td>tf-seq2seq <a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">(Britz et al., 2017)</a></td>
<td>22.2</td>
<td></td>
</tr>
</tbody>
</table>
<p>25.2<br>GNMT <a href="https://research.google.com/pubs/pub45610.html" target="_blank" rel="noopener">(Wu et al., 2016)</a> |<br><strong>24.6</strong> |  -  </p>
<p>上述结果表明，我们的模型在类似架构的模型中是非常有竞争力的。<br>[请注意，OpenNMT使用较小的模型，目前最好的结果（截至撰写本文时）是由Transformer网络（Vaswani et<br>al。，2017）获得的28.4，具有明显不同的架构。</p>
<h2><span id="标准hparams">标准HParams</span></h2><p>我们提供了 一套标准的hparams 使用预先训练的检查点推断或训练NMT架构 在基准中使用。</p>
<p>我们将使用WMT16的德英数据，您可以通过下载数据 以下命令。</p>
<pre><code>nmt/scripts/wmt16_en_de.sh /tmp/wmt16
</code></pre><p>以下是加载预先训练的GNMT WMT德语 - 英语的示例命令 检查点推断。</p>
<pre><code>python -m nmt.nmt \
    --src=de --tgt=en \
    --ckpt=/path/to/checkpoint/translate.ckpt \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \
    --inference_output_file=/tmp/deen_gnmt/output_infer \
    --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en
</code></pre><p>这里是GNMT WMT德 - 英模型的示例命令。</p>
<pre><code>python -m nmt.nmt \
    --src=de --tgt=en \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \
    --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \
    --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000
</code></pre><h1><span id="其他资源">其他资源</span></h1><p>深入阅读“神经机器翻译”和“序列到序列” 型号，我们强烈推荐以下材料 通过 Luong，Cho，Manning，（2016）; Luong，（2016）;<br>和Neubig（2017）。</p>
<p>建立seq2seq模型有各种各样的工具，所以我们选择一个 语言：\ 斯坦福大学NMT<br><a href="https://nlp.stanford.edu/projects/nmt/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/nmt/</a> [Matlab] \ TF-seq2seq<br><a href="https://github.com/google/seq2seq" target="_blank" rel="noopener">https://github.com/google/seq2seq</a> [TensorFlow] \ Nemantus<br><a href="https://github.com/rsennrich/nematus" target="_blank" rel="noopener">https://github.com/rsennrich/nematus</a> [Theano] \ OpenNMT <a href="http://opennmt.net/" target="_blank" rel="noopener">http://opennmt.net/</a><br>[火炬] \ OpenNMT-py <a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">https://github.com/OpenNMT/OpenNMT-py</a> [PyTorch]</p>
<h1><span id="承认">承认</span></h1><p>我们要感谢Denny Britz，Anna Goldie，Derek Murray和Cinjon<br>Resnick为TensorFlow和seq2seq库带来的新功能。另外感谢Lukasz Kaiser在seq2seq代码库上的初步帮助。 Quoc<br>Le提出复制GNMT的建议;关于GNMT系统的细节，吴永辉和陈志峰;以及Google Brain团队的支持和反馈！</p>
<h1><span id="参考">参考</span></h1><p>Dzmitry Bahdanau，Kyunghyun Cho和Yoshua    Bengio。神经机器翻译通过共同学习来对齐和翻译。 ICLR。<br>Minh-Thang Luong，Hieu Pham和Christopher D.    曼宁。有效的基于注意力的神经机器翻译方法。 EMNLP。 Ilya<br>Sutskever，Oriol Vinyals和Quoc    V. Le。 2014年。序列学习与神经网络。 NIPS。</p>


        <p style="margin-top:2em; text-align:left; font-weight:bold; font-style: italic;">未经作者同意，本文严禁转载，违者必究！</p>
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://www.tracholar.top/2018/01/01/seq2seq/" data-title="神经机器翻译（seq2seq）教程 | 智子" data-tsina="" class="share clearfix">
	  </div>
	
	</div>


</footer>


	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/01/01/wide_and_deep/" title="TensorFlow广泛深度学习教程">
  <strong>上一篇：</strong><br/>
  <span>
  TensorFlow广泛深度学习教程</span>
</a>
</div>


<div class="next">
<a href="/2018/01/01/using_gpu/"  title="使用GPU">
 <strong>下一篇：</strong><br/> 
 <span>使用GPU
</span>
</a>
</div>

</nav>

	



</div>

      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">神经机器翻译（seq2seq）教程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">基本</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.1.</span> <span class="toc-text">神经机器翻译的背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.2.</span> <span class="toc-text">安装教程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.</span> <span class="toc-text">培训 - 如何建立我们的第一个NMT系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.1.</span> <span class="toc-text">嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.2.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.3.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.4.</span> <span class="toc-text">失利</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.3.5.</span> <span class="toc-text">梯度计算和优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.4.</span> <span class="toc-text">动手 - 让我们训练一个NMT模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.5.</span> <span class="toc-text">推论 - 如何生成翻译</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">中间</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.1.</span> <span class="toc-text">注意机制的背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.2.</span> <span class="toc-text">注意包装API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.3.</span> <span class="toc-text">动手 - 建立一个基于注意力的NMT模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">提示与技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.1.</span> <span class="toc-text">构建训练，评估和推理图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.2.</span> <span class="toc-text">数据输入管道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.</span> <span class="toc-text">更好的NMT模型的其他细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.1.</span> <span class="toc-text">双向RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.2.</span> <span class="toc-text">梁搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.3.</span> <span class="toc-text">超参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.4.</span> <span class="toc-text">多GPU训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">6.</span> <span class="toc-text">基准</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.1.</span> <span class="toc-text">IWSLT英语 - 越南语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.2.</span> <span class="toc-text">WMT德语 - 英语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.3.</span> <span class="toc-text">WMT英语 - 德语 - 全面比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.4.</span> <span class="toc-text">标准HParams</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">7.</span> <span class="toc-text">其他资源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">8.</span> <span class="toc-text">承认</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">9.</span> <span class="toc-text">参考</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- side-bar-ad -->
<ins class="adsbygoogle"
     style="display:block; overflow:hidden;"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="2232545787"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


  


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/javascript/" title="javascript">javascript<sup>207</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>205</sup></a></li>
			
		
			
				<li><a href="/tags/html/" title="html">html<sup>203</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>199</sup></a></li>
			
		
			
				<li><a href="/tags/bash/" title="bash">bash<sup>198</sup></a></li>
			
		
			
				<li><a href="/tags/php/" title="php">php<sup>197</sup></a></li>
			
		
			
				<li><a href="/tags/css/" title="css">css<sup>88</sup></a></li>
			
		
			
				<li><a href="/tags/shell/" title="shell">shell<sup>78</sup></a></li>
			
		
			
				<li><a href="/tags/jquery/" title="jquery">jquery<sup>61</sup></a></li>
			
		
			
				<li><a href="/tags/linux/" title="linux">linux<sup>57</sup></a></li>
			
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>41</sup></a></li>
			
		
			
				<li><a href="/tags/unix/" title="unix">unix<sup>30</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/html5/" title="html5">html5<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/xml/" title="xml">xml<sup>13</sup></a></li>
			
		
			
				<li><a href="/tags/http/" title="http">http<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/区块链/" title="区块链">区块链<sup>1</sup></a></li>
			
		
			
		
			
		
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="https://tracholar.github.io" target="_blank" title="个人博客">个人博客</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>

    </div>
    <footer><div id="footer" >
	
	
	<section class="info">
		<p> To be or not to be, that is a question. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		版权所有 © 2018 本站文章未经同意，禁止转载！作者：
		
		<a href="/about" target="_blank" title="zhizi">zhizi</a>
		


		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>











<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
