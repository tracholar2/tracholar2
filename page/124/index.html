
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>智子</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="zhizi">
    

    
    <meta name="description" content="IT技术、编程、web开发以及新兴的技术翻译与总结">
<meta property="og:type" content="website">
<meta property="og:title" content="智子">
<meta property="og:url" content="https://www.tracholar.top/page/124/index.html">
<meta property="og:site_name" content="智子">
<meta property="og:description" content="IT技术、编程、web开发以及新兴的技术翻译与总结">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="智子">
<meta name="twitter:description" content="IT技术、编程、web开发以及新兴的技术翻译与总结">

    
    <link rel="alternative" href="/atom.xml" title="智子" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">

    <!-- ad start -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6300557868920774",
    enable_page_level_ads: true
  });
</script>

    <!-- ad end -->

    <!--  stat -->
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4036f580b1119e720db871571faa68cc";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-78529611-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-78529611-1');
</script>

    <!-- end stat -->
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="智子">智子</a></h1>
				<h2 class="blog-motto">智子之家</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:www.tracholar.top">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">


   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/linear/" title="具有张量流的大规模线性模型" itemprop="url">具有张量流的大规模线性模型</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="具有张量流的大规模线性模型">具有张量流的大规模线性模型</span></h1><p>tf.estimator API为其提供了一套丰富的工具 在TensorFlow中使用线性模型。本文件提供了一个概述 那些工具。它解释说：</p>
<p>线性模型是什么。 为什么你可能想要使用线性模型。 tf.estimator如何使TensorFlow中的线性模型变得容易。<br>如何使用tf.estimator将线性模型与    深入学习以获得双方的优势。</p>
<p>阅读本概述以确定tf.estimator线性模型工具是否可能 对你有用。然后做线性模型教程 试一试。这个概述使用了教程中的代码示例，但是<br>教程更详细地介绍代码。</p>
<p>要理解这个概述，这将有助于一些熟悉 有基本的机器学习概念，还有tf.estimator。</p>
<h2><span id="什么是线性模型">什么是线性模型？</span></h2><p>线性模型使用单个加权和特征进行预测。 例如，如果你有数据 年龄，受教育年限和每周小时数 为一个人口工作，你可以学习每个这样的数字的权重<br>他们的加权总和估计一个人的工资。您也可以使用线性模型 进行分类。</p>
<p>一些线性模型将加权和转换成更方便的形式。对于 例如，逻辑回归将加权和插入逻辑 函数将输出转换为介于0和1之间的值。但是你仍然只是 每个输入特征都有一个权重。</p>
<h2><span id="你为什么要使用线性模型">你为什么要使用线性模型？</span></h2><p>为什么你要在最近的研究中使用这么简单的模型？ 展示了更复杂的多层神经网络的力量？</p>
<p>线性模型：</p>
<p>训练很快，比较深的神经网络。 可以在非常大的功能集上运行良好。 可以用不需要太多摆弄的算法来训练    学习率等 可以比神经网络更容易地解释和调试。<br>您可以检查分配给每个功能的权重来确定是什么    对预测影响最大。 为学习机器学习提供了一个很好的起点。 在工业中被广泛使用。</p>
<h2><span id="tfestimator如何帮助您构建线性模型">tf.estimator如何帮助您构建线性模型？</span></h2><p>您可以在TensorFlow中从头开始构建一个线性模型，而无需借助 特殊的API。但是tf.estimator提供了一些工具，使它更容易构建<br>有效的大型线性模型。</p>
<h3><span id="功能列和转换">功能列和转换</span></h3><p>设计线性模型的大部分工作都是转换原始数据 到合适的输入功能。 Tensorflow使用<code>FeatureColumn</code>抽象 启用这些转换。</p>
<p><code>FeatureColumn</code>代表数据中的单一功能。 <code>FeatureColumn</code> 可能代表像“高度”这样的数量，也可能代表类似的数量<br>‘eye_color’其中值是从一组离散的可能性中绘制的 {‘蓝色’，’棕色’，’绿色’}。</p>
<p>在“高度”和“分类”等连续特征的情况下 诸如“eye_color”之类的特征，数据中的单个值可能会被转换 在输入到模型中之前将其转换为数字序列。该<br><code>FeatureColumn</code>抽象允许您将该功能作为一个单独的操作 尽管如此，语义单位。你可以指定转换和 选择要包含的特征而不处理特定的索引<br>张量你喂入模型。</p>
<h4><span id="稀疏的列">稀疏的列</span></h4><p>线性模型中的分类特征通常会转换为稀疏分类 矢量，其中每个可能的值具有对应的索引或id。对于 例如，如果只有三种可能的眼睛颜色，您可以表示<br>‘eye_color’作为长度为3的矢量：’brown’会变成[1,0,0]，’blue’会变成<br>变成[0,1,0]，“绿色”变成[0,0,1]。这些载体被调用 “稀疏”，因为它们可能很长，有很多的零时，集合 可能的值非常大（比如所有的英文单词）。</p>
<p>虽然你不需要使用分类列来使用tf.estimator linear 模型，线性模型的优势之一就是他们处理的能力 大的稀疏矢量。稀疏特征是一个主要的用例<br>tf.estimator线性模型工具。</p>
<h5><span id="编码稀疏列">编码稀疏列</span></h5><p><code>FeatureColumn</code>处理分类值向量的转换 自动，与这样的代码：</p>
<pre><code>eye_color = tf.feature_column.categorical_column_with_vocabulary_list(
    &quot;eye_color&quot;, vocabulary_list=[&quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;])
</code></pre><p>其中<code>eye_color</code>是源数据中列的名称。</p>
<p>您也可以生成<code>FeatureColumn</code>作为您的分类功能 不知道所有可能的价值。对于这种情况下，你会使用<br><code>categorical_column_with_hash_bucket()</code>，它使用散列函数来分配 指标的特征值。</p>
<pre><code>education = tf.feature_column.categorical_column_with_hash_bucket(
    &quot;education&quot;, hash_bucket_size=1000)
</code></pre><h5><span id="特色十字架">特色十字架</span></h5><p>因为线性模型分配独立的权重分离功能，他们 不能学习特定组合的相对重要性<br>值。如果你有一个功能’favorite_sport’和一个功能’home_city’和 你正在试图预测一个人是否喜欢穿红色，你的线性模型<br>将无法从圣路易斯学习棒球迷特别喜欢 穿红色。</p>
<p>您可以通过创建一个新功能来解决这个限制 ‘favorite_sport_x_home_city’。这个特性对于一个给定的人的价值是 只是两个源特征值的连接：<br>例如，’baseball_x_stlouis’。这种组合功能被称为 一个功能十字架。</p>
<p><code>crossed_column()</code>方法可以轻松设置特征十字：</p>
<pre><code>sport_x_city = tf.feature_column.crossed_column(
    [&quot;sport&quot;, &quot;city&quot;], hash_bucket_size=int(1e4))
</code></pre><h4><span id="连续的列">连续的列</span></h4><p>您可以像这样指定一个连续的功能：</p>
<pre><code>age = tf.feature_column.numeric_column(&quot;age&quot;)
</code></pre><p>虽然作为一个单一的实数，通常可以输入一个连续的特征 直接进入模型，Tensorflow为这种类型提供了有用的转换 的列也是如此。</p>
<h5><span id="桶化">桶化</span></h5><p>分行化将连续的列变成分类列。这个 转换可以让你在特征十字中使用连续的特征，或者学习 具体价值范围特别重要的案例。</p>
<p>分行化将可能值的范围划分为子范围 水桶：</p>
<pre><code>age_buckets = tf.feature_column.bucketized_column(
    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
</code></pre><p>值落入的桶成为分类标签 那个价值。</p>
<h4><span id="输入功能">输入功能</span></h4><p><code>FeatureColumn</code>为您的模型输入数据提供了一个规范， 指示如何表示和转换数据。但他们不提供 数据本身。您通过输入功能提供数据。</p>
<p>输入函数必须返回张量字典。每个键对应于 <code>FeatureColumn</code>的名称。每个键的值是一个张量包含的 所有数据实例的该功能的值。看到<br>使用tf.estimator为输入构建输入函数 更全面的看输入功能，和<code>input_fn</code>中 线性模型教程代码 作为输入函数的示例实现。</p>
<p>输入功能被传递给<code>train()</code>和<code>evaluate()</code>呼叫 开始培训和测试，如下一节所述。</p>
<h3><span id="线性估计器">线性估计器</span></h3><p>Tensorflow估算器类提供统一的培训和评估工具 用于回归和分类模型。他们照顾的细节 训练和评估循环，并允许用户专注于模型输入和 建筑。</p>
<p>要建立一个线性估计器，你可以使用 <code>tf.estimator.LinearClassifier</code>估算器或<br><code>tf.estimator.LinearRegressor</code>估算器，用于分类和 回归分别。</p>
<p>与所有张量流估计器一样，运行估算器只需要：</p>
<p>实例化估计器类。对于两个线性估计器类，    您将<code>FeatureColumn</code>的列表传递给构造函数。 调用估算器的<code>train()</code>方法进行训练。<br>调用估算器的<code>evaluate()</code>方法来看看它是如何工作的。</p>
<p>例如：</p>
<pre><code>e = tf.estimator.LinearClassifier(
    feature_columns=[
        native_country, education, occupation, workclass, marital_status,
        race, age_buckets, education_x_occupation,
        age_buckets_x_race_x_occupation],
    model_dir=YOUR_MODEL_DIRECTORY)
e.train(input_fn=input_fn_train, steps=200)
# Evaluate for one step (one pass through the test data).
results = e.evaluate(input_fn=input_fn_test)

# Print the stats for the evaluation.
for key in sorted(results):
    print(&quot;%s: %s&quot; % (key, results[key]))
</code></pre><h3><span id="广泛深入的学习">广泛深入的学习</span></h3><p>tf.estimator API还提供了一个可以联合使用的估计器类 训练线性模型和深度神经网络。这种新颖的方法结合了 线性模型用泛化“记忆”关键特征的能力<br>神经网络的能力。使用<code>tf.estimator.DNNLinearCombinedClassifier</code>来 创造这种“宽而深”的模式：</p>
<pre><code>e = tf.estimator.DNNLinearCombinedClassifier(
    model_dir=YOUR_MODEL_DIR,
    linear_feature_columns=wide_columns,
    dnn_feature_columns=deep_columns,
    dnn_hidden_units=[100, 50])
</code></pre><p>有关更多信息，请参阅广泛深入学习教程。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  
      <ins class="adsbygoogle"
     style="display:block;  overflow:hidden;"
     data-ad-format="fluid"
     data-ad-layout-key="-ej+6f-q-c7+ou"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="5206371097"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/layers/" title="TF层指南：建立卷积神经网络" itemprop="url">TF层指南：建立卷积神经网络</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="tf层指南建立卷积神经网络">TF层指南：建立卷积神经网络</span></h1><p>TensorFlow <code>layers</code>模块提供了一个高级API 很容易构建一个神经网络。它提供了方便的方法 创建密集（完全连接）层和卷积层，增加<br>激活功能，并应用失落正则化。在本教程中， 您将学习如何使用<code>layers</code>构建卷积神经网络模型 识别MNIST数据集中的手写数字。</p>
<p><img src="https://www.tensorflow.org/images/mnist_0-9.png" alt="handwritten digits 0-9 from the MNIST data
set"></p>
<p>MNIST数据集包含60,000 训练实例和手写数字0-9的10000个测试例子， 格式化为28x28像素的单色图像。</p>
<h2><span id="入门">入门</span></h2><p>让我们为我们的TensorFlow程序设置骨架。创建一个名为的文件 <code>cnn_mnist.py</code>，并添加以下代码：</p>
<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

# Imports
import numpy as np
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)

# Our application logic will be added here

if __name__ == &quot;__main__&quot;:
  tf.app.run()
</code></pre><p>在学习本教程时，您将添加代码来构建，训练和 评估卷积神经网络。完整的，最终的代码可以 在这里找到。</p>
<h2><span id="卷积神经网络简介">卷积神经网络简介</span></h2><p>卷积神经网络（CNN）是目前最先进的模型 图像分类任务的体系结构。 CNNs应用一系列的过滤器 图像的原始像素数据提取并学习更高级别的特征<br>该模型可以用于分类。 CNN包含三个组件：</p>
<p>卷积层，应用指定数量的卷积     过滤到图像。对于每个分区域，图层执行一组     数学运算在输出特征映射中产生单个值。     卷积层然后通常应用一个<br>ReLU激活功能     输出将非线性引入到模型中。 合并图层，其中     对图像数据进行下采样     由卷积层提取，以减少维数<br>功能图，以减少处理时间。一个常用的池     算法是最大池，提取特征地图的子区域     （例如，2×2像素的图块）保持其最大值，并丢弃所有其他图像<br>值。 密集（完全连接）图层，执行分类     由卷积层提取的特征和由下采样的特征     合并图层。在一个密集的图层中，图层中的每个节点都连接到<br>前一层中的每个节点。</p>
<p>通常，CNN是由一堆执行的卷积模块组成的 特征提取。每个模块由卷积层和后面的一个组成 合并图层。最后一个卷积模块之后是一个或多个密集的 执行分类的图层。<br>CNN中最后的密集层包含一个 模型中每个目标类的单个节点（所有可能的类） 模型可能预测），与一个 softmax激活功能<br>为每个节点生成0-1之间的值（所有这些softmax值的总和 等于1）。我们可以解释给定图像的softmax值 图像落入每个目标的可能性的相对测量 类。</p>
<blockquote>
<p>注意：有关CNN体系结构的更全面的演练，请参阅Stanford 大学的 卷积神经网络的视觉识别课程教材。</p>
</blockquote>
<h2><span id="建立cnn-mnist分类器">建立CNN MNIST分类器</span></h2><p>让我们建立一个模型来分类MNIST数据集中的图像 遵循CNN架构：</p>
<p>卷积层＃1：应用32个5x5滤波器（提取5x5像素     子区域），具有ReLU激活功能 池层＃1：使用2x2过滤器和2的步幅执行最大池化<br>（它指定汇集区域不重叠） 卷积层＃2：应用64个5x5滤波器，并激活ReLU     功能 池层＃2：同样，使用2x2过滤器执行最大池化     2的步幅<br>密集层＃1：1,024个神经元，丢失正则化率为0.4     （在训练期间任何给定元素将被丢弃的概率为0.4） 密集层＃2（Logits<br>Layer）：10个神经元，每个数字目标一个     类（0-9）。</p>
<p><code>tf.layers</code>模块包含创建三种图层类型的方法 以上：</p>
<p><code>conv2d()</code>。构造一个二维卷积层。号码     过滤器，过滤内核大小，填充和激活功能     参数。<br><code>max_pooling2d()</code>。构造一个二维池使用的层     最大池算法。采用过滤器大小和步幅作为参数。<br><code>dense()</code>。构建一个密集的图层。需要数量的神经元和激活     函数作为参数。</p>
<p>这些方法中的每一个都接受一个张量作为输入，并返回一个变换张量 作为输出。这使得连接一层到另一层变得很容易：<br>从一个图层创建方法输出，并将其作为输入提供给另一个。</p>
<p>打开<code>cnn_mnist.py</code>，添加以下<code>cnn_model_fn</code>功能 符合TensorFlow的Estimator<br>API所期望的界面（更多内容请参考这里 稍后在创建估算器）。 <code>cnn_mnist.py</code>需要 MNIST功能数据，标签和<br>模型模式（<code>TRAIN</code>，<code>EVAL</code>，<code>PREDICT</code>）作为参数; 配置CNN;并返回预测，丢失和训练操作：</p>
<pre><code>def cnn_model_fn(features, labels, mode):
  &quot;&quot;&quot;Model function for CNN.&quot;&quot;&quot;
  # Input Layer
  input_layer = tf.reshape(features[&quot;x&quot;], [-1, 28, 28, 1])

  # Convolutional Layer #1
  conv1 = tf.layers.conv2d(
      inputs=input_layer,
      filters=32,
      kernel_size=[5, 5],
      padding=&quot;same&quot;,
      activation=tf.nn.relu)

  # Pooling Layer #1
  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

  # Convolutional Layer #2 and Pooling Layer #2
  conv2 = tf.layers.conv2d(
      inputs=pool1,
      filters=64,
      kernel_size=[5, 5],
      padding=&quot;same&quot;,
      activation=tf.nn.relu)
  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

  # Dense Layer
  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])
  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
  dropout = tf.layers.dropout(
      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

  # Logits Layer
  logits = tf.layers.dense(inputs=dropout, units=10)

  predictions = {
      # Generate predictions (for PREDICT and EVAL mode)
      &quot;classes&quot;: tf.argmax(input=logits, axis=1),
      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
      # `logging_hook`.
      &quot;probabilities&quot;: tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)
  }

  if mode == tf.estimator.ModeKeys.PREDICT:
    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

  # Calculate Loss (for both TRAIN and EVAL modes)
  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)
  loss = tf.losses.softmax_cross_entropy(
      onehot_labels=onehot_labels, logits=logits)

  # Configure the Training Op (for TRAIN mode)
  if mode == tf.estimator.ModeKeys.TRAIN:
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

  # Add evaluation metrics (for EVAL mode)
  eval_metric_ops = {
      &quot;accuracy&quot;: tf.metrics.accuracy(
          labels=labels, predictions=predictions[&quot;classes&quot;])}
  return tf.estimator.EstimatorSpec(
      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)
</code></pre><p>以下部分（与上面每个代码块对应的标题） 更深入地介绍用于创建每个图层的<code>tf.layers</code>代码，以及如何实现 计算损失，配置训练op，并产生预测。如果<br>你已经有了CNN和TensorFlow <code>Estimator</code>的经验， 并找到上面的代码直观，你可能想略过这些部分或只是 跳到“培训和评估CNN<br>MNIST” 分类”。</p>
<h3><span id="输入层">输入层</span></h3><p><code>layers</code>模块中用于创建卷积层和汇聚层的方法 对于二维图像数据，期望输入张量具有一个形状<br>[batch_size，image_width，image_height， 渠道]，定义如下：</p>
<p><code>batch_size</code>。执行时要使用的示例子集的大小     训练期间的梯度下降。 <code>image_width</code>。示例图像的宽度。<br><code>image_height</code>。示例图像的高度。 <code>channels</code>。示例图像中的颜色通道数量。对于颜色<br>图像，通道数量是3（红色，绿色，蓝色）。对于单色     图像，只有1个通道（黑色）。</p>
<p>这里，我们的MNIST数据集是由单色的28x28像素图像组成的，所以 我们输入图层所需的形状是[batch_size，28,28， 1]。</p>
<p>为了把我们的输入特征图（<code>features</code>）转换成这个形状，我们可以执行这个 遵循<code>reshape</code>操作：</p>
<pre><code>input_layer = tf.reshape(features[&quot;x&quot;], [-1, 28, 28, 1])
</code></pre><p>请注意，我们已经指出了<code>-1</code>的批量大小，它指定了这一点 维度应根据输入值的数量动态计算 <code>features[&quot;x&quot;]</code>，其它尺寸的尺寸保持不变。这允许<br>我们把<code>batch_size</code>作为我们可以调节的超参数。例如，如果 我们分批举例将样品投入到5台<code>features[&quot;x&quot;]</code>中<br>3,920个值（每个图像中每个像素一个值），<code>input_layer</code>将会 具有<code>[5, 28, 28, 1]</code>的形状。同样，如果我们分批举例<br>100，<code>features[&quot;x&quot;]</code>将包含78,400个值，而<code>input_layer</code>将包含一个 <code>[100, 28, 28, 1]</code>的形状。</p>
<h3><span id="卷积层1">卷积层＃1</span></h3><p>在我们的第一个卷积图层中，我们希望将32个5x5滤波器应用于输入 层，具有ReLU激活功能。我们可以使用<code>conv2d()</code>方法<br><code>layers</code>模块创建此图层如下：</p>
<pre><code>conv1 = tf.layers.conv2d(
    inputs=input_layer,
    filters=32,
    kernel_size=[5, 5],
    padding=&quot;same&quot;,
    activation=tf.nn.relu)
</code></pre><p><code>inputs</code>参数指定了我们的输入张量，它必须具有形状 [batch_size，image_width，image_height，<br>信道]。在这里，我们正在连接我们的第一个卷积层 到<code>input_layer</code>，其形状为[batch_size，28,28， 1]。</p>
<blockquote>
<p>注意：<code>conv2d()</code>将改为接受一个形状 [channels，batch_size，image_width， image_height]传递参数时<br><code>data_format=channels_first</code>。</p>
</blockquote>
<p><code>filters</code>参数指定要应用的过滤器数（此处为32），以及 <code>kernel_size</code>将过滤器的尺寸指定为[宽度， 高度]（这里是<code>[5, 5]</code>）。</p>
<p>提示：如果过滤器宽度和高度具有相同的值，则可以改为指定一个 <code>kernel_size</code>的单个整数，例如<code>kernel_size=5</code>。</p>
<p><code>padding</code>参数指定了两个枚举值中的一个 （不区分大小写）：<code>valid</code>（默认值）或<code>same</code>。要指定的<br>输出张量应该具有与输入张量相同的宽度和高度值， 我们在这里设置了<code>padding=same</code>，它指示TensorFlow为其添加0值<br>输入张量的边缘保留28的宽度和高度。（没有填充， 在28x28张量上的5x5卷积将产生24x24的张量 24x24位置从28x28网格中提取5x5的图块。）</p>
<p><code>activation</code>参数指定要应用于的激活函数 卷积的输出。在这里，我们指定了ReLU激活 <code>tf.nn.relu</code>。</p>
<p>我们的<code>conv2d()</code>产生的输出张量有一个形状 <code>[ _batch_size_ , 28, 28, 32]</code>：宽度和高度相同<br>尺寸作为输入，但现在有32个通道保持每个输出 的过滤器。</p>
<h3><span id="池层1">池层＃1</span></h3><p>接下来，我们将我们的第一个池层连接到卷积层 创建。我们可以用<code>max_pooling2d()</code>中的<code>layers</code>方法构建一个<br>用2×2滤波器执行最大汇聚的层，步长为2：</p>
<pre><code>pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)
</code></pre><p>再一次，<code>inputs</code>指定输入张量，形状为 [batch_size，image_width，image_height，<br>信道]。在这里，我们的输入张量是来自<code>conv1</code>的输出 第一卷积层，其形状为[batch_size， 28，28，32]。</p>
<blockquote>
<p>注：与<code>conv2d()</code>一样，<code>max_pooling2d()</code>将改为 接受一个[channels，batch_size，<br>image_width，image_height]传递参数时 <code>data_format=channels_first</code>。</p>
</blockquote>
<p><code>pool_size</code>参数指定最大池过滤器的大小为 <code>[ _width_ , _height_ ]</code>（这里是<code>[2, 2]</code>）。如果两个<br>维度具有相同的值，您可以改为指定一个整数（例如， <code>pool_size=2</code>）。</p>
<p><code>strides</code>参数指定步幅的大小。在这里，我们迈出了一大步 2，这表明过滤器提取的子区域应该是 在宽度和高度尺寸上相隔2个像素（对于2×2滤波器，<br>这意味着没有任何提取的区域会重叠）。如果你想设置 不同的宽度和高度的跨度值，你可以改为指定一个元组或者 列表（例如，<code>stride=[3, 6]</code>）。</p>
<p>我们的<code>max_pooling2d()</code>（<code>pool1</code>）产生的张量具有 <code>[ _batch_size_ , 14, 14,
32]</code>：2x2滤波器可以减小宽度和 身高各减50％。</p>
<h3><span id="卷积层2和汇聚层2">卷积层＃2和汇聚层＃2</span></h3><p>我们可以使用第二个卷积和连接层来连接CNN <code>conv2d()</code>和<code>max_pooling2d()</code>。对于卷积层＃2，我们<br>使用ReLU激活配置64个5x5过滤器，并为第2层合并使用 与汇聚层＃1（跨度为2的2x2最大汇集过滤器）相同的规格：</p>
<pre><code>conv2 = tf.layers.conv2d(
    inputs=pool1,
    filters=64,
    kernel_size=[5, 5],
    padding=&quot;same&quot;,
    activation=tf.nn.relu)

pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)
</code></pre><p>请注意，卷积层＃2获取我们第一个池的输出张量 （<code>pool1</code>）作为输入，产生张量<code>conv2</code>作为输出。 <code>conv2</code> 具有<code>[
_batch_size_ , 14, 14, 64]</code>的形状，宽度相同 和<code>pool1</code>（由于<code>padding=&quot;same&quot;</code>）的高度，以及64通道的64<br>应用过滤器。</p>
<p>汇聚层＃2以<code>conv2</code>为输入，生成<code>pool2</code>作为输出。 <code>pool2</code> 具有形状<code>[ _batch_size_ , 7, 7,
64]</code>（减少50％的宽度 和<code>conv2</code>的高度）。</p>
<h3><span id="密集层">密集层</span></h3><p>接下来，我们要添加一个致密层（具有1024个神经元和ReLU激活） 我们的CNN对所提取的特征进行分类 卷积/合并图层。然而，在我们连接图层之前，我们会变平<br>我们的功能图（<code>pool2</code>）可以将[batch_size， 特征]，所以我们的张量只有两个维度：</p>
<pre><code>pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])
</code></pre><p>在上述<code>reshape()</code>操作中，<code>-1</code>表示<code>batch_size</code> 维度将根据我们的例子数量动态计算 输入数据。每个例子有7个（<code>pool2</code>宽度）<em><br>7（<code>pool2</code>高度）</em> 64 （<code>pool2</code>通道）功能，所以我们希望<code>features</code>尺寸有一个值 7 <em> 7 </em><br>64（总计3136）。输出张量<code>pool2_flat</code>具有形状 <code>[ _batch_size_ , 3136]</code>。</p>
<p>现在我们可以使用<code>dense()</code>中的<code>layers</code>方法来连接我们的密集层 如下：</p>
<pre><code>dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
</code></pre><p><code>inputs</code>参数指定输入张量：我们的平坦特征图， <code>pool2_flat</code>。 <code>units</code>参数指定密度中的神经元数量 层（1,024）。<br><code>activation</code>参数采用激活功能;再次， 我们将使用<code>tf.nn.relu</code>添加ReLU激活。</p>
<p>为了帮助改进我们模型的结果，我们也应用了辍学正规化 使用<code>dropout</code>中的<code>layers</code>方法：</p>
<pre><code>dropout = tf.layers.dropout(
    inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)
</code></pre><p>再次，<code>inputs</code>规定了输入张量，这是我们的输出张量 致密层（<code>dense</code>）。</p>
<p><code>rate</code>参数指定丢失率;在这里，我们使用<code>0.4</code>，这意味着 40％的元素将在训练中随机退出。</p>
<p><code>training</code>参数采用布尔值来指定模型是否为 目前正在训练模式下运行;退出将只会被执行，如果<br><code>training</code>是<code>True</code>。在这里，我们检查<code>mode</code>是否传递给我们的模型功能 <code>cnn_model_fn</code>是<code>TRAIN</code>模式。</p>
<p>我们的输出张量<code>dropout</code>有形状<code>[ _batch_size_ , 1024]</code>。</p>
<h3><span id="logits图层">Logits图层</span></h3><p>在我们的神经网络的最后一层是logits层，它将返回 我们预测的原始值。我们创建了一个密集的10层神经元层（1层） 每个目标类0-9），线性激活（默认）：</p>
<pre><code>logits = tf.layers.dense(inputs=dropout, units=10)
</code></pre><p>CNN的最终输出张量<code>logits</code>已经形成 <code>[ _batch_size_ , 10]</code>。</p>
<h3><span id="生成预测">生成预测</span></h3><p>我们模型的logits层将我们的预测作为原始值返回给a <code>[ _batch_size_ , 10]</code>维张量。我们来转换这些<br>原始值转换为我们的模型函数可以返回的两种不同的格式：</p>
<p>每个示例的预测类别：0-9的数字。 每个例子的每个可能的目标类的概率：     例子的概率是0，是1，是2等等</p>
<p>对于给定的例子，我们预测的类是相应行中的元素 具有最高原始价值的logits张量。我们可以找到这个索引 元素使用<code>tf.argmax</code> 功能：</p>
<pre><code>tf.argmax(input=logits, axis=1)
</code></pre><p><code>input</code>参数指定从中提取最大值的张量 值 - 在这里<code>logits</code>。 <code>axis</code>自变量指定<code>input</code>的轴<br>张量沿其找到最大的价值。在这里，我们想找到最大的 指数为1的维度值与我们的预测相符 （回想一下，我们的logits<br>tensor已经形成[batch_size， 10]）。</p>
<p>我们可以通过应用softmax激活从我们的logits层中得出概率 使用<code>tf.nn.softmax</code>：</p>
<pre><code>tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)
</code></pre><blockquote>
<p>注意：我们使用<code>name</code>参数来明确命名这个操作 <code>softmax_tensor</code>，所以稍后可以参考。 （我们将设置日志记录<br>在“设置记录挂钩”中的softmax值。</p>
</blockquote>
<p>我们用一个字典来编译我们的预测，然后返回一个<code>EstimatorSpec</code>对象：</p>
<pre><code>predictions = {
    &quot;classes&quot;: tf.argmax(input=logits, axis=1),
    &quot;probabilities&quot;: tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)
}
if mode == tf.estimator.ModeKeys.PREDICT:
  return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)
</code></pre><h3><span id="计算损失">计算损失</span></h3><p>对于培训和评估，我们需要定义一个 损失功能 它衡量模型的预测与目标类别的匹配程度。对于 多类分类问题如MNIST， 通常使用交叉熵<br>作为损失度量。下面的代码计算模型的交叉熵 以<code>TRAIN</code>或<code>EVAL</code>模式运行：</p>
<pre><code>onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)
loss = tf.losses.softmax_cross_entropy(
    onehot_labels=onehot_labels, logits=logits)
</code></pre><p>让我们仔细看看上面发生的事情。</p>
<p>我们的<code>labels</code>张量包含了我们例子的预测列表， [1， 9，…]。为了计算交叉熵，首先我们需要转换<code>labels</code> 到相应的 热门编码：</p>
<pre><code>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
 ...]
</code></pre><p>我们使用<code>tf.one_hot</code>功能 执行此转换。 <code>tf.one_hot()</code>有两个必需的参数：</p>
<p><code>indices</code>。单张张量中的位置将会“打开”     值“ - 即如上所示张量中的<code>1</code>值的位置。 <code>depth</code>。单热张量的深度 -<br>即目标类别的数量。     这里的深度是<code>10</code>。</p>
<p>以下代码为我们的标签<code>onehot_labels</code>创建了单张张量：</p>
<pre><code>onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)
</code></pre><p>由于<code>labels</code>包含0-9的一系列值，所以<code>indices</code>就是我们的 <code>labels</code>张量，值转换为整数。 <code>depth</code>是<code>10</code>，因为我们<br>有10个可能的目标类别，每个数字一个。</p>
<p>接下来，我们计算<code>onehot_labels</code>的交叉熵和最大值的softmax 来自我们的logits层的预测。<br><code>tf.losses.softmax_cross_entropy()</code>需要 <code>onehot_labels</code>和<code>logits</code>作为参数执行softmax激活<br><code>logits</code>计算交叉熵，并将<code>loss</code>作为标量<code>Tensor</code>返回：</p>
<pre><code>loss = tf.losses.softmax_cross_entropy(
    onehot_labels=onehot_labels, logits=logits)
</code></pre><h3><span id="配置培训操作">配置培训操作</span></h3><p>在上一节中，我们将CNN的损失定义为softmax logits层的交叉熵和我们的标签。让我们来配置我们的模型<br>在训练期间优化这个损失值。我们将使用0.001的学习率 随机梯度下降 作为优化算法：</p>
<pre><code>if mode == tf.estimator.ModeKeys.TRAIN:
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
  train_op = optimizer.minimize(
      loss=loss,
      global_step=tf.train.get_global_step())
  return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)
</code></pre><blockquote>
<p>注意：要更深入地了解为Estimator模型配置培训操作 功能，请参阅“定义 在“创造估计”中的模型的训练 tf.estimator“教程。</p>
</blockquote>
<h3><span id="添加评估指标">添加评估指标</span></h3><p>为了在我们的模型中增加准确性度量，我们在EVAL中定义了<code>eval_metric_ops</code>字典 模式如下：</p>
<pre><code>eval_metric_ops = {
    &quot;accuracy&quot;: tf.metrics.accuracy(
        labels=labels, predictions=predictions[&quot;classes&quot;])}
return tf.estimator.EstimatorSpec(
    mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)
</code></pre><h2><span id="培训和评估cnn-mnist分类器">培训和评估CNN MNIST分类器</span></h2><p>我们编写了我们的MNIST CNN模型函数。现在我们准备好进行培训和评估 它。</p>
<h3><span id="加载训练和测试数据">加载训练和测试数据</span></h3><p>首先，让我们加载我们的训练和测试数据。添加<code>main()</code>功能 <code>cnn_mnist.py</code>使用以下代码：</p>
<pre><code>def main(unused_argv):
  # Load training and eval data
  mnist = tf.contrib.learn.datasets.load_dataset(&quot;mnist&quot;)
  train_data = mnist.train.images # Returns np.array
  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
  eval_data = mnist.test.images # Returns np.array
  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)
</code></pre><p>我们存储训练特征数据（55000个图像的原始像素值） 手绘数字）和训练标签（相应的数值从0到9） 每个图像）作为numpy 阵列<br>分别在<code>train_data</code>和<code>train_labels</code>中。同样，我们存储的 <code>eval_data</code>评估特征数据（10,000张图像）和评估标签<br>和<code>eval_labels</code>。</p>
<h3><span id="创建估算器">创建估算器</span></h3><p>接下来，我们来创建一个<code>Estimator</code>（一个TensorFlow类，用于执行高级功能 模型训练，评估和推理）。添加下面的代码 以<code>main()</code>：</p>
<pre><code># Create the Estimator
mnist_classifier = tf.estimator.Estimator(
    model_fn=cnn_model_fn, model_dir=&quot;/tmp/mnist_convnet_model&quot;)
</code></pre><p><code>model_fn</code>参数指定用于训练的模型函数， 评估和预测;我们通过了我们创建的<code>cnn_model_fn</code> “建立CNN MNIST分类器”。该<br><code>model_dir</code>参数指定模型数据（检查点）所在的目录 保存（在这里，我们指定临时目录<code>/tmp/mnist_convnet_model</code>，但是<br>随意更改到您选择的另一个目录）。</p>
<blockquote>
<p>注意：有关TensorFlow <code>Estimator</code> API的深入演练，请参阅 教程“创建tf.estimator中的估计器”</p>
</blockquote>
<h3><span id="设置日志钩子">设置日志钩子</span></h3><p>由于CNN可能需要一段时间才能训练，所以我们建立一些日志记录，以便跟踪<br>培训期间的进展。我们可以使用TensorFlow的<code>tf.train.SessionRunHook</code>创建一个<br><code>tf.train.LoggingTensorHook</code> 这将记录来自CNN的softmax层的概率值。添加 遵循<code>main()</code>：</p>
<pre><code># Set up logging for predictions
  tensors_to_log = {&quot;probabilities&quot;: &quot;softmax_tensor&quot;}
  logging_hook = tf.train.LoggingTensorHook(
      tensors=tensors_to_log, every_n_iter=50)
</code></pre><p>我们存储了一张我们想要登录<code>tensors_to_log</code>的张量词典。每个关键是一个 我们选择的标签将被打印在日志输出中，而<br>对应的标签是TensorFlow图中<code>Tensor</code>的名称。在这里，我们的<br><code>probabilities</code>可以在<code>softmax_tensor</code>中找到，我们给的名字叫softmax<br>当我们在<code>cnn_model_fn</code>中产生概率时，我们可以更早地进行操作。</p>
<blockquote>
<p>注意：如果您未通过<code>name</code>明确指定操作名称 参数，TensorFlow将分配一个默认名称。一对简单的方法 发现应用到操作的名称是可视化您的图形<br>TensorBoard）或启用TensorFlow调试器 （tfdbg）。</p>
</blockquote>
<p>接下来，我们创建了<code>LoggingTensorHook</code>，通过了<code>tensors_to_log</code><br><code>tensors</code>的说法。我们设置了<code>every_n_iter=50</code>，它规定了概率 应在每50个步骤的训练后记录。</p>
<h3><span id="训练模型">训练模型</span></h3><p>现在我们准备培训我们的模型，我们可以通过创建<code>train_input_fn</code>来完成<br>在<code>train()</code>上调用<code>mnist_classifier</code>。将以下内容添加到<code>main()</code>中：</p>
<pre><code># Train the model
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={&quot;x&quot;: train_data},
    y=train_labels,
    batch_size=100,
    num_epochs=None,
    shuffle=True)
mnist_classifier.train(
    input_fn=train_input_fn,
    steps=20000,
    hooks=[logging_hook])
</code></pre><p>在<code>numpy_input_fn</code>调用中，我们将训练特征数据和标签传递给 <code>x</code>（作为字典）和<code>y</code>。我们设置<code>batch_size</code> <code>100</code>（其中<br>意味着模型将在每个步骤的100个示例的minibatches上进行训练）。 <code>num_epochs=None</code>表示模型将训练到指定的数量<br>步骤到达。我们还设置了<code>shuffle=True</code>来洗牌训练数据。 在<code>train</code>调用中，我们设置了<code>steps=20000</code><br>（这意味着该模型将训练总计20,000步）。我们通过我们的 <code>logging_hook</code>转换为<code>hooks</code>参数，以便在此期间触发 训练。</p>
<h3><span id="评估模型">评估模型</span></h3><p>一旦培训完成，我们要评估我们的模型来确定它的 MNIST测试集的准确性。我们称<code>evaluate</code>方法为评估<br>我们在<code>eval_metric_ops</code>的<code>model_fn</code>参数中指定的度量。 将以下内容添加到<code>main()</code>中：</p>
<pre><code># Evaluate the model and print results
eval_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={&quot;x&quot;: eval_data},
    y=eval_labels,
    num_epochs=1,
    shuffle=False)
eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)
print(eval_results)
</code></pre><p>为了创建<code>eval_input_fn</code>，我们设置了<code>num_epochs=1</code>，以便模型评估 一个数据时代的指标并返回结果。我们也设置<br><code>shuffle=False</code>按顺序循环访问数据。</p>
<h3><span id="运行模型">运行模型</span></h3><p>我们编写了CNN模型功能<code>Estimator</code>和培训/评估 逻辑;现在让我们看看结果。运行<code>cnn_mnist.py</code>。</p>
<blockquote>
<p>注意：培训CNN是相当计算密集型的。预计完成 <code>cnn_mnist.py</code>的时间将取决于您的处理器，但可能会有所不同<br>在CPU上超过1小时。为了更快地训练，你可以减少 <code>steps</code>的编号传给了<code>train()</code>，但是请注意这会影响准确性。</p>
</blockquote>
<p>模型训练时，您会看到如下所示的日志输出：</p>
<pre><code>INFO:tensorflow:loss = 2.36026, step = 1
INFO:tensorflow:probabilities = [[ 0.07722801  0.08618255  0.09256398, ...]]
...
INFO:tensorflow:loss = 2.13119, step = 101
INFO:tensorflow:global_step/sec: 5.44132
...
INFO:tensorflow:Loss for final step: 0.553216.

INFO:tensorflow:Restored model from /tmp/mnist_convnet_model
INFO:tensorflow:Eval steps [0,inf) for training step 20000.
INFO:tensorflow:Input iterator is exhausted.
INFO:tensorflow:Saving evaluation summary for step 20000: accuracy = 0.9733, loss = 0.0902271
{&apos;loss&apos;: 0.090227105, &apos;global_step&apos;: 20000, &apos;accuracy&apos;: 0.97329998}
</code></pre><p>在这里，我们的测试数据集已经达到了97.3％的准确率。</p>
<h2><span id="其他资源">其他资源</span></h2><p>要了解有关TensorFlow中的TensorFlow估算器和CNN的更多信息，请参阅 以下资源：</p>
<p>在tf.estimator中创建估计器一个     介绍了TensorFlow Estimator API     配置估算器，编写模型函数，计算损失，以及<br>定义一个训练操作。 专家深度MNIST：建立一个多层CNN。自助游     通过如何构建不使用图层的MNIST CNN分类模型<br>低层次的TensorFlow操作。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/image_recognition/" title="图像识别" itemprop="url">图像识别</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="图像识别">图像识别</span></h1><p>我们的大脑让视觉看起来很容易。人类不需要付出任何努力 分辨一只狮子和一只捷豹，读一个标志，或者辨认一个人的脸。<br>但是，这些实际上是用计算机解决的难题：只有他们 看起来很容易，因为我们的大脑非常善于理解图像。</p>
<p>在过去的几年中，机器学习领域取得了巨大成就 在解决这些难题方面取得进展。特别是，我们已经 发现一种叫做“深”的模式 卷积神经网络<br>可以在硬视觉识别任务上达到合理的性能 - 在某些领域匹配或超过人的表现。</p>
<p>研究人员已经证明了稳定的进展 在计算机视觉中通过验证他们的工作 ImageNet - 计算机视觉的学术基准。 连续的模型继续表现出改进，每次都实现<br>一个新的最新的结果： QuocNet，AlexNet，Inception（GoogLeNet），BN-Inception-v2。<br>Google内部和外部的研究人员已经发表了描述所有内容的论文 这些模型但结果仍难以重现。 我们现在正在通过释放运行图像识别的代码来进行下一步<br>在我们最新的模型上，Inception-v3。</p>
<p>Inception-v3针对ImageNet大型视觉识别挑战进行了培训 使用2012年的数据。这是计算机视觉的标准任务， 模型试图分类整个<br>图像分为1000个类，如“斑马”，“达尔马提亚”和“洗碗机”。 例如，下面是AlexNet对一些图像进行分类的结果：</p>
<p><img src="https://www.tensorflow.org/images/AlexClassification.png" alt=""></p>
<p>为了比较模型，我们检查模型无法预测的频率 正确的答案作为他们的前5个猜测之一 - 被称为“前5的错误率”。<br>AlexNet通过在2012年设置15.3％的前5个错误率来实现 验证数据集;初始（GoogLeNet）达到6.67％; BN-Inception-<br>v2达到4.9％; Inception-v3达到3.46％。</p>
<blockquote>
<p>人类在ImageNet挑战上的表现如何？有一个博客帖子 试图衡量他自己的表现的Andrej Karpathy。他到了 5.1％前5的错误率。</p>
</blockquote>
<p>本教程将教你如何使用Inception-v3。你将学习如何 在Python或C ++中将图像分类为1000个类。我们也将讨论如何<br>从这个模型中提取更高层次的特征，可以重用其他特征 视觉任务。</p>
<p>我们很高兴看到社区将如何使用这个模型。</p>
<h2><span id="与python-api一起使用">与Python API一起使用</span></h2><p><code>classify_image.py</code>从<code>tensorflow.org</code>下载训练有素的型号 当程序第一次运行。你将需要大约200M的可用空间<br>可用在您的硬盘上。</p>
<p>首先克隆GitHub的TensorFlow模型回购。运行以下命令：</p>
<pre><code>cd models/tutorials/image/imagenet
python classify_image.py
</code></pre><p>上述命令将分类提供的一只熊猫的图像。</p>
<p><img src="https://www.tensorflow.org/images/cropped_panda.jpg" alt=""></p>
<p>如果模型正确运行，脚本将产生以下输出：</p>
<pre><code>giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.88493)
indri, indris, Indri indri, Indri brevicaudatus (score = 0.00878)
lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00317)
custard apple (score = 0.00149)
earthstar (score = 0.00127)
</code></pre><p>如果您想提供其他JPEG图像，可以通过编辑来完成 <code>--image_file</code>的说法。</p>
<blockquote>
<p>如果您将模型数据下载到不同的目录中，您 需要将<code>--model_dir</code>指向所使用的目录。</p>
</blockquote>
<h2><span id="与c-api一起使用">与C ++ API一起使用</span></h2><p>您可以在C ++中运行相同的Inception-v3模型以用于生产 环境。您可以下载包含定义的GraphDef的存档<br>像这样的模型（从TensorFlow的根目录运行 库）：</p>
<pre><code>curl -L &quot;https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz&quot; |
  tar -C tensorflow/examples/label_image/data -xz
</code></pre><p>接下来，我们需要编译C ++二进制文件，其中包含加载和运行图形的代码。 如果你跟着 说明下载TensorFlow的源码安装<br>对于您的平台，您应该可以通过构建示例 从你的shell终端运行这个命令：</p>
<pre><code>bazel build tensorflow/examples/label_image/...
</code></pre><p>这应该创建一个二进制可执行文件，然后可以像这样运行：</p>
<pre><code>bazel-bin/tensorflow/examples/label_image/label_image
</code></pre><p>这使用框架附带的默认示例图像，应该 输出类似这样的东西：</p>
<pre><code>I tensorflow/examples/label_image/main.cc:206] military uniform (653): 0.834306
I tensorflow/examples/label_image/main.cc:206] mortarboard (668): 0.0218692
I tensorflow/examples/label_image/main.cc:206] academic gown (401): 0.0103579
I tensorflow/examples/label_image/main.cc:206] pickelhaube (716): 0.00800814
I tensorflow/examples/label_image/main.cc:206] bulletproof vest (466): 0.00535088
</code></pre><p>在这种情况下，我们使用的默认图像 格雷斯霍珀将军，你可以 看到网络正确识别她身着军装，身高很高 得分为0.8。</p>
<p><img src="https://www.tensorflow.org/images/grace_hopper.jpg" alt=""></p>
<p>接下来，通过提供–image =参数（例如，</p>
<pre><code>bazel-bin/tensorflow/examples/label_image/label_image --image=my_image.png
</code></pre><p>如果你看<code>tensorflow/examples/label_image/main.cc</code> 文件，你可以找到<br>怎么运行的。我们希望这个代码能帮助你将TensorFlow集成到 您自己的应用程序，所以我们将一步一步通过主要功能：</p>
<p>命令行标志控制文件的加载位置，以及输入图像的属性。 该模型预计将得到正方形299x299 RGB图像，所以那些是<code>input_width</code><br>和<code>input_height</code>标志。我们还需要从整数中缩放像素值 在图形操作的浮点值的0到255之间。<br>我们用<code>input_mean</code>和<code>input_std</code>标志来控制缩放比例：我们先减去<br><code>input_mean</code>从每个像素值，然后用<code>input_std</code>分开。</p>
<p>这些值可能看起来有些神奇，但是它们只是被定义的 基于他/她想用作输入图像的原始模型作者 训练。如果你有自己训练过的图表，你只需要<br>调整值以匹配您在训练过程中使用的任何值。</p>
<p>你可以看到它们是如何应用到图像中的 <code>ReadTensorFromImageFile()</code> 功能。</p>
<pre><code>// Given an image file name, read in the data, try to decode it as an image,
// resize it to the requested size, and then scale the values as desired.
Status ReadTensorFromImageFile(string file_name, const int input_height,
                               const int input_width, const float input_mean,
                               const float input_std,
                               std::vector&lt;Tensor&gt;* out_tensors) {
  tensorflow::GraphDefBuilder b;
</code></pre><p>我们首先创建一个<code>GraphDefBuilder</code>，这是一个我们可以使用的对象 指定要运行或加载的模型。</p>
<pre><code>string input_name = &quot;file_reader&quot;;
string output_name = &quot;normalized&quot;;
tensorflow::Node* file_reader =
    tensorflow::ops::ReadFile(tensorflow::ops::Const(file_name, b.opts()),
                              b.opts().WithName(input_name));
</code></pre><p>然后，我们开始为我们想要运行的小模型创建节点 加载，调整大小和缩放像素值，以获得主模型的结果<br>希望成为其投入。我们创建的第一个节点只是一个<code>Const</code>操作，它拥有一个 张量与我们想要加载的图像的文件名称。然后通过了 首先输入到<code>ReadFile</code><br>op。您可能会注意到我们正在通过<code>b.opts()</code>作为最后一个 所有创建函数的参数。参数确保节点被添加到<br>模型定义在<code>GraphDefBuilder</code>中。我们也命名<code>ReadFile</code> 通过拨打<code>WithName()</code>呼叫<code>b.opts()</code>。这给节点一个名字，<br>这是不必要的，因为如果你不这样做，会自动分配一个名字 这样做，但它确实使调试更容易一些。</p>
<pre><code>// Now try to figure out what kind of file it is and decode it.
const int wanted_channels = 3;
tensorflow::Node* image_reader;
if (tensorflow::StringPiece(file_name).ends_with(&quot;.png&quot;)) {
  image_reader = tensorflow::ops::DecodePng(
      file_reader,
      b.opts().WithAttr(&quot;channels&quot;, wanted_channels).WithName(&quot;png_reader&quot;));
} else {
  // Assume if it&apos;s not a PNG then it must be a JPEG.
  image_reader = tensorflow::ops::DecodeJpeg(
      file_reader,
      b.opts().WithAttr(&quot;channels&quot;, wanted_channels).WithName(&quot;jpeg_reader&quot;));
}
// Now cast the image data to float so we can do normal math on it.
tensorflow::Node* float_caster = tensorflow::ops::Cast(
    image_reader, tensorflow::DT_FLOAT, b.opts().WithName(&quot;float_caster&quot;));
// The convention for image ops in TensorFlow is that all images are expected
// to be in batches, so that they&apos;re four-dimensional arrays with indices of
// [batch, height, width, channel]. Because we only have a single image, we
// have to add a batch dimension of 1 to the start with ExpandDims().
tensorflow::Node* dims_expander = tensorflow::ops::ExpandDims(
    float_caster, tensorflow::ops::Const(0, b.opts()), b.opts());
// Bilinearly resize the image to fit the required dimensions.
tensorflow::Node* resized = tensorflow::ops::ResizeBilinear(
    dims_expander, tensorflow::ops::Const({input_height, input_width},
                                          b.opts().WithName(&quot;size&quot;)),
    b.opts());
// Subtract the mean and divide by the scale.
tensorflow::ops::Div(
    tensorflow::ops::Sub(
        resized, tensorflow::ops::Const({input_mean}, b.opts()), b.opts()),
    tensorflow::ops::Const({input_std}, b.opts()),
    b.opts().WithName(output_name));
</code></pre><p>然后，我们继续添加更多的节点，将文件数据解码为图像，来投射 整数转换为浮点值，重新调整它，然后最后运行 对像素值进行减法和除法运算。</p>
<pre><code>// This runs the GraphDef network definition that we&apos;ve just constructed, and
// returns the results in the output tensor.
tensorflow::GraphDef graph;
TF_RETURN_IF_ERROR(b.ToGraphDef(&amp;graph));
</code></pre><p>在这个结尾我们有 存储在b变量中的模型定义，我们将其转化为完整的图形 用<code>ToGraphDef()</code>功能定义。</p>
<pre><code>std::unique_ptr&lt;tensorflow::Session&gt; session(
    tensorflow::NewSession(tensorflow::SessionOptions()));
TF_RETURN_IF_ERROR(session-&gt;Create(graph));
TF_RETURN_IF_ERROR(session-&gt;Run({}, {output_name}, {}, out_tensors));
return Status::OK();
</code></pre><p>然后我们创建一个<code>tf.Session</code> 对象，这是实际运行图形的接口，并运行它， 指定我们想要从哪个节点获得输出，以及在哪里放置 输出数据。</p>
<p>这给了我们一个<code>Tensor</code>对象的矢量，在这种情况下，我们知道只会是一个 单个对象很长。您可以将<code>Tensor</code>视为一个多维数组<br>上下文，它拥有299像素高，299像素宽，3通道图像作为浮点 值。如果你已经在你的产品中有你自己的图像处理框架，那么你 应该可以使用它，只要你应用相同的转换<br>将图像输入主图之前</p>
<p>这是一个在C ++中动态创建一个小型TensorFlow图形的简单例子， 但是对于预先训练的Inception模型，我们希望从中加载更大的定义<br>一份文件。您可以在<code>LoadGraph()</code>功能中看到我们如何做到这一点。</p>
<pre><code>// Reads a model graph definition from disk, and creates a session object you
// can use to run it.
Status LoadGraph(string graph_file_name,
                 std::unique_ptr&lt;tensorflow::Session&gt;* session) {
  tensorflow::GraphDef graph_def;
  Status load_graph_status =
      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &amp;graph_def);
  if (!load_graph_status.ok()) {
    return tensorflow::errors::NotFound(&quot;Failed to load compute graph at &apos;&quot;,
                                        graph_file_name, &quot;&apos;&quot;);
  }
</code></pre><p>如果你已经看过图像加载代码，很多的术语应该看起来很熟悉。而不是<br>使用<code>GraphDefBuilder</code>生成一个<code>GraphDef</code>对象，我们加载一个protobuf文件 直接包含<code>GraphDef</code>。</p>
<pre><code>  session-&gt;reset(tensorflow::NewSession(tensorflow::SessionOptions()));
  Status session_create_status = (*session)-&gt;Create(graph_def);
  if (!session_create_status.ok()) {
    return session_create_status;
  }
  return Status::OK();
}
</code></pre><p>然后我们从<code>GraphDef</code>创建一个Session对象 把它传回给调用者，以便以后可以运行它。</p>
<p>除了在这种情况下，<code>GetTopLabels()</code>功能与图像加载非常相似 我们想要运行主图的结果，并把它变成一个排序列表<br>的得分最高的标签。就像图像加载器一样，它创建一个 <code>GraphDefBuilder</code>，增加了一些节点，然后运行短图得到一个<br>输出张量对。在这种情况下，他们代表排序的分数和指数 最高的结果的位置。</p>
<pre><code>// Analyzes the output of the Inception graph to retrieve the highest scores and
// their positions in the tensor, which correspond to categories.
Status GetTopLabels(const std::vector&lt;Tensor&gt;&amp; outputs, int how_many_labels,
                    Tensor* indices, Tensor* scores) {
  tensorflow::GraphDefBuilder b;
  string output_name = &quot;top_k&quot;;
  tensorflow::ops::TopK(tensorflow::ops::Const(outputs[0], b.opts()),
                        how_many_labels, b.opts().WithName(output_name));
  // This runs the GraphDef network definition that we&apos;ve just constructed, and
  // returns the results in the output tensors.
  tensorflow::GraphDef graph;
  TF_RETURN_IF_ERROR(b.ToGraphDef(&amp;graph));
  std::unique_ptr&lt;tensorflow::Session&gt; session(
      tensorflow::NewSession(tensorflow::SessionOptions()));
  TF_RETURN_IF_ERROR(session-&gt;Create(graph));
  // The TopK node returns two outputs, the scores and their original indices,
  // so we have to append :0 and :1 to specify them both.
  std::vector&lt;Tensor&gt; out_tensors;
  TF_RETURN_IF_ERROR(session-&gt;Run({}, {output_name + &quot;:0&quot;, output_name + &quot;:1&quot;},
                                  {}, &amp;out_tensors));
  *scores = out_tensors[0];
  *indices = out_tensors[1];
  return Status::OK();
</code></pre><p><code>PrintTopLabels()</code>功能将这些分类结果打印出来并打印出来 友好的方式。 <code>CheckTopLabel()</code>功能非常相似，但只是确保<br>为了调试目的，最高标签是我们期望的。</p>
<p>最后，<code>main()</code> 把所有这些电话联系在一起。</p>
<pre><code>int main(int argc, char* argv[]) {
  // We need to call this to set up global state for TensorFlow.
  tensorflow::port::InitMain(argv[0], &amp;argc, &amp;argv);
  Status s = tensorflow::ParseCommandLineFlags(&amp;argc, argv);
  if (!s.ok()) {
    LOG(ERROR) &lt;&lt; &quot;Error parsing command line flags: &quot; &lt;&lt; s.ToString();
    return -1;
  }

  // First we load and initialize the model.
  std::unique_ptr&lt;tensorflow::Session&gt; session;
  string graph_path = tensorflow::io::JoinPath(FLAGS_root_dir, FLAGS_graph);
  Status load_graph_status = LoadGraph(graph_path, &amp;session);
  if (!load_graph_status.ok()) {
    LOG(ERROR) &lt;&lt; load_graph_status;
    return -1;
  }
</code></pre><p>我们加载主图。</p>
<pre><code>// Get the image from disk as a float array of numbers, resized and normalized
// to the specifications the main graph expects.
std::vector&lt;Tensor&gt; resized_tensors;
string image_path = tensorflow::io::JoinPath(FLAGS_root_dir, FLAGS_image);
Status read_tensor_status = ReadTensorFromImageFile(
    image_path, FLAGS_input_height, FLAGS_input_width, FLAGS_input_mean,
    FLAGS_input_std, &amp;resized_tensors);
if (!read_tensor_status.ok()) {
  LOG(ERROR) &lt;&lt; read_tensor_status;
  return -1;
}
const Tensor&amp; resized_tensor = resized_tensors[0];
</code></pre><p>加载，调整大小和处理输入图像。</p>
<pre><code>// Actually run the image through the model.
std::vector&lt;Tensor&gt; outputs;
Status run_status = session-&gt;Run({ {FLAGS_input_layer, resized_tensor}},
                                 {FLAGS_output_layer}, {}, &amp;outputs);
if (!run_status.ok()) {
  LOG(ERROR) &lt;&lt; &quot;Running model failed: &quot; &lt;&lt; run_status;
  return -1;
}
</code></pre><p>在这里，我们运行加载的图形作为输入图像。</p>
<pre><code>// This is for automated testing to make sure we get the expected result with
// the default settings. We know that label 866 (military uniform) should be
// the top label for the Admiral Hopper image.
if (FLAGS_self_test) {
  bool expected_matches;
  Status check_status = CheckTopLabel(outputs, 866, &amp;expected_matches);
  if (!check_status.ok()) {
    LOG(ERROR) &lt;&lt; &quot;Running check failed: &quot; &lt;&lt; check_status;
    return -1;
  }
  if (!expected_matches) {
    LOG(ERROR) &lt;&lt; &quot;Self-test failed!&quot;;
    return -1;
  }
}
</code></pre><p>出于测试目的，我们可以检查以确保我们得到我们期望的输出。</p>
<pre><code>// Do something interesting with the results we&apos;ve generated.
Status print_status = PrintTopLabels(outputs, FLAGS_labels);
</code></pre><p>最后我们打印我们找到的标签。</p>
<pre><code>if (!print_status.ok()) {
  LOG(ERROR) &lt;&lt; &quot;Running print failed: &quot; &lt;&lt; print_status;
  return -1;
}
</code></pre><p>这里的错误处理是使用TensorFlow的<code>Status</code> 对象，这是非常方便的，因为它让你知道是否有任何错误<br>与<code>ok()</code>检查器一起发生，然后可以打印出来以提供可读的错误 信息。</p>
<p>在这种情况下，我们正在展示对象识别，但你应该能够 在自己找到或训练过的其他模型上使用非常类似的代码 所有<br>种类的域名。我们希望这个小例子给你一些关于如何使用的想法 您自己的产品中的TensorFlow。</p>
<blockquote>
<p>练习：转移学习是一个想法，如果你知道如何解决一个好的任务，你 应该能够把一些理解转移到解决相关的问题 问题。执行转移学习的一种方法是删除最终的<br>网络的分类层和提取 CNN的倒数第二层，在这种情况下是2048维向量。 在how-to部分有一个指导。</p>
</blockquote>
<h2><span id="学习更多的资源">学习更多的资源</span></h2><p>一般来说，要学习神经网络，Michael Nielsen的 免费在线书 是一个很好的资源。对于卷积神经网络来说， 克里斯·奥拉有一些 漂亮的博客文章，<br>而Michael Nielsen的书有一个 伟大的篇章 覆盖他们。</p>
<p>要了解有关执行卷积神经网络的更多信息，可以跳转 到TensorFlow深度卷积网络教程， 或者用我们的轻轻一点开始 ML初学者或ML专家<br>MNIST初学者教程。最后，如果你想加快研究速度 在这方面，你可以 阅读本教程中引用的所有论文的最新工作。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/image_retraining/" title="如何重新开始新类别的最后一层" itemprop="url">如何重新开始新类别的最后一层</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="如何重新开始新类别的最后一层">如何重新开始新类别的最后一层</span></h1><p>现代物体识别模型具有数百万个参数，可能需要数周时间 充分训练。转移学习是一个捷径，很多这个技术 通过为ImageNet等一系列类别提供完整的模型<br>从现有的新班级重新训练。在这个例子中，我们将会 重新训练最后一层，同时保持所有其他层不变。 有关可以看到的方法的更多信息 本文关于Decaf。</p>
<p>虽然这不如完整的训练，但这是非常有效的 对于许多应用程序，并可以运行在一个小小的三十分钟 笔记本电脑，而不需要GPU。本教程将向您展示如何运行<br>在你自己的图像上的脚本示例，并将解释你的一些选项 帮助控制培训过程。</p>
<p>注意：本教程的这个版本主要使用bazel。一个bazel免费版本是 也提供 作为codelab。</p>
<h2><span id="对花的培训">对花的培训</span></h2><p><img src="https://www.tensorflow.org/images/daisies.jpg" alt="Daisies by Kelly Sikkema"></p>
<p>图片由凯利Sikkema</p>
<p>在开始任何培训之前，您需要一组图像来教授网络 关于你想要认识的新课程。有一个后面的部分 解释如何准备自己的图像，但为了让我们轻松创建一个<br>最初使用的创作共享许可花照片档案。为了得到 花卉照片集，运行这些命令：</p>
<pre><code>cd ~
curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
tar xzf flower_photos.tgz
</code></pre><p>一旦你有了这些图像，你可以从根本上像这样建立一个修剪器 您的TensorFlow源代码目录：</p>
<pre><code>bazel build tensorflow/examples/image_retraining:retrain
</code></pre><p>如果你有一台支持的机器 AVX指令集 （在过去几年中生产的x86 CPU中很常见），您可以改善运行<br>通过建筑物的再培训速度，就像这样（在<code>configure</code>中选择合适的选项后）：</p>
<pre><code>bazel build --config opt tensorflow/examples/image_retraining:retrain
</code></pre><p>然后可以这样运行retrainer：</p>
<pre><code>bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos
</code></pre><p>该脚本加载预先训练的Inception v3模型，删除旧的顶层， 并在您下载的花卉照片上训练一个新的。没有一朵花<br>物种是在完整的网络培训的原始ImageNet类。 转移学习的魔力是经过训练的较低层次 区分一些对象可以重复用于许多识别任务 没有任何改动。</p>
<h2><span id="瓶颈">瓶颈</span></h2><p>脚本可能需要30分钟或更长时间才能完成，具体取决于速度 的机器。第一阶段分析磁盘上的所有图像并进行计算 他们每个人的瓶颈值。 “瓶颈”是一个非正式的术语<br>经常用于实际上最终输出层之前的层 分类。这个倒数第二层已被训练输出一组 值足以让分类器用来区分全部 它被要求承认的课程。这意味着它必须是一个有意义的<br>和图像的紧凑总结，因为它必须包含足够的信息 为分类器在一个非常小的值集合中做出一个好的选择。该 原因是我们的最后一层再培训可以在新班上工作，事实证明<br>需要的信息种类区分了所有1000个类别 ImageNet通常也可以用来区分新类型的对象。</p>
<p>因为在训练和计算过程中每个图像都被多次重复使用 每个瓶颈都需要花费大量的时间，这会加快速度 将这些瓶颈值缓存在磁盘上，这样就不必重复<br>重新计算。默认情况下，它们存储在<code>/tmp/bottleneck</code>目录中 如果你重新运行脚本，他们将被重用，所以你不必等待 部分再次。</p>
<h2><span id="训练">训练</span></h2><p>一旦瓶颈完成，最后一层的实际培训 网络开始。你会看到一系列的步骤输出，每个输出显示训练 准确性，验证准确性和交叉熵。训练的准确性<br>显示当前培训批次中使用的图像的百分比 标有正确的类。验证的准确性是a的精度 随机选择一组不同的图像。关键的区别是 训练的准确性是基于网络已经能够的图像<br>学习如此，网络可以适应训练数据中的噪声。一个 衡量网络性能的真正措施就是衡量其性能 一个没有包含在训练数据中的数据集 - 这是由<br>验证准确性。如果列车的准确度高但验证的准确性 仍然很低，这意味着网络过度配合和记忆特别 训练图像中的特征通常没有帮助。交叉<br>熵是一个损失函数，它可以窥见学习的好坏 进程正在进行中。培训的目标是使损失如此小 可能的，所以你可以通过密切关注学习是否有效<br>损失是否继续下降，忽略了短期的噪音。</p>
<p>默认情况下，这个脚本将运行4000个训练步骤。每一步选择十个 随机从训练集中找到图像，从缓存中找到它们的瓶颈， 并将它们馈送到最后一层以获得预测。那些预言是<br>然后与实际标签进行比较以更新最终图层的权重 通过反向传播过程。随着过程的继续，你应该看到 报告的准确性提高，毕竟所有的步骤都完成了，最后的测试<br>准确性评估运行在与训练分开的一组图像上 并验证图片。这个测试评估是如何的最好的估计 训练好的模型将在分类任务上执行。你应该看到一个<br>准确度在90％到95％之间，但确切的数值会随着运行而变化 因为在训练过程中存在随机性。这个数字是基于 测试集中图像的百分比是正确的标签 模型完成训练后。</p>
<h2><span id="用tensorboard可视化再训练">用TensorBoard可视化再训练</span></h2><p>该脚本包含TensorBoard摘要，使得更容易理解，调试和优化再培训。例如，您可以将图形和统计信息可视化，例如在训练期间权重或准确性如何变化。</p>
<p>要启动TensorBoard，请在再培训期间或之后运行此命令：</p>
<pre><code>tensorboard --logdir /tmp/retrain_logs
</code></pre><p>一旦TensorBoard正在运行，浏览您的网络浏览器到<code>localhost:6006</code>来查看TensorBoard。</p>
<p>该脚本默认将TensorBoard摘要记录到<code>/tmp/retrain_logs</code>。您可以使用<code>--summaries_dir</code>标志更改目录。</p>
<p>TensorBoard的GitHub有更多关于TensorBoard使用的信息，包括提示和技巧以及调试信息。</p>
<h2><span id="使用再培训模式">使用再培训模式</span></h2><p>该脚本将写出一个版本的Inception v3网络与决赛 层重新训练到您的类别/tmp/output_graph.pb和一个文本文件<br>包含标签到/tmp/output_labels.txt。这些都是在一个格式 C ++和Python图像分类的例子<br>可以阅读，所以你可以立即开始使用你的新模型。既然你已经 替换顶层，您将需要在脚本中指定新名称<br>如果您使用的是label_image，则使用<code>--output_layer=final_result</code>标志。</p>
<p>下面是一个如何构建和运行label_image示例的例子 再培训图表：</p>
<pre><code>bazel build tensorflow/examples/image_retraining:label_image &amp;&amp; \
bazel-bin/tensorflow/examples/image_retraining/label_image \
--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \
--output_layer=final_result:0 \
--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg
</code></pre><p>你应该看到一个花标签列表，在大多数情况下菊花在上面 （虽然每个重新训练的模型可能略有不同）。你可以更换<br><code>--image</code>参数与您自己的图像来尝试这些，并使用C ++代码 作为与您自己的应用程序集成的模板。</p>
<p>如果你想在自己的Python程序中使用再培训的模型，那么 以上 <code>label_image</code>脚本 是一个合理的出发点。</p>
<p>如果您发现默认的Inception v3模型对您来说太大或太慢 应用程序，看看其他模型体系结构部分 下面的选项可以加快你的网络。</p>
<h2><span id="根据自己的类别进行培训">根据自己的类别进行培训</span></h2><p>如果你已经设法让脚本在花图像上工作，你 可以开始看教学，以识别你关心的类别。 从理论上讲，你需要做的就是把它指向一组子文件夹，每个文件夹都被命名<br>在你的一个类别之后，只包含那个类别的图片。如果 你这样做，并传递子目录的根文件夹作为参数 <code>--image_dir</code>，剧本应该像对待花一样训练。</p>
<p>以下是鲜花档案的文件夹结构的样子，给你 以及脚本所寻找布局的例子：</p>
<p><img src="https://www.tensorflow.org/images/folder_structure.png" alt="Folder Structure"></p>
<p>在实践中，可能需要一些工作来获得所需的准确性。我会尝试 指导您解决下面可能遇到的一些常见问题。</p>
<h2><span id="创建一组训练图像">创建一组训练图像</span></h2><p>从第一个开始就是看你收集的图像，因为 我们在培训中看到的最常见的问题来自所馈入的数据。</p>
<p>为了使训练工作顺利，你至少应该收集一百张照片 你想认识的对象。你能收集的越多，越好 你的训练模型的准确性可能是。你还需要确保<br>照片是您的应用程序实际上的一个很好的代表 遭遇。例如，如果你把所有的照片都放在室内的墙上 而你的用户正试图识别户外的物体，你可能不会看到 当你部署好结果。</p>
<p>另一个要避免的缺陷是学习过程会随时随地进行 标记的图像彼此有共同的，如果你不是 小心这可能是没有用的东西。例如，如果你拍照<br>一种是蓝色的房间，另一种是绿色的，然后是模型 最终将基于其背景颜色的预测，而不是 你真正关心的对象。为了避免这种情况，请尝试使用as拍照<br>尽可能地在不同的时间和不同的情况下尽可能地处理各种情况 设备。如果你想知道更多关于这个问题，你可以阅读关于这个问题 经典（也可能是杜撰的） 坦克识别问题。</p>
<p>您可能还想考虑您使用的类别。这可能是值得的 将涵盖许多不同物理形式的大类分成几类 较小的那些在视觉上更明显。例如，而不是“车辆”<br>你可以使用“汽车”，“摩托车”和“卡车”。这也值得思考 无论你是“封闭的世界”还是“开放的世界”问题。在封闭的世界里，<br>你将被要求分类的唯一的东西是你的对象类 知道关于。这可能适用于您认识用户的植物识别应用程序 很可能会拍摄一朵花，所以你所要做的就是决定<br>哪些物种。相比之下，漫游机器人可能会看到各种各样的不同 通过它的相机，当它在世界各地漫游。在这种情况下，你会的<br>要分类器报告，如果它不知道它看到了什么。这可以 很难做好，但如果你经常收集大量典型的“背景” 没有相关对象的照片，您可以将它们添加到额外的“未知”<br>在你的图像文件夹中的类。</p>
<p>这也是值得检查，以确保所有的图像被标记 正确。为了我们的目的，用户生成的标签通常是不可靠的 例如使用#daisy为一个叫Daisy的人拍照。如果你经历过<br>你的图像，并剔除它可以做你的整体奇迹的任何错误 准确性。</p>
<h2><span id="培训步骤">培训步骤</span></h2><p>如果您对自己的图片感到满意，可以考虑改善效果 通过改变学习过程的细节。最简单的尝试是<br><code>--how_many_training_steps</code>。这默认为4,000，但如果你增加到 8000将训练两倍的时间。准确度提高的速度<br>减慢你训练的时间，并在某个时刻完全停止，但你 可以试验一下，看看你什么时候达到你的模型的极限。</p>
<h2><span id="扭曲">扭曲</span></h2><p>改善图像训练结果的常用方法是变形， 裁剪或以随机方式使训练输入增亮。这有 有利于扩大培训数据的有效规模 相同图像的可能变化，并倾向于帮助网络学习<br>应付所有在现实生活中会出现的扭曲现象 分类。在我们的脚本中使这些扭曲最大的缺点 是瓶颈缓存不再有用，因为输入图像永远不会<br>完全重用。这意味着培训过程需要更长的时间，所以我 建议尝试这种方式来微调你的模型，一旦你有一个 你相当满意。</p>
<p>您可以通过<code>--random_crop</code>，<code>--random_scale</code>和 <code>--random_brightness</code>到脚本。这些都是百分比值<br>控制每个图像应用了多少失真。它的 合理的，从他们每个人的5或10的值开始，然后进行实验 看看他们中的哪些人帮助您的应用程序。<br><code>--flip_left_right</code>将会 随机镜像一半的水平，这是有道理的，只要 这些反转可能发生在您的应用程序中。例如它<br>如果你试图识别字母，就不是一个好主意，因为翻转 他们破坏了他们的意思。</p>
<h2><span id="超参数">超参数</span></h2><p>还有其他几个参数可以尝试调整，看看他们是否有帮助 你的结果。 <code>--learning_rate</code>控制更新的大小 训练期间的最后一层。直观地说，如果这比学习小<br>将需要更长的时间，但最终可能会帮助整体精度。那不是 总是这样，所以你需要仔细试验，看看有什么作品 为你的情况。<br><code>--train_batch_size</code>控制检查的图像数量 在一个训练步骤中，并且因为学习速率是每批应用的 如果您有更大的批次以获得相同的整体，则需要减少它<br>影响。</p>
<h2><span id="培训验证和测试集">培训，验证和测试集</span></h2><p>当您将脚本指向文件夹时，脚本将在其中进行引导 的图像被分成三个不同的集合。最大的是通常 训练集是培训期间输入网络的所有图像，<br>结果用于更新模型的权重。你可能想知道我们为什么 不要使用所有的图像进行培训？我们正在做的一个很大的潜在问题 机器学习是我们的模型可能只是记住不相关的细节<br>培训图像拿出正确的答案。例如，你可以 想象一下，在每张照片背景中记住一个图案的网络 显示，并使用它来匹配标签与对象。它可以产生好的<br>结果在训练期间之前看到的所有图像，但是然后失败 图像，因为它没有学习对象的一般特征，只是 记住训练图像的不重要的细节。</p>
<p>这个问题被称为overfitting，为了避免它，我们保留了一些我们的数据 脱离训练过程，使模型不能记住它们。我们然后使用<br>这些图像作为检查，以确保过度配合不发生，因为如果 我们看到他们很好的准确性，这是一个好的迹象，网络不是过度配合。该<br>通常的分割是将80％的图像放入主训练集中，保持10％ 除了在培训期间经常作为验证运行，然后最终有10％ 作为一个测试集来预测真实世界的性能，使用较少<br>的分类器。这些比例可以使用 <code>--testing_percentage</code>和<code>--validation_percentage</code>标志。一般来说<br>你应该能够将这些值保留在默认值，因为你不会 通常找到任何有利于培训来调整他们。</p>
<p>请注意，该脚本使用图像文件名（而不是一个完全随机的 功能）在训练，验证和测试集之间划分图像。 这样做是为了确保图像不会在训练和测试之间移动<br>设置不同的运行，因为这可能是一个问题，如果图像已经 用于训练模型随后用于验证集合中。</p>
<p>您可能会注意到验证准确性在迭代中波动。许多 这种波动起因于验证随机子集的事实 每个验证准确度测量选择一组。波动可以<br>大大减少了，而以一些训练时间的增加为代价，通过选择 <code>--validation_batch_size=-1</code>，它使用整个验证集 准确性计算。</p>
<p>一旦培训完成，您可能会发现审查错误分类的洞察力 测试集中的图像。这可以通过添加标志来完成<br><code>--print_misclassified_test_images</code>。这可能会帮助你得到一个感觉 图像的类型最容易混淆的模型，以及哪些类别<br>最难区分。例如，你可能会发现一些 特定类别的子类型，或一些不寻常的照片角度 很难确定，这可能会鼓励你添加更多的训练图像<br>那个亚型。通常情况下，检查错误的图像也可以指向 输入数据集中的错误，如错误标记，低质量或含糊不清 图片。但是，通常应该避免在个别错误中加入点<br>测试集，因为它们很可能仅仅反映更为普遍的问题 （更大的）训练集。</p>
<h2><span id="其他模型架构">其他模型架构</span></h2><p>默认情况下，脚本使用Inception v3模型的预训练版本 建筑。这是一个很好的开始，因为它提供了高精度 结果，但如果你打算在移动设备或其他部署你的模型<br>资源受限的环境中，您可能需要权衡一些准确性 对于更小的文件或更快的速度。为了帮助，这个 retrain.py脚本<br>在Mobilenet架构上支持32种不同的变体。</p>
<p>这些比Inception v3精确一点，但可能会导致很多 较小的文件大小（小于一兆字节），可以快很多倍<br>跑步。用这些模型之一进行训练，通过<code>--architecture</code>标志， 例如：</p>
<pre><code>python tensorflow/examples/image_retraining/retrain.py \
    --image_dir ~/flower_photos --architecture mobilenet_0.25_128_quantized
</code></pre><p>这将在<code>/tmp/output_graph.pb</code>中创建一个941KB的模型文件，占25％<br>完整的Mobilenet的参数，采取128x128大小的输入图像，并与 它的权重量化到磁盘上的八位。你可以选择’1.0’，’0.75’，<br>‘0.50’或’0.25’来控制权重参数的数量等等文件 大小（在一定程度上是速度），“224”，“192”，“160”或“128”<br>图像大小，更小的尺寸提供更快的速度，并可选 在最后“_quantized”来指示文件是否应该包含8位或 32位浮点权重。</p>
<p>速度和尺寸的优势当然会损失精度，但对于许多人来说 这个目的并不重要。他们也可以有所抵消，改善 训练数据。例如，扭曲训练使我能够达到80％以上<br>即使使用上述的0.25 / 128 /量化图，花数据集也是精确的。</p>
<p>如果你打算在label_image中使用Mobilenet模型或者你自己的模型 程序，您需要将指定大小的图像转换为<br>将范围浮动到“输入”张量中。通常24位图像在范围内 [0,255]，你必须将它们转换为[-1,1]所预期的浮点范围 型号为<code>(image -
128.)/128.</code>。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/kernel_methods/" title="用显式核方法改进线性模型" itemprop="url">用显式核方法改进线性模型</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="用显式核方法改进线性模型">用显式核方法改进线性模型</span></h1><p>在本教程中，我们将演示如何组合（显式）的内核方法 线性模型可以大大提高后者的预测质量 而不会显着增加训练和推理时间。不像双重<br>内核方法，显式（原始）内核方法可以很好地扩展 训练数据集在训练/推理时间和方面 内存要求。</p>
<p>目标读者：即使我们提供了概念的高层次概述 与显式内核方法相关，本教程主要针对读者 已经至少有内核方法和支持向量的基本知识<br>机器（SVM）。如果您对内核方法不熟悉，请参阅以下任一内核方法 以下来源介绍：</p>
<p>如果你有一个强大的数学背景： 机器学习中的核心方法 内核方法维基百科页面</p>
<p>目前，TensorFlow仅支持密集特征的显式核心映射; TensorFlow将在稍后的版本中提供对稀疏功能的支持。</p>
<p>本教程使用tf.contrib.learn （TensorFlow的高级机器学习API）我们的ML模型的估计器。<br>如果你不熟悉这个API，tf.estimator快速入门 是一个开始的好地方。我们将使用MNIST数据集。教程包括 以下步骤：</p>
<p>加载并准备MNIST数据进行分类。 构建一个简单的线性模型，对其进行训练，并在评估数据上进行评估。 用线性模型替换线性模型，重新训练和 重新评估。</p>
<h2><span id="加载并准备mnist数据进行分类">加载并准备MNIST数据进行分类</span></h2><p>运行以下实用程序命令加载MNIST数据集：</p>
<pre><code>data = tf.contrib.learn.datasets.mnist.load_mnist()
</code></pre><p>上述方法加载整个MNIST数据集（包含70K个样本）和 将其分为55K，5K和10K样本的训练，验证和测试数据<br>分别。每个拆分包含一个numpy阵列的图像（与形状 [sample_size，784]），另一个用于标签（形状为[sample_size，1]）。在这<br>教程，我们只使用火车和验证分组来训练和评估我们的 模型。</p>
<p>为了将数据提供给tf.contrib.learn估算器，转换是有帮助的 它对张量。为此，我们将使用添加Ops的<code>input function</code><br>TensorFlow图表，当执行时，创建要使用的小批量的张量 下游。有关输入功能的更多背景信息，请检查 用tf.contrib.learn构建输入函数。在这<br>例如，我们将使用除转换之外的<code>tf.train.shuffle_batch</code> Op num数组到Tensors，允许我们指定batch_size和是否<br>每次执行input_fn Ops时随机化输入（随机化 通常在训练期间加速收敛）。完整的代码加载和 准备数据显示在下面的代码片段中。在这个例子中，我们使用<br>256个小批量培训和整个样本（5K条目） 评价。随意尝试不同的批量大小。</p>
<pre><code>import numpy as np
import tensorflow as tf

def get_input_fn(dataset_split, batch_size, capacity=10000, min_after_dequeue=3000):

  def _input_fn():
    images_batch, labels_batch = tf.train.shuffle_batch(
        tensors=[dataset_split.images, dataset_split.labels.astype(np.int32)],
        batch_size=batch_size,
        capacity=capacity,
        min_after_dequeue=min_after_dequeue,
        enqueue_many=True,
        num_threads=4)
    features_map = {&apos;images&apos;: images_batch}
    return features_map, labels_batch

  return _input_fn

data = tf.contrib.learn.datasets.mnist.load_mnist()

train_input_fn = get_input_fn(data.train, batch_size=256)
eval_input_fn = get_input_fn(data.validation, batch_size=5000)
</code></pre><h2><span id="培训一个简单的线性模型">培训一个简单的线性模型</span></h2><p>现在我们可以在MNIST数据集上训练一个线性模型。我们将使用 <code>tf.contrib.learn.LinearClassifier</code>估计器有10个类代表<br>10位数字。输入特征形成一个784维密集的矢量，可以 具体说明如下：</p>
<pre><code>image_column = tf.contrib.layers.real_valued_column(&apos;images&apos;, dimension=784)
</code></pre><p>用于构建，训练和评估LinearClassifier的完整代码 估算器如下：</p>
<pre><code>import time

# Specify the feature(s) to be used by the estimator.
image_column = tf.contrib.layers.real_valued_column(&apos;images&apos;, dimension=784)
estimator = tf.contrib.learn.LinearClassifier(feature_columns=[image_column], n_classes=10)

# Train.
start = time.time()
estimator.fit(input_fn=train_input_fn, steps=2000)
end = time.time()
print(&apos;Elapsed time: {} seconds&apos;.format(end - start))

# Evaluate and report metrics.
eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)
print(eval_metrics)
</code></pre><p>下表总结了评估数据的结果。</p>
<table>
<thead>
<tr>
<th>metric</th>
<th>value  </th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>0.25 to 0.30  </td>
</tr>
<tr>
<td>accuracy</td>
<td>92.5%  </td>
</tr>
<tr>
<td>training time</td>
<td>~25 seconds on my machine  </td>
</tr>
</tbody>
</table>
<p>注意：度量标准会根据各种因素而有所不同。</p>
<p>除了尝试（培训）批量大小和数量 训练步骤，还有其他一些可以调整的参数。 例如，您可以更改用于最小化损失的优化方法 通过显式选择集合中的另一个优化器<br>可用的优化器。 作为一个例子，下面的代码构造了一个LinearClassifier估计器 使用遵循正则化领导者（FTRL）的优化策略<br>具体学习率和L2正则化。</p>
<pre><code>optimizer = tf.train.FtrlOptimizer(learning_rate=5.0, l2_regularization_strength=1.0)
estimator = tf.contrib.learn.LinearClassifier(
    feature_columns=[image_column], n_classes=10, optimizer=optimizer)
</code></pre><p>无论参数的值如何，最大精度都是一个线性模型 可以在这个数据集上达到93％左右。</p>
<h2><span id="使用线性模型的显式核映射">使用线性模型的显式核映射。</span></h2><p>MNIST上线性模型的相对较高的误差（〜7％）表明 输入数据不是线性可分的。我们将使用显式的内核映射 减少分类错误。</p>
<p>直觉：高层次的想法是使用非线性映射来转换 输入空间到另一个特征空间（可能更高的维度） （变换）的特征是（几乎）线性可分的，然后应用一个线性的<br>模型映射的功能。如下图所示：</p>
<p><img src="https://www.tensorflow.org/versions/master/images/kernel_mapping.png" alt=""></p>
<h3><span id="技术细节">技术细节</span></h3><p>在这个例子中，我们将使用随机傅立叶特征，在中引入 “大规模内核机器的随机特性” 由Rahimi和Recht撰写的论文来绘制输入数据。随机傅立叶特征映射a<br>（\ mathbf {x} \ in \ mathbb {R} ^ d \）到\（\ mathbf {x’} \ in \ mathbb {R} ^ D<br>\） 通过以下映射：</p>
<p>$$ RFFM(\cdot): \mathbb{R}^d \to \mathbb{R}^D, \quad RFFM(\mathbf{x}) =<br>\cos(\mathbf{\Omega} \cdot \mathbf{x}+ \mathbf{b}) $$</p>
<p>其中\（\ mathbf {\ Omega} \ in \ mathbb {R} ^ {D \ times d} \）， \（\ mathbf {x} \<br>in \ mathbb {R} ^ d，\）\（\ mathbf {b} \ in \ mathbb {R} ^ D \）和 余弦应用于元素明智的。</p>
<p>在这个例子中，\（\ mathbf {\ Omega} \）和\（\ mathbf {b} \）的条目是 从分布取样，使映射满足以下内容 属性：</p>
<p>$$ RFFM(\mathbf{x})^T \cdot RFFM(\mathbf{y}) \approx e^{-\frac{|\mathbf{x} -<br>\mathbf{y}|^2}{2 \sigma^2}} $$</p>
<p>上面表达式的右边的量被称为RBF（或 高斯）核函数。这个函数是使用最广泛的内核之一 在机器学习中的功能并且隐含地测量不同的， 比原来的尺寸高得多。看到<br>径向基函数内核 更多细节。</p>
<h3><span id="内核分类器">内核分类器</span></h3><p><code>tf.contrib.kernel_methods.KernelLinearClassifier</code>是预包装的<br><code>tf.contrib.learn</code>估算器结合了显式核映射的功能 与线性模型。它的构造函数几乎和 LinearClassifier估计器与附加选项指定的列表<br>明确的内核映射被应用到分类器使用的每个特征。该 以下代码片段演示了如何用LinearClassifier替换 KernelLinearClassifier。</p>
<pre><code># Specify the feature(s) to be used by the estimator. This is identical to the
# code used for the LinearClassifier.
image_column = tf.contrib.layers.real_valued_column(&apos;images&apos;, dimension=784)
optimizer = tf.train.FtrlOptimizer(
   learning_rate=50.0, l2_regularization_strength=0.001)

kernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(
  input_dim=784, output_dim=2000, stddev=5.0, name=&apos;rffm&apos;)
kernel_mappers = {image_column: [kernel_mapper]}
estimator = tf.contrib.kernel_methods.KernelLinearClassifier(
   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)

# Train.
start = time.time()
estimator.fit(input_fn=train_input_fn, steps=2000)
end = time.time()
print(&apos;Elapsed time: {} seconds&apos;.format(end - start))

# Evaluate and report metrics.
eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)
print(eval_metrics)
</code></pre><p>传递给<code>KernelLinearClassifier</code>的唯一附加参数是字典 从feature_columns到要应用于内核映射的列表<br>对应的功能栏。以下几行指示分类器 首先将初始784维图像映射到2000维矢量 随机傅立叶特征，然后学习转换后的线性模型 向量：</p>
<pre><code>kernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(
  input_dim=784, output_dim=2000, stddev=5.0, name=&apos;rffm&apos;)
kernel_mappers = {image_column: [kernel_mapper]}
estimator = tf.contrib.kernel_methods.KernelLinearClassifier(
   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)
</code></pre><p>注意<code>stddev</code>参数。这是标准偏差（\（\ sigma \））的 近似的RBF核和控制中使用的相似性度量 分类。<br><code>stddev</code>通常通过超参数调整来确定。</p>
<p>运行上述代码的结果汇总在下表中。 我们可以通过增加输出尺寸来进一步提高精度 映射和调整标准偏差。</p>
<table>
<thead>
<tr>
<th>metric</th>
<th>value  </th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>0.10  </td>
</tr>
<tr>
<td>accuracy</td>
<td>97%  </td>
</tr>
<tr>
<td>training time</td>
<td>~35 seconds on my machine  </td>
</tr>
</tbody>
</table>
<h3><span id="stddev">STDDEV</span></h3><p>分类质量对stddev的值非常敏感。该 下表显示了分类器在eval数据上的准确性 不同的stddev值。最佳值是stddev = 5.0。注意也是如此<br>小或太高的stddev值可以大大降低的准确性 分类。</p>
<table>
<thead>
<tr>
<th>stddev</th>
<th>eval accuracy  </th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td>0.1362  </td>
</tr>
<tr>
<td>2.0</td>
<td>0.4764  </td>
</tr>
<tr>
<td>4.0</td>
<td>0.9654  </td>
</tr>
<tr>
<td>5.0</td>
<td>0.9766  </td>
</tr>
<tr>
<td>8.0</td>
<td>0.9714  </td>
</tr>
<tr>
<td>16.0</td>
<td>0.8878  </td>
</tr>
</tbody>
</table>
<h3><span id="输出维度">输出维度</span></h3><p>直观地说，映射的输出维数越大， 两个映射向量的内积近似于内核，这通常是 转换为更好的分类准确性。另一种思考方式是 输出维数等于线性模型的权数;该<br>这个尺寸越大，模型的“自由度”就越大。 但是，经过一定的门槛后，产出规模越来越大 准确度很低，同时使训练需要更多的时间。这显示在<br>下面的两幅图描绘了作为一个函数的评估精度 输出维数和训练时间。</p>
<p><img src="https://www.tensorflow.org/versions/master/images/acc_vs_outdim.png" alt="image"><br><img src="https://www.tensorflow.org/versions/master/images/acc-vs-
trn_time.png" alt="image"></p>
<h2><span id="概要">概要</span></h2><p>显式核映射将非线性模型的预测能力与 线性模型的可扩展性。与传统的双核方法不同， 显式的内核方法可以扩展到数百万甚至数亿<br>样本。在使用显式内核映射时，请考虑以下提示：</p>
<p>随机傅立叶特征可以对密集的数据集特别有效 特征。 内核映射的参数通常是数据相关的。模型质量 可以对这些参数非常敏感。使用超参数调整来查找 最佳值。<br>如果你有多个数字特征，把它们连成一个单一的 多维特征并将内核映射应用到串联 向量。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/audio_recognition/" title="简单的音频识别" itemprop="url">简单的音频识别</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="简单的音频识别">简单的音频识别</span></h1><p>本教程将向您展示如何构建一个基本的语音识别网络 承认十个不同的单词。知道真实的言论和知识是很重要的 音频识别系统要复杂得多，但像MNIST这样的图像，<br>应该让你对所涉及的技术有一个基本的了解。一旦你 完成本教程，您将有一个试图分类一秒钟的模型 音频剪辑为沉默，未知单词，“是”，“否”，“上”，“下”，<br>“左”，“右”，“上”，“关”，“停止”或“去”。你也可以把这个 模型并在Android应用程序中运行。</p>
<h2><span id="制备">制备</span></h2><p>你应该确保你已经安装了TensorFlow，并且从脚本开始 下载超过1GB的训练数据，你需要一个良好的互联网连接和<br>您的机器上有足够的可用空间。培训过程本身可能需要几个 小时，所以确保你有一台机器可用的那么久。</p>
<h2><span id="训练">训练</span></h2><p>要开始训练过程，请转到TensorFlow源代码树并运行：</p>
<pre><code>python tensorflow/examples/speech_commands/train.py
</code></pre><p>脚本将从下载语音命令开始 数据集， 它由三十五个人的六万五千个WAVE音频文件组成 话。这些数据由Google收集，并以CC BY许可证的形式发布<br>你可以通过贡献自己的五分钟帮助改善它 语音。档案是 超过1GB，所以这部分可能需要一段时间，但你应该看到进度日志，和<br>一旦它被下载一次，你将不需要再做这一步。</p>
<p>一旦下载完成，你会看到日志信息 喜欢这个：</p>
<pre><code>I0730 16:53:44.766740   55030 train.py:176] Training from step: 1
I0730 16:53:47.289078   55030 train.py:217] Step #1: rate 0.001000, accuracy 7.0%, cross entropy 2.611571
</code></pre><p>这表明初始化过程完成，训练循环已经完成 开始。你会看到它为每个训练步骤输出信息。这是一个 打破它的意思：</p>
<p><code>Step #1</code>显示我们正在进行训练循环的第一步。在这种情况下 总共有18000步，所以你可以看一下步数 以了解它是如何接近完成。</p>
<p><code>rate 0.001000</code>是控制速度的学习速率 网络的重量更新。在这个早期是一个相对较高的数字（0.001），<br>但是对于以后的训练周期来说，它会减少10倍，达到0.0001。</p>
<p><code>accuracy 7.0%</code>是多少类正确预测在这个 训练步骤。这个值往往会波动很多，但是应该增加 平均随着培训的进展。模型输出一个数字数组，一个用于<br>每个标签，每个数字是输入的预测可能性 类。预测的标签是通过选择最高的条目来挑选的 得分了。分数总是在0到1之间，数值越高越好 对结果表示更多的信心。</p>
<p><code>cross entropy 2.611571</code>是我们正在使用的丢失功能的结果 指导培训过程。这是通过比较得到的分数 从当前训练运行到正确标签的分数向量<br>在训练中应该趋于下降。</p>
<p>经过一百步后，你应该看到这样的一行：</p>
<p>I0730 16：54：41.813438 55030 train.py:252]保存到<br>“/tmp/speech_commands_train/conv.ckpt-100”</p>
<p>这节省了目前训练过的检查点文件的权重。如果你的 训练脚本被中断，你可以查找最后保存的检查点和 然后用重新启动脚本<br><code>--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-100</code>作为命令行 从这一点开始。</p>
<h2><span id="混乱矩阵">混乱矩阵</span></h2><p>经过四百步之后，这些信息将被记录下来：</p>
<pre><code>I0730 16:57:38.073667   55030 train.py:243] Confusion Matrix:
 [[258   0   0   0   0   0   0   0   0   0   0   0]
 [  7   6  26  94   7  49   1  15  40   2   0  11]
 [ 10   1 107  80  13  22   0  13  10   1   0   4]
 [  1   3  16 163   6  48   0   5  10   1   0  17]
 [ 15   1  17 114  55  13   0   9  22   5   0   9]
 [  1   1   6  97   3  87   1  12  46   0   0  10]
 [  8   6  86  84  13  24   1   9   9   1   0   6]
 [  9   3  32 112   9  26   1  36  19   0   0   9]
 [  8   2  12  94   9  52   0   6  72   0   0   2]
 [ 16   1  39  74  29  42   0   6  37   9   0   3]
 [ 15   6  17  71  50  37   0   6  32   2   1   9]
 [ 11   1   6 151   5  42   0   8  16   0   0  20]]
</code></pre><p>第一部分是一个混乱 矩阵。至 明白它的意思，你首先需要知道正在使用的标签， 这种情况是“沉默”，“不明”，“是”，“否”，“上”，“下”，“左”<br>“右”，“上”，“关”，“停”和“走”。每列代表一组样本 被预测为每个标签，所以第一列代表所有的 所有被预测为沉默的剪辑，所有那些剪辑<br>预测是未知的话，第三个“是”，等等。</p>
<p>每行代表剪辑的正确的，地面的真相标签。第一行 是所有沉默的剪辑，第二个剪辑是未知的单词， 第三个“是”等</p>
<p>这个矩阵可能比仅仅一个精度分数更有用，因为它 给出了网络正在发生的错误的一个很好的总结。在这个例子中你 可以看到，第一行中的所有条目都是零，除了<br>最初的一个。因为第一行是所有实际上是沉默的剪辑， 这意味着它们中没有一个被错误地标记为单词，所以我们没有 对沉默的负面否定。这显示网络已经变得漂亮了<br>善于区分沉默与言语。</p>
<p>如果我们往下看第一列，我们会看到很多非零值。该 列代表所有被预测为沉默的剪辑，如此积极 第一个单元格外的数字是错误的。这意味着一些真实的剪辑<br>口头上的话实际上被预言是沉默，所以我们确实有相当的一个 很少有误报。</p>
<p>一个完美的模型将产生一个混乱矩阵，其中所有条目都是 零通过中心的对角线。发现偏离 该模式可以帮助你弄清楚模型是如何最容易混淆的<br>一旦你确定了问题，你可以通过添加更多的数据或者解决它们 清理类别。</p>
<h2><span id="验证">验证</span></h2><p>混淆矩阵后，你应该看到这样一行：</p>
<p>步骤400：验证准确度= 26.3％ （N = 3093）</p>
<p>将数据集分成三类是一个很好的做法。最大的 （在这种情况下大约80％的数据）用于训练网络，a 较小的一套（这里10％，被称为“验证”）被保留用于评估<br>在训练期间的准确性，另一组（最后的10％，“测试”）被使用 在训练结束后评估一次的准确性。</p>
<p>这种分裂的原因是网络总会有危险 在训练期间开始记忆他们的输入。通过保持验证集 单独，您可以确保该模型与以前从未见过的数据一起工作。<br>测试集是一个额外的保证，以确保你不只是 一直以适合训练和训练的方式调整你的模型 验证集合，但不是更广泛的输入。</p>
<p>训练脚本自动将数据集分成这三个 类别，上面的日志行显示了运行时模型的准确性 验证集。理想情况下，这应该相当接近培训<br>准确性。如果训练的准确性增加，但验证没有，那就是 一个标志，过度配合正在发生，你的模型只是学习的东西 关于培训片段，而不是泛泛的模式。</p>
<h2><span id="tensorboard">Tensorboard</span></h2><p>想象如何使用Tensorboard进行培训的一个好方法就是使用Tensorboard。通过 默认情况下，脚本将事件保存到/ tmp /<br>retrain_logs，并且可以加载 这些通过运行：</p>
<p><code>tensorboard --logdir /tmp/retrain_logs</code></p>
<p>然后在浏览器中导航到http：// localhost：6006， 你会看到图表和图表显示你的模型的进展。</p>
<p><img src="https://storage.googleapis.com/download.tensorflow.org/example_images/speech_commands_tensorflow.png" alt=""></p>
<h2><span id="培训完成">培训完成</span></h2><p>经过几个小时的训练（取决于你的机器的速度），脚本 应该已经完成​​了所有18000个步骤。这将打印出最后的混乱 矩阵，以及准确性分数，都在测试集上运行。随着<br>默认设置，你应该看到在85％和90％之间的准确性。</p>
<p>因为音频识别功能在移动设备上特别有用，接下来我们将会 将其导出为便于在这些平台上工作的紧凑格式。去做 那就运行这个命令行：</p>
<pre><code>python tensorflow/examples/speech_commands/freeze.py \
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \
--output_file=/tmp/my_frozen_graph.pb
</code></pre><p>冷冻模型创建完成后，您可以使用<code>label_wav.py</code>进行测试 脚本，如下所示：</p>
<pre><code>python tensorflow/examples/speech_commands/label_wav.py \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav
</code></pre><p>这应该打印出三个标签：</p>
<pre><code>left (score = 0.81477)
right (score = 0.14139)
_unknown_ (score = 0.03808)
</code></pre><p>希望“离开”是最高分，因为这是正确的标签，但自从 训练是随机的，它可能不会为您尝试的第一个文件。尝试一些 在同一个文件夹中的其他.wav文件，看看它有多好。</p>
<p>分数在0到1之间，数值越高意味着模型越多 对预测充满信心。</p>
<h2><span id="在android应用程序中运行模型">在Android应用程序中运行模型</span></h2><p>了解这个模型在真实应用程序中的最简单方法是下载 预构建的Android演示 应用 并将它们安装在您的手机上。你会看到’TF<br>Speech’出现在你的应用程序列表中， 打开它就会显示出我们刚刚培训过的动作词语列表 我们的模型，从“是”和“否”开始。一旦你给了应用程序的权限<br>要使用麦克风，你应该可以尝试说出这些话，看看他们 当模型识别其中的一个时，在用户界面中突出显示。</p>
<p>你也可以自己构建这个应用程序，因为它是开源的 作为TensorFlow存储库的一部分提供 github上。 默认情况下，它从一个预训练模型下载<br>tensorflow.org， 但是您可以轻松地将其替换为您已训练的模型 你自己。 如果你这样做，你需要确保主要的常量 SpeechActivity<br>Java源码 文件 像<code>SAMPLE_RATE</code>和<code>SAMPLE_DURATION</code>匹配您所做的任何更改 在训练时默认。你也会看到有一个Java版本的<br>RecognizeCommands 模 这与本教程中的C ++版本非常相似。如果你调整了<br>参数，您也可以在SpeechActivity中更新它们以获得相同的结果 结果与您的服务器测试一样。</p>
<p>演示应用程序根据标签自动更新其UI结果列表 您可以将文本文件复制到您的冻结图形的旁边，这意味着您可以 轻松地尝试不同的模型，而无需做任何代码更改。您<br>将需要更新<code>LABEL_FILENAME</code>和<code>MODEL_FILENAME</code>指向文件 你已经添加如果你改变路径。</p>
<h2><span id="这个模型如何工作">这个模型如何工作？</span></h2><p>本教程中使用的体系结构基于本文中描述的一些体系结构 用于小尺寸关键字的卷积神经网络 斑点。 选择它是因为它比较简单，快速训练，而且容易<br>理解，而不是最先进的技术。有很多不同的 方法来建立神经网络模型来处理音频，包括 经常性网络或扩张 （atrous） 卷积。<br>本教程是基于那种感觉非常好的卷积网络 熟悉任何使用图像识别的人员。这似乎令人惊讶 首先，因为音频本质上是一维连续信号 随着时间的推移，不是一个2D空间问题。</p>
<p>我们通过定义一个时间窗口来相信我们所说的话语来解决这个问题 应该适合，并将该窗口中的音频信号转换成图像。 这是通过将传入的音频样本分成短片段来完成的，<br>几毫秒长，并计算跨越a的频率的强度 一组乐队。来自一个段的每组频率强度被视为一个 数字矢量，而这些矢量按时间顺序排列组成<br>二维数组。这个值的数组可以被视为一个 单通道图像，并被称为一个 谱图。如果你想查看<br>音频采样产生了什么样的图像，你可以运行`wav_to_spectrogram 工具：</p>
<pre><code>bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- \
--input_wav=/tmp/speech_dataset/happy/ab00c4b2_nohash_0.wav \
--output_png=/tmp/spectrogram.png
</code></pre><p>如果你打开<code>/tmp/spectrogram.png</code>你应该看到这样的东西：</p>
<p><img src="https://storage.googleapis.com/download.tensorflow.org/example_images/spectrogram.png" alt=""></p>
<p>由于TensorFlow的记忆顺序，此图像中的时间从顶部增加 到底，频率从左到右，不像平常 时间从左到右的谱图的约定。你应该能够<br>看到几个不同的部分，与第一个音节“哈”不同 “PPY”。</p>
<p>由于人耳对某些频率比其他频率更敏感， 传统的语音识别方法对此做了进一步的处理 代表把它变成一套梅尔倒频谱 系数或MFCC<br>简而言之。这也是一个二维的单通道表示法 也被当作一个形象来对待。如果你的目标是一般的声音，而不是 你可能会发现你可以跳过这一步，直接操作 谱图。</p>
<p>这些处理步骤产生的图像然后被输入到一个 多层卷积神经网络，具有完整的连接层 由softmax结束。你可以看到这部分的定义 tensorflow /示例/<br>speech_commands / models.py。</p>
<h2><span id="流媒体的准确性">流媒体的准确性</span></h2><p>大多数音频识别应用程序需要在连续的音频流上运行， 而不是单独的剪辑。在此使用模型的典型方法 环境是在不同的偏移量和时间平均重复应用<br>结果在一个短的窗口产生一个平滑的预测。如果你认为 的输入作为图像，它不断沿着时间轴滚动。该 我们想要认识的话可以随时开始，所以我们需要采取一系列的措施<br>快照有机会获得大部分的对齐 在时间窗口中的话语我们进入模型。如果我们抽高一些 足够的速度，那么我们有一个很好的机会来捕捉这个词在多个<br>窗户，所以平均的结果提高了整体的信心 预测。</p>
<p>有关如何在流式数据上使用模型的示例，可以查看 test_streaming_accuracy.cc。 这使用了 RecognizeCommands<br>类通过一个长形式的输入音频来运行，尝试发现单词，并进行比较 这些预测是针对标签和时间的基本事实清单。这使得它 一个将模型应用于音频信号流的好例子。</p>
<p>您将需要一个长音频文件来测试它，并显示标签 在哪里说每个字。如果你不想自己录制一个，你可以<br>使用<code>generate_streaming_test_wav</code>生成一些综合测试数据 效用。默认情况下，这将创建一个大约10分钟的.wav文件<br>每三秒钟一个文本文件，每个文件包含每个时间的实际情况 说话。这些单词是从你当前的测试部分中提取的 数据集，与背景噪音混合在一起。运行它，使用：</p>
<pre><code>bazel run tensorflow/examples/speech_commands:generate_streaming_test_wav
</code></pre><p>这将保存一个.wav文件到<code>/tmp/speech_commands_train/streaming_test.wav</code>， 和一个列出标签的文本文件<br><code>/tmp/speech_commands_train/streaming_test_labels.txt</code>。你可以运行 准确性测试：</p>
<pre><code>bazel run tensorflow/examples/speech_commands:test_streaming_accuracy -- \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_commands_train/streaming_test.wav \
--ground_truth=/tmp/speech_commands_train/streaming_test_labels.txt \
--verbose
</code></pre><p>这将输出正确匹配的单词数量的信息，如何 许多人被给出了错误的标签，以及当模型触发多少次 没有真正的言语。有各种各样的参数控制如何<br>信号均值工作，包括设定长度的<code>--average_window_ms</code> 时间平均结果超过，<code>--clip_stride_ms</code>之间的时间<br><code>--suppression_ms</code>型号的应用，停止后续的单词 在发现初始的一段时间后触发一段时间的检测<br><code>--detection_threshold</code>，控制平均得分必须达到多高 在它被认为是一个坚实的结果之前。</p>
<p>您会看到，流式精确度会输出三个数字，而不仅仅是 在训练中使用的一个度量。这是因为不同的应用程序 不同的要求，有些能够容忍频繁的不正确<br>结果只要找到真正的单词（高回忆），而另一些非常集中 确保预测的标签即使有些也很可能是正确的 没有被检测到（高精度）。工具中的数字给你一个想法<br>你的模型将如何在应用程序中执行，你可以尝试调整 信号平均参数调整它给你想要的性能。 要了解什么是适合您的应用程序的正确参数，您可以查看 在创造一个中华民国<br>曲线帮助 你明白这个权衡。</p>
<h2><span id="识别命令">识别命令</span></h2><p>流媒体精确度工具使用一个简单的解码器包含在一个小的C ++类 叫 RecognizeCommands。<br>这个类是随着时间的推移运行TensorFlow模型的输出 对信号进行平均，并在足够的时候返回关于标签的信息 证据认为已经发现了一个公认的词。执行是<br>相当小，只是跟踪最后的几个预测和平均他们， 所以很容易根据需要移植到其他平台和语言。例如，<br>在Android或Python上的Java级别上做类似的事情是很方便的 在树莓派。只要这些实现共享相同的逻辑，你 可以使用流测试来调整控制平均的参数<br>工具，然后将它们转移到您的应用程序以获得类似的结果。</p>
<h2><span id="高级培训">高级培训</span></h2><p>训练脚本的默认设计是为了产生良好的端对端 导致一个相对较小的文件，但有很多选项，你可以 根据自己的要求更改以自定义结果。</p>
<h3><span id="自定义培训数据">自定义培训数据</span></h3><p>默认情况下，脚本将下载语音命令 数据集，但是 你也可以提供你自己的训练数据。为了训练你自己的数据，你 应该确保每个声音至少有数百个录音<br>你想认识，并按课程安排成文件夹。对于 例如，如果你试图从猫咪中识别出狗吠，那么你会的 创建一个名为<code>animal_sounds</code>的根文件夹，然后在那两个之内<br>子文件夹称为<code>bark</code>和<code>miaow</code>。你会然后组织你的音频文件 进入适当的文件夹。</p>
<p>要将脚本指向新的音频文件，您需要将<code>--data_url=</code>设置为 禁用语音命令数据集的下载，以及<br><code>--data_dir=/your/data/folder/</code>来查找刚创建的文件。</p>
<p>文件本身应该是16位小端PCM编码的WAVE格式。该 采样率默认为16,000，但只要所有音频一致 相同的速度（脚本不支持重新采样），你可以改变这一点<br><code>--sample_rate</code>的说法。剪辑也应该大致相同 持续时间。默认的预期持续时间是一秒钟，但您可以使用此设置<br><code>--clip_duration_ms</code>标志。如果你有可变金额的剪辑 在开始的沉默，你可以看看字对齐工具来标准化他们<br>（这里是一个快速和肮脏的方法，你可以使用 太）。</p>
<p>要注意的一个问题是，你可能会有非常类似的重复 在你的数据集中有相同的声音，如果是这样的话，这些可能会产生误导性的指标<br>分布在您的培训，验证和测试集。例如，演讲 设置的命令有人多次重复相同的单词。每一个 这些重复可能是非常接近其他人，所以如果训练是<br>过度拟合和记忆，它可能表现得不切实际 在测试集中看到一个非常相似的副本。为了避免这种危险，语音指令 要确保所有剪辑都包含一个人说的同一个词<br>被放入同一个分区。剪辑分配给培训，测试或 验证集基于它们的文件名的散列，以确保 即使添加了新的剪辑，作业也保持稳定，并避免任何培训<br>样本迁移到其他集合。确保所有给定的发言者 单词在同一个桶中，散列 功能 计算时忽略“nohash”后的文件名中的任何内容<br>分配。这意味着如果你有文件名如<code>pete_nohash_0.wav</code>和 <code>pete_nohash_1.wav</code>，他们保证在同一个集合。</p>
<h3><span id="未知的类">未知的类</span></h3><p>您的应用程序很可能会听到不在您的培训中的声音 设置，你会希望模型表明它不能识别噪音 在这些情况下。为了帮助网络学习什么可以忽略，你需要<br>提供一些既不是你的课堂音频片段。要做到这一点，你会的 创建<code>quack</code>，<code>oink</code>和<code>moo</code>子文件夹， 你的用户可能遇到的其他动物。<br><code>--wanted_words</code>的论点 脚本定义你关心哪些类，所有其他提到的类 在训练期间，子文件夹名称将用于填充<code>_unknown_</code>类。<br>语音命令数据集在其未知的类中包含二十个词，包括 数字从零到九，随机名称如“Sheila”。</p>
<p>默认情况下，10％的训练样例是从未知类中挑选出来的，但是 你可以用<code>--unknown_percentage</code>标志来控制它。增加这个意志<br>使得模型不太可能将不知名的单词误认为想要的单词，而是使其成为可能 它太大，可能会适得其反，因为模型可能会认为这是最安全的分类 所有的话都是未知的！</p>
<h3><span id="背景噪音">背景噪音</span></h3><p>即使有其他不相关的情况，真正的应用程序也必须识别音频 发生在环境中的声音。要建立一个这种强大的模型 的干扰，我们需要训练与录制的音频相似<br>属性。语音命令数据集中的文件被捕获在一个变种上 用户在许多不同的环境中，而不是在工作室，这样的设备 有助于增加一些现实主义的培训。要添加更多，你可以随机混合<br>环境音频段到训练输入。在语音命令中 设置有一个名为<code>_background_noise_</code>的特殊文件夹 一分钟长的WAVE文件，白噪声，机器和日常录音<br>家庭活动。</p>
<p>随机选择这些文件的小片段，并以低量混合 在训练期间进入剪辑。响度也是随机选择的，并受到控制<br>由<code>--background_volume</code>的论点作为一个比例，其中0是沉默，1 是完整的数量。不是所有的剪辑都添加了背景，所以<br><code>--background_frequency</code>标志控制着它们混合的比例。</p>
<p>您自己的应用程序可能在不同的环境中运行 背景噪音模式比这些默认，所以你可以提供自己的音频<br>剪辑在<code>_background_noise_</code>文件夹中。这些应该是相同的采样率 作为你的主要数据集，但是持续时间要长得多，这样一套好的随机性<br>段可以从中选择。</p>
<h3><span id="安静">安静</span></h3><p>在大多数情况下，你所关心的声音将是间歇性的，所以它是 重要的是要知道什么时候没有匹配的音频。为了支持这个，有一个<br>特殊的<code>_silence_</code>标签，指示模型什么时候什么都没有检测到 有趣。因为在真实环境中从来没有完全沉默，我们<br>实际上不得不提供安静和不相关的音频的例子。为此，我们 重新使用<code>_background_noise_</code>文件夹，这个文件夹也混合到了真正的剪辑中，<br>拖动音频数据的短小部分，并将其与地面进行馈送 <code>_silence_</code>的真理级。默认提供10％的训练数据<br>这个，但<code>--silence_percentage</code>可以用来控制比例。如 与未知的话，设置这个更高可以减轻模型的结果有利于<br>真正的保持沉默，牺牲了文字的否定，但也是如此 很大一部分会导致它陷入一直猜测的陷阱 安静。</p>
<h3><span id="时间转移">时间转移</span></h3><p>增加背景噪音是扭曲a中训练数据的一种方式 现实的方式来有效地增加数据集的大小，等等增加 整体的准确性和时间的转移是另一回事。这涉及到一个随机偏移量<br>训练样本数据的时间，使得开始或结束的一小部分是 切断，对面的部分用零填充。这模仿了自然 训练数据中的起始时间的变化，并且由… …控制<br><code>--time_shift_ms</code>标志，默认为100ms。增加这个值将会 提供更多的变化，但有切断重要部分的风险<br>音频。用现实的扭曲来增加数据的一个相关的方法是通过 使用时间拉伸和俯仰 缩放， 但这超出了本教程的范围。</p>
<h2><span id="定制模型">定制模型</span></h2><p>这个脚本使用的默认模型是相当大的，超过8亿 FLOPs为每个推断和使用940,000重量参数。这运行在 台式机或现代手机可用的速度，但涉及太多<br>计算在具有更多限制的设备上以交互式速度运行 资源。为了支持这些用例，有几个选择 可供选择：</p>
<p>low_latency_conv 基于卷积中描述的’cnn-one-fstride4’拓扑 用于小型关键字点击的神经网络 纸。<br>精度略低于“conv”，但重量参数的数量 大致相同，只需要1100万FLOP就能进行一次预测， 使其更快。</p>
<p>要使用此型号，请指定<code>--model_architecture=low_latency_conv</code> 命令行。您还需要更新培训费率和数量<br>的步骤，所以完整的命令将如下所示：</p>
<pre><code>python tensorflow/examples/speech_commands/train \
--model_architecture=low_latency_conv \
--how_many_training_steps=20000,6000 \
--learning_rate=0.01,0.001
</code></pre><p>这就要求脚本以20000步的学习速率进行训练 然后以小10倍的速度微调6000步。</p>
<p>low_latency_svdf 基于压缩深度神经网络的拓扑结构， Rank-Constrained Topology论文。<br>准确度也低于’conv’，但只用了大约75万 参数，最重要的是，它允许在一个优化的执行 测试时间（即，当你真的在你的应用程序中使用它），结果<br>在750万FLOPs。</p>
<p>要使用此型号，请指定<code>--model_architecture=low_latency_svdf</code> 命令行，并更新培训率和数量<br>的步骤，所以完整的命令将如下所示：</p>
<pre><code>python tensorflow/examples/speech_commands/train \
--model_architecture=low_latency_svdf \
--how_many_training_steps=100000,35000 \
--learning_rate=0.01,0.005
</code></pre><p>请注意，尽管需要比前两个更多的步骤 拓扑结构，减少计算的数量意味着训练应该采取 大约在同一时间，最后达到85％左右的精度。<br>你还可以进一步调整拓扑相当容易计算和 通过改变SVDF层中的这些参数来提高精度：</p>
<p>等级 - 近似的等级（更高通常更好，但结果是          更多的计算）。 num_units - 类似于其他图层类型，指定中的节点数<br>层（更多的节点更好的质量，更多的计算）。</p>
<p>关于运行时，由于该层允许缓存一些优化 内部神经网络激活，你需要确保使用一致的 （例如’clip_stride_ms’标志），当你冻结图表，以及什么时候<br>以流模式执行模型（例如test_streaming_accuracy.cc）。</p>
<p>其他参数要定制 如果你想尝试定制模型，一个好的开始是通过 调整光谱图创建参数。这具有改变的效果 输入图像到模型的大小，以及创建代码 models.py<br>将自动调整计算和权重的数量以适应 不同的尺寸。如果您将输入变小，模型将需要更少 计算来处理它，所以它是一个折衷一些准确性的好方法 以改善延迟。<br><code>--window_stride_ms</code>控制每个相隔多远 频率分析样本是从以前的。如果你增加这个值，那么 在给定的持续时间内采样的数量越少，输入的时间轴越少<br>会缩小。 <code>--dct_coefficient_count</code>标志控制着多少桶 用于频率计数，所以减少这个会缩小输入 其他维度。<br><code>--window_size_ms</code>参数不影响大小，但是 确实控制了用于计算频率的区域的宽度 样品。减少训练样本的持续时间，由…控制<br><code>--clip_duration_ms</code>，如果你要找的声音很短， 因为这也减少了输入的时间维度。你需要做的 确保所有的训练数据在开始部分都包含正确的音频<br>虽然剪辑。</p>
<p>如果你对于你的问题有一个完全不同的模型，你可能会发现 你可以插入它 models.py 并让脚本的其余部分处理所有的预处理和训练<br>力学。你会添加一个新的条款<code>create_model</code>，寻找的名字 你的架构，然后调用模型创建功能。这个功能是 给定光谱图输入的大小以及其他模型信息<br>预计会创建TensorFlow操作来读取并生成输出 预测向量和占位符来控制丢失率。剩下的 脚本将把这个模型整合到一个更大的图中去做<br>输入计算并应用softmax和损失函数来训练它。</p>
<p>当你调整模型和训练超参数时，一个常见的问题是 由于数值精度问题，非数值可能会蔓延。在 一般来说，你可以通过减少像学习这样的东西来解决这些问题<br>费率和重量初始化函数，但如果它们是持久的，你可以 启用<code>--check_nans</code>标志来追踪错误的来源。这会<br>在TensorFlow中的大多数常规操作之间插入检查操作，然后中止 培训过程中遇到一个有用的错误消息。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/deep_cnn/" title="卷积神经网络" itemprop="url">卷积神经网络</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="卷积神经网络">卷积神经网络</span></h1><blockquote>
<p>注意：本教程适用于TensorFlow的高级用户 并承担机器学习方面的专业知识和经验。</p>
</blockquote>
<h2><span id="概观">概观</span></h2><p>CIFAR-10分类是机器学习中常见的基准问题。该 问题是分类10个类别的RGB 32x32像素图像：</p>
<pre><code>airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.
</code></pre><p>有关更多详细信息，请参阅CIFAR-10页面 和一份技术报告 Alex Krizhevsky。</p>
<h3><span id="目标">目标</span></h3><p>本教程的目标是建立一个相对较小的卷积神经网络 网络（CNN） 识别图像。在这个过程中，本教程：</p>
<p>突出了一个规范的网络架构组织， 培训和评估。 为构建更大更复杂的模型提供了一个模板。</p>
<p>选择CIFAR-10的原因是它足够复杂 TensorFlow的大部分功能可以扩展到大型模型。与此同时， 该模型足够小，可以快速训练，这是尝试的理想选择<br>新的想法和新技术的尝试。</p>
<h3><span id="教程的亮点">教程的亮点</span></h3><p>CIFAR-10教程演示了几个重要的构造 在TensorFlow中设计更大更复杂的模型：</p>
<p>核心数学组件包括卷积 （维基） 纠正线性激活 （维基） 最大池 （维基） 和本地响应正常化 （第3.3章 AlexNet论文）。 可视化<br>训练期间的网络活动，包括输入图像， 激活和渐变的损失和分布。 用于计算的例程 移动平均线 学习的参数和使用这些平均值 在评估期间提高预测性能。 执行一个<br>学习率计划 随着时间的推移有系统地减少。 预取队列 输入 数据隔离模型从磁盘延迟和昂贵的图像预处理。</p>
<p>我们也提供了一个多GPU版本 该模型的演示：</p>
<p>配置一个模型来并行训练多个GPU卡。 在多个GPU之间共享和更新变量。</p>
<p>我们希望这个教程提供了一个启动点来构建更大的CNN 视觉任务在TensorFlow。</p>
<h3><span id="模型架构">模型架构</span></h3><p>这个CIFAR-10教程中的模型是一个多层体系结构 交替卷积和非线性。这些层完全跟随 连接层导致softmax分类器。模型遵循 架构所描述的<br>亚历克斯Krizhevsky，几个 在最上面几层的差异。</p>
<p>这个模型在几个小时内达到了高达86％的精度 在GPU上的训练时间。请参阅下面的代码 了解详情。它由1,068,298个可学习的参数组成，需要大约<br>19.5M乘加操作来计算单个图像的推理。</p>
<h2><span id="代码组织">代码组织</span></h2><p>本教程的代码驻留在 <code>models/tutorials/image/cifar10/</code>。</p>
<table>
<thead>
<tr>
<th>File</th>
<th>Purpose  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_input.py" target="_blank" rel="noopener"><code>cifar10_input.py</code></a><br>| Reads the native CIFAR-10 binary file format.<br><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10.py" target="_blank" rel="noopener"><code>cifar10.py</code></a><br>| Builds the CIFAR-10 model.<br><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_train.py" target="_blank" rel="noopener"><code>cifar10_train.py</code></a><br>| Trains a CIFAR-10 model on a CPU or GPU.<br><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py" target="_blank" rel="noopener"><code>cifar10_multi_gpu_train.py</code></a><br>| Trains a CIFAR-10 model on multiple GPUs.<br><a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_eval.py" target="_blank" rel="noopener"><code>cifar10_eval.py</code></a><br>| Evaluates the predictive performance of a CIFAR-10 model.  </p>
<h2><span id="cifar-10型号">CIFAR-10型号</span></h2><p>CIFAR-10网络主要包含在内 <code>cifar10.py</code>。 完整的培训 图形包含大约765个操作。我们发现我们可以使代码最多<br>通过使用以下模块构建图表可重复使用：</p>
<p>型号输入：<code>inputs()</code>和<code>distorted_inputs()</code>添加 操作，读取和预处理CIFAR图像进行评估和培训， 分别。<br>型号预测：<code>inference()</code> 增加对提供的图像执行推断（即，分类）的操作。 型号培训：<code>loss()</code>和<code>train()</code> 添加计算损失的操作，<br>渐变，可变更新和可视化摘要。</p>
<h3><span id="模型输入">模型输入</span></h3><p>该型号的输入部分由功能<code>inputs()</code>和 <code>distorted_inputs()</code>从CIFAR-10二进制数据文件读取图像。<br>这些文件包含固定的字节长度记录，所以我们使用 <code>tf.FixedLengthRecordReader</code>。 请参阅读取数据<br>详细了解<code>Reader</code>的工作原理。</p>
<p>图像处理如下：</p>
<p>他们被裁剪为24 x 24像素，集中评估或    随机进行培训。 他们大约变白了    使模型对动态范围不敏感。</p>
<p>对于训练，我们另外应用一系列的随机失真 人为地增加数据集的大小：</p>
<p>随机从左到右翻转图像。 随机扭曲图像的亮度。 随机扭曲的图像对比度。</p>
<p>请参阅图像页面的列表 可用的扭曲。我们也附上一个 <code>tf.summary.image</code>的图像 以便我们可以在TensorBoard中将它们可视化。<br>这是验证输入是否正确构建的良好实践。</p>
<p><img src="https://www.tensorflow.org/images/cifar_image_summary.png" alt=""></p>
<p>从磁盘读取图像和扭曲它们可以使用不平凡的数量 处理时间。为了防止这些操作放慢训练，我们运行 它们在16个独立的线程内连续填充一个TensorFlow 队列。</p>
<h3><span id="模型预测">模型预测</span></h3><p>该模型的预测部分由<code>inference()</code>功能构建 这增加了计算预测的逻辑的操作。那部分 该模型组织如下：</p>
<table>
<thead>
<tr>
<th>Layer Name</th>
<th>Description  </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>conv1</code></td>
<td></td>
</tr>
</tbody>
</table>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">convolution</a> and<br><a href="https://www.tensorflow.org/api_docs/python/tf/nn/relu" target="_blank" rel="noopener">rectified linear</a><br>activation.<br><code>pool1</code> | <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener">max<br>pooling</a>.<br><code>norm1</code> | <a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="noopener">local response<br>normalization</a>.<br><code>conv2</code> |<br><a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">convolution</a> and<br><a href="https://www.tensorflow.org/api_docs/python/tf/nn/relu" target="_blank" rel="noopener">rectified linear</a><br>activation.<br><code>norm2</code> | <a href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" target="_blank" rel="noopener">local response<br>normalization</a>.<br><code>pool2</code> | <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener">max<br>pooling</a>.<br><code>local3</code> | <a href="https://www.tensorflow.org/api_guides/python/nn" target="_blank" rel="noopener">fully connected layer with rectified linear<br>activation</a>.<br><code>local4</code> | <a href="https://www.tensorflow.org/api_guides/python/nn" target="_blank" rel="noopener">fully connected layer with rectified linear<br>activation</a>.<br><code>softmax_linear</code> | linear transformation to produce logits.  </p>
<p>这是一个由TensorBoard生成的图形，描述了推理操作：</p>
<p><img src="https://www.tensorflow.org/images/cifar_graph.png" alt=""></p>
<blockquote>
<p>练习：<code>inference</code>的输出是非标准化的对象。尝试编辑 网络体系结构返回归一化预测使用 <code>tf.nn.softmax</code>。</p>
</blockquote>
<p><code>inputs()</code>和<code>inference()</code>功能提供所有组件 进行模型评估是必要的。我们现在把重点转移到了 建立操作模型的训练。</p>
<blockquote>
<p>练习：<code>inference()</code>中的模型架构与之不同 在CIFAR-10模型中指定 CUDA-convnet。特别是顶部<br>亚历克斯的原始模型的层是本地连接，并没有完全连接。 尝试编辑体系结构以准确重现本地连接 架构在顶层。</p>
</blockquote>
<h3><span id="模型训练">模型训练</span></h3><p>训练网络执行N路分类的通常方法是 多项式逻辑回归， 又名。 softmax回归。 Softmax回归应用a softmax非线性的 网络的输出和计算 交叉熵<br>在标准化的预测与a 标签的1热编码。 为了正规化，我们也应用通常的 所有学习的重量衰减损失 变量。该模型的目标函数是交叉熵的和<br>以及<code>loss()</code>功能返回的所有这些重量衰减条款。</p>
<p>我们使用<code>tf.summary.scalar</code>在TensorBoard中将其可视化：</p>
<p><img src="https://www.tensorflow.org/images/cifar_loss.png" alt="CIFAR-10 Loss"></p>
<p>我们使用标准来训练模型 梯度下降 算法（请参阅其他方法的培训） 学习率是 呈指数衰减 随着时间的推移。</p>
<p><img src="https://www.tensorflow.org/images/cifar_lr_decay.png" alt="CIFAR-10 Learning Rate
Decay"></p>
<p><code>train()</code>功能通过添加所需的操作来最小化目标 计算梯度和更新学习的变量（见 <code>tf.train.GradientDescentOptimizer</code><br>详情）。它返回一个执行所有计算的操作 需要训练和更新一批图像的模型。</p>
<h2><span id="启动和训练模型">启动和训练模型</span></h2><p>我们已经建立了模型，现在启动它并运行训练操作 脚本<code>cifar10_train.py</code>。</p>
<pre><code>python cifar10_train.py
</code></pre><blockquote>
<p>注：第一次在CIFAR-10教程中运行任何目标时， CIFAR-10数据集自动下载。数据集是〜160MB 所以你可能想为你的第一次尝试拿一杯咖啡。</p>
</blockquote>
<p>你应该看到输出：</p>
<pre><code>Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2015-11-04 11:45:45.927302: step 0, loss = 4.68 (2.0 examples/sec; 64.221 sec/batch)
2015-11-04 11:45:49.133065: step 10, loss = 4.66 (533.8 examples/sec; 0.240 sec/batch)
2015-11-04 11:45:51.397710: step 20, loss = 4.64 (597.4 examples/sec; 0.214 sec/batch)
2015-11-04 11:45:54.446850: step 30, loss = 4.62 (391.0 examples/sec; 0.327 sec/batch)
2015-11-04 11:45:57.152676: step 40, loss = 4.61 (430.2 examples/sec; 0.298 sec/batch)
2015-11-04 11:46:00.437717: step 50, loss = 4.59 (406.4 examples/sec; 0.315 sec/batch)
...
</code></pre><p>该脚本报告每10步的总损失以及速度 最后一批数据被处理。几点意见：</p>
<p>第一批数据可能非常慢（如几分钟） 预处理线程用20,000处理的CIFAR填充混洗队列 图片。 报告的损失是最近一批的平均损失。请记住<br>这个损失是交叉熵和所有重量衰减项之和。 密切关注批次的处理速度。上面显示的数字是 在Tesla K40c上获得。如果您在CPU上运行，则性能会降低。</p>
<blockquote>
<p>练习：在试验时，有时候第一个很烦人 训练步骤可能需要很长时间。尝试减少图像的数量<br>最初填满队列。搜索<code>min_fraction_of_examples_in_queue</code> 在<code>cifar10_input.py</code>中。</p>
</blockquote>
<p><code>cifar10_train.py</code>定期保存 所有的模型参数 检查点文件 但它不评估模型。检查点文件 将被<code>cifar10_eval.py</code>用于测量预测值<br>性能（请参阅下面的评估模型）。</p>
<p>如果你按照前面的步骤，那么你现在已经开始训练 一个CIFAR-10模型。恭喜！</p>
<p>从<code>cifar10_train.py</code>返回的终端文本提供了最小的洞察力 模型是如何训练的。我们希望在培训期间更深入地了解模型：</p>
<p>损失是真的减少还是只是噪音？ 模型是否提供了适当的图像？ 梯度，激活和权重是否合理？ 目前的学习率是多少？</p>
<p>TensorBoard提供了这个 功能，显示<code>cifar10_train.py</code>定期输出的数据 一个 <code>tf.summary.FileWriter</code>。</p>
<p>比如，我们可以看到激活的分布和度数 <code>local3</code>特性中的稀疏性在训练过程中发展：</p>
<p><img src="https://www.tensorflow.org/images/cifar_sparsity.png" alt=""><br><img src="https://www.tensorflow.org/images/cifar_activations.png" alt=""></p>
<p>个人损失函数以及全部损失尤其如此 有趣的跟踪时间。但是，损失表现出相当的数量 由于培训使用小批量的噪音。实际上我们发现<br>除了原始数据之外，还可以看到他们的移动平均线 值。看看脚本如何使用 <code>tf.train.ExponentialMovingAverage</code> 以此目的。</p>
<h2><span id="评估一个模型">评估一个模型</span></h2><p>现在让我们来评估训练好的模型在保持数据集上的表现如何。 该模型由<code>cifar10_eval.py</code>脚本进行评估。它构建了模型<br>与<code>inference()</code>功能一起使用，并在评估套件中使用全部10,000张图像 CIFAR-10。它计算的精度为1：最高预测的频率 匹配图像的真实标签。</p>
<p>为了监测训练期间模型的改进情况，评估脚本运行 定期在<code>cifar10_train.py</code>创建的最新检查点文件上。</p>
<pre><code>python cifar10_eval.py
</code></pre><blockquote>
<p>小心不要在同一个GPU上运行评估和训练二进制文件 否则你可能会用完内存。考虑运行评估 一个单独的GPU（如果可用）或在运行时挂起训练二进制文件<br>在同一个GPU上进行评估。</p>
</blockquote>
<p>你应该看到输出：</p>
<pre><code>2015-11-06 08:30:44.391206: precision @ 1 = 0.860
...
</code></pre><p>脚本只是定期返回精度@ 1，在这种情况下 它返回了86％的准确性。 <code>cifar10_eval.py</code>也<br>出口摘要可能在TensorBoard中可视化。这些总结 在评估过程中提供对模型的更多见解。</p>
<p>训练脚本计算 移动平均线 所有学习变量的版本。评估脚本替代 所有学习的移动平均版本的模型参数。这个 替代在评估时提升模型表现。</p>
<blockquote>
<p>练习：使用平均参数可以提高预测性能 按精度@ 1测量约3％。编辑<code>cifar10_eval.py</code>不使用 模型的平均参数和验证预测性能 下降。</p>
</blockquote>
<h2><span id="使用多个gpu卡培训模型">使用多个GPU卡培训模型</span></h2><p>现代工作站可能包含多个GPU进行科学计算。 TensorFlow可以利用这个环境来运行培训操作 同时跨多个卡。</p>
<p>以平行，分布式的方式训练模型需要 协调培训流程。以下我们称为模型副本 成为数据子集模型训练的一个副本。</p>
<p>天真地使用模型参数的异步更新 导致次优训练绩效 因为一个单独的模型副本可能会被过时训练 模型参数的副本。相反，采用完全同步 更新将会像最慢的模型副本一样慢。</p>
<p>在具有多个GPU卡的工作站中，每个GPU将具有相似的速度 并包含足够的内存来运行整个CIFAR-10模型。因此，我们选择 按照以下方式设计我们的培训系统：</p>
<p>在每个GPU上放置一个模型副本。 等待所有的GPU完成，同步更新模型参数 处理一批数据。</p>
<p>这是这个模型的图表：</p>
<p><img src="https://www.tensorflow.org/images/Parallelism.png" alt=""></p>
<p>请注意，每个GPU计算推理以及独特的渐变 一批数据。这种设置有效地允许分割更大的批次 的GPU数据。</p>
<p>此设置要求所有GPU共享模型参数。一个众所周知的 事实上从GPU传输数据的速度相当慢。为了这 原因，我们决定存储和更新CPU上的所有模型参数（请参阅<br>绿色框）。一组新的模型参数被传送到GPU 当一批新的数据被所有GPU处理时。</p>
<p>GPU在运行中同步。所有的渐变都是从 GPU和平均值（见绿框）。模型参数用更新 所有模型副本的平均梯度。</p>
<h3><span id="在设备上放置变量和操作">在设备上放置变量和操作</span></h3><p>在设备上放置操作和变量需要一些特殊的东西 抽象。</p>
<p>我们需要的第一个抽象是一个计算推理的功能 单个模型副本的渐变。在代码中我们称这个抽象 一个“塔”。我们必须为每个塔设置两个属性：</p>
<p>塔内所有操作的唯一名称。 <code>tf.name_scope</code>提供 这个独特的名字通过预先的范围。例如，所有的操作 第一塔预先装有<code>tower_0</code>，例如，<br><code>tower_0/conv1/Conv2D</code>。 在塔内运行操作的首选硬件设备。 <code>tf.device</code>具体说明了这一点。对于<br>实例中，第一塔的所有操作都驻留在<code>device(&#39;/device:GPU:0&#39;)</code>内 范围指示它们应该在第一个GPU上运行。</p>
<p>所有变量都被固定到CPU并通过访问 <code>tf.get_variable</code> 以便在多GPU版本中共享它们。 查看如何共享变量。</p>
<h3><span id="在多个gpu卡上启动和训练模型">在多个GPU卡上启动和训练模型</span></h3><p>如果您的计算机上安装了多个GPU卡，则可以使用它们 用<code>cifar10_multi_gpu_train.py</code>脚本更快地训练模型。这个<br>训练脚本的版本在多个GPU卡上并行化模型。</p>
<pre><code>python cifar10_multi_gpu_train.py --num_gpus=2
</code></pre><p>请注意，所使用的GPU卡数量默认为1.此外，如果只有1 GPU在你的机器上是可用的，所有的计算都将被放置在它上面，即使 你要求更多。</p>
<blockquote>
<p>练习：<code>cifar10_train.py</code>的默认设置是<br>在批处理大小128上运行。尝试在2个GPU上运行<code>cifar10_multi_gpu_train.py</code> 批量大小为64，比较训练速度。</p>
</blockquote>
<h2><span id="下一步">下一步</span></h2><p>恭喜！你有 完成了CIFAR-10教程。</p>
<p>如果你现在有兴趣开发和培养自己的形象 分类系统，我们建议分叉这个教程并替换 组件来解决你的图像分类问题。</p>
<blockquote>
<p>练习：下载 街景房屋号码（SVHN）数据集。 分叉CIFAR-10教程和交换在SVHN作为输入数据。尝试适应 网络架构来提高预测性能。</p>
</blockquote>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/mobile_intro/" title="TensorFlow Mobile简介" itemprop="url">TensorFlow Mobile简介</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="tensorflow-mobile简介">TensorFlow Mobile简介</span></h1><p>TensorFlow的设计从根本上是一个很好的深度学习解决方案 适用于Android和iOS等移动平台。这个移动指南应该帮助你<br>了解机器学习如何在移动平台上工作，以及如何 将TensorFlow高效地整合到您的移动应用程序中。</p>
<h2><span id="关于本指南">关于本指南</span></h2><p>本指南针对的是拥有TensorFlow模型的开发人员 成功地在桌面环境中工作，谁想要将其整合进去 一个移动应用程序，并且不能使用TensorFlow<br>Lite。这里有 在这个过程中你将面临的主要挑战：</p>
<p>了解如何使用Tensorflow进行移动。 为您的平台构建TensorFlow。 将TensorFlow库集成到您的应用程序中。<br>准备您的模型文件进行移动部署。 优化延迟，RAM使用率，模型文件大小和二进制大小。</p>
<h2><span id="移动机器学习的常见用例">移动机器学习的常见用例</span></h2><p>为什么要在手机上运行TensorFlow？</p>
<p>传统上，深度学习与数据中心和巨人联系在一起 高性能GPU机器集群。但是，它可能是非常昂贵的 耗费时间来发送设备通过网络访问的所有数据<br>连接。在移动设备上运行使得交互性非常好 应用程序的方式是不可能的，当你不得不等待一个网络 往返。</p>
<p>以下是设备深度学习的一些常见用例：</p>
<h3><span id="语音识别">语音识别</span></h3><p>有很多有趣的应用程序可以用一个 语音驱动的界面，其中许多需要在设备上处理。大多数 用户没有发出命令的时间，等等<br>远程服务器将是一个浪费带宽，因为它将主要是沉默或 背景噪音。为了解决这个问题，通常有一个小的神经 运行在设备上的网络监听特定的关键字。<br>一旦关键字被发现，其余的 会话可以传送到服务器进行进一步处理 需要更多的计算能力。</p>
<h3><span id="图像识别">图像识别</span></h3><p>移动应用程序能够理解照相机可能非常有用 图片。如果你的用户正在拍照，认识到他们的内容可以帮助你 相机应用程序应用适当的过滤器，或标记照片，以便他们很容易<br>玛丽萨。对于嵌入式应用程序也很重要，因为您可以使用图像 传感器来检测各种有趣的条件，无论是发现 濒临灭绝的野生动物 要么 报告你的火车运行有多迟。</p>
<p>TensorFlow附带了几个识别对象类型的例子 内部图像以及各种不同的预先训练的模型，他们可以 全部在移动设备上运行。你可以试试 我们的 诗人和诗人的张量<br>诗人2的Tensorflow：优化移动代码到 看看如何采取预训练模式，并运行一些非常快速和轻量级 训练教它识别特定的对象，然后优化它 在手机上运行。</p>
<h3><span id="对象本地化">对象本地化</span></h3><p>有时候知道图像中的物体是什么以及什么是很重要的 他们是。有很多增强现实的使用情况可能会有益于 移动应用程序，比如在向用户提供正确的组件时引导用户<br>帮助修复他们的无线网络或提供信息覆盖 景观特点。嵌入式应用程序通常需要计算对象 路过它们，无论是农作物领域的害虫，还是人，车，路 自行车通过路灯。</p>
<p>TensorFlow提供了一个预制模型来绘制围绕人的边界框 在图像中检测到，随着时间的推移跟踪代码。该 跟踪对于您要计数的应用程序尤其重要<br>随着时间的推移有多少物体，因为它给你一个好主意，当一个 新物体进入或离开现场。我们有一些示例代码 可用于Android上 Github上，<br>也是一个更一般的对象检测 模型 也可以使用。</p>
<h3><span id="手势识别">手势识别</span></h3><p>能够用手或其他控制应用程序可能是有用的 手势，无论是从图像识别或通过分析加速度计 传感器数据。创建这些模型超出了本指南的范围，但是<br>TensorFlow是部署它们的有效方法。</p>
<h3><span id="光学字符识别">光学字符识别</span></h3><p>谷歌翻译的实时相机视图是如何有效的一个很好的例子 交互式设备上的文本检测可以。</p>
<p>识别图像中的文字涉及多个步骤。你先有 确定文本所在的区域，这是一个变体 对象本地化问题，可以用类似的技术来解决。一旦您<br>有一个区域的文字，然后你需要把它解释为字母，然后使用一个 语言模型来帮助猜测他们代表的是什么词。最简单的方法 估计出现的字母是将文本行分割成单独的<br>字母，然后将一个简单的神经网络应用到每个的边界框。您 可以得到良好的结果，用于MNIST，你可以找到的模型<br>在TensorFlow的教程中，尽管您可能需要更高分辨率的输入。一个 更先进的选择是使用LSTM模型来处理整个系列 文本一次，与模型本身处理分割成不同的<br>字符。</p>
<h3><span id="翻译">翻译</span></h3><p>即使你，也能快速而准确地从一种语言翻译成另一种语言 没有网络连接，是一个重要的用例。深度网络是 这种任务非常有效，你可以找到很多的描述<br>文学中的不同模型。通常这些是序列到序列 经常使用的模型，您可以运行一个图形来完成整个工作 翻译，而无需运行单独的解析阶段。</p>
<h3><span id="文本分类">文本分类</span></h3><p>如果您想根据用户输入的内容向用户提示相关的提示 阅读，理解文本的含义是非常有用的。这是 文本分类是在哪里进行的。文本分类是一个总括性术语<br>涵盖了从情感分析到主题发现的所有内容。你很有可能 有自己的类别或标签，你想申请，所以最好的地方 开始就是一个例子 喜欢 跳思考， 然后训练你自己的例子。</p>
<h3><span id="语音合成">语音合成</span></h3><p>合成语音可以是给予用户反馈或帮助的好方法 可访问性，以及近期的进展如 WaveNet秀 深度学习可以提供非常自然的演讲。</p>
<h2><span id="移动机器学习和云">移动机器学习和云</span></h2><p>这些用例的例子给出了设备网络如何实现的概念 补充云服务。云计算有很大的计算能力 受控环境，但在设备上运行可以提供更高的交互性。<br>在云无法使用或云容量有限的情况下， 您可以提供脱机体验，或通过处理减少云工作负载 容易在设备上的情况。</p>
<p>在设备上进行计算也可以发出何时切换到工作状态的信号 在云上。语音中的热词检测就是一个很好的例子。以来 设备能够不断地收听关键字，然后触发一个<br>一旦被识别出来，很多流量都基于云端的语音识别。没有 在设备上的组件，整个应用程序将是不可行的，这一点 模式也存在于其他几个应用程序中。认识到一些<br>传感器输入是足够有趣的进一步处理使得很多 有趣的产品可能。</p>
<h2><span id="你应该有什么硬件和软件">你应该有什么硬件和软件？</span></h2><p>TensorFlow在Ubuntu Linux，Windows 10和OS X上运行 支持的操作系统和安装TensorFlow的说明，请参阅<br>安装Tensorflow。</p>
<p>请注意，我们为移动TensorFlow提供的一些示例代码需要您 从源头上编译TensorFlow，所以你不仅需要<code>pip install</code><br>通过所有示例代码工作。</p>
<p>要尝试移动示例，您需要为开发设置一个设备， 运用 Android Studio， 或Xcode，如果你正在开发的iOS。</p>
<h2><span id="在开始之前你应该做什么">在开始之前你应该做什么？</span></h2><p>在考虑如何在移动设备上获得解决方案之前，</p>
<p>确定你的问题是否可以通过移动机器学习解决 创建一个带标签的数据集来定义你的问题 为这个问题选择一个有效的模型</p>
<p>我们将在下面更详细地讨论这些。</p>
<h3><span id="移动机器学习能解决您的问题吗">移动机器学习能解决您的问题吗？</span></h3><p>一旦你了解了你想解决的问题，你需要制定一个计划 如何建立你的解决方案。最重要的第一步是确保 你的问题实际上是可以解决的，而最好的办法就是嘲笑它<br>在循环中使用人类。</p>
<p>例如，如果您想使用声控命令驾驶机器人玩具车，请尝试 从设备录制一些音频，然后回听，看看是否可以 理解所说的话。你经常会发现有问题<br>捕获过程中，如电机淹没语音或不能听到 在远方，你应该在投资这些问题之前解决这些问题 建模过程。</p>
<p>另一个例子是从你的应用程序提供的照片给人看看他们 可以按照您所寻找的方式对它们进行分类。如果他们做不到的话 （例如，试图从照片估计食物的卡路里可能是<br>因为所有的白汤都是一样的），那么你需要重新设计 你的经验，以应付这一点。一个好的经验法则是，如果一个人不能 处理任务然后培训一台电脑将会很困难。</p>
<h3><span id="创建一个带标签的数据集">创建一个带标签的数据集</span></h3><p>在解决了你的用例的基本问题之后，你需要 创建一个带标签的数据集来定义你想要解决的问题。这个 步骤是非常重要的，而不是选择使用哪种模型。你想要它<br>尽可能代表您的实际使用情况，因为这个模型 只会在你教你的任务上有效。这也是值得投资的 使数据标记尽可能高效和准确的工具。对于<br>例如，如果您能够从单击网页上的某个按钮切换 界面简单的键盘快捷键，你可以加快速度 生成过程很多。你也应该开始做最初的标签<br>你自己，所以你可以了解的困难和可能的错误，和 可能会更改您的标签或数据捕获过程以避免它们。一旦您 和你的团队能够不断标注的例子（这是你一次<br>大多数例子一般都认同同样的标签），那么你可以试试 捕捉你的知识手册，教外部评估员如何运行相同 处理。</p>
<h3><span id="选择一个有效的模型">选择一个有效的模型</span></h3><p>下一步是选择一个有效的模型来使用。你可能可以避免 如果其他人已经实施了模型，则从头开始训练模型 类似于你所需要的;我们有一个模型库在<br>你可以看Github上的TensorFlow 通过。倾向于找到最简单的模型，并尝试开始 只要你有少量的标签数据，因为你会得到最好的<br>结果，当你能够快速迭代。花费的时间越短 尝试训练一个模型，并在实际应用中运行它，总体上会更好 结果你会看到。一个算法获得很好的训练精度是很常见的<br>数字，但在实际的应用程序中失败，因为有一个 数据集与实际用法之间不匹配。原型端到端的使用尽快 尽可能创造一致的用户体验。</p>
<h2><span id="下一步">下一步</span></h2><p>我们建议您开始制作我们的一个演示 Android或iOS。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/mechanics/" title="张量流动力学101" itemprop="url">张量流动力学101</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="张量流动力学101">张量流动力学101</span></h1><p>代码：tensorflow / examples / tutorials / mnist /</p>
<p>本教程的目标是展示如何使用TensorFlow来训练和 评估一个简单的手写数字前馈神经网络 使用（经典）MNIST数据集进行分类。目标受众<br>本教程是有兴趣使用的有经验的机器学习用户 TensorFlow。</p>
<p>这些教程不适用于一般的机器学习教学。</p>
<p>请确保您已按照说明进行操作 安装TensorFlow。</p>
<h2><span id="教程文件">教程文件</span></h2><p>本教程引用了以下文件：</p>
<table>
<thead>
<tr>
<th>File</th>
<th>Purpose  </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><a href="https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist.py" target="_blank" rel="noopener"><code>mnist.py</code></a><br>| The code to build a fully-connected MNIST model.<br><a href="https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/fully_connected_feed.py" target="_blank" rel="noopener"><code>fully_connected_feed.py</code></a><br>| The main code to train the built MNIST model against the downloaded dataset<br>using a feed dictionary.  </p>
<p>只需直接运行<code>fully_connected_feed.py</code>文件即可开始培训：</p>
<pre><code>python fully_connected_feed.py
</code></pre><h2><span id="准备数据">准备数据</span></h2><p>MNIST是机器学习中的经典问题。问题是看 手写数字的灰度28x28像素图像，并确定哪个数字 图像表示从零到九的所有数字。</p>
<p><img src="https://www.tensorflow.org/images/mnist_digits.png" alt="MNIST Digits"></p>
<p>欲了解更多信息，请参阅Yann LeCun的MNIST页面 或克里斯·奥拉的MNIST的可视化。</p>
<h3><span id="下载">下载</span></h3><p>在<code>run_training()</code>方法的顶部，<code>input_data.read_data_sets()</code> 功能将确保正确的数据已被下载到您的本地<br>培训文件夹，然后解压缩该数据返回<code>DataSet</code>字典 实例。</p>
<pre><code>data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)
</code></pre><p>注意：<code>fake_data</code>标志用于单元测试目的，可能是 安全地被读者忽略。</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Purpose  </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>data_sets.train</code></td>
<td>55000 images and labels, for primary training.  </td>
</tr>
<tr>
<td><code>data_sets.validation</code></td>
<td>5000 images and labels, for iterative validation of</td>
</tr>
</tbody>
</table>
<p>training accuracy.<br><code>data_sets.test</code> | 10000 images and labels, for final testing of trained<br>accuracy.  </p>
<h3><span id="输入和占位符">输入和占位符</span></h3><p><code>placeholder_inputs()</code>功能创建两个<code>tf.placeholder</code> ops定义输入的形状，包括<code>batch_size</code><br>图表的其余部分，并将实际的训练实例馈送到其中。</p>
<pre><code>images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,
                                                       mnist.IMAGE_PIXELS))
labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))
</code></pre><p>再往下，在训练循环中，完整的图像和标签数据集是 切成适合<code>batch_size</code>的每一步，与这些占位符相匹配<br>然后使用<code>sess.run()</code>进入<code>feed_dict</code>功能 参数。</p>
<h2><span id="构建图表">构建图表</span></h2><p>在为数据创建占位符之后，图形将从 <code>mnist.py</code>文件按照三阶段模式：<code>inference()</code>，<code>loss()</code>和 <code>training()</code>。</p>
<p><code>inference()</code> - 根据运行需要构建图形 网络转发做出预测。 <code>loss()</code> - 向推理图添加生成所需的操作 失利。<br><code>training()</code> - 向损失图表添加计算所需的操作 并应用渐变。</p>
<p><img src="https://www.tensorflow.org/images/mnist_subgraph.png" alt=""></p>
<h3><span id="推理">推理</span></h3><p><code>inference()</code>功能根据需要构建图表 返回将包含输出预测的张量。</p>
<p>它将图像占位符作为输入并建立在顶部 它是一对完全连接的层，ReLU激活，然后是十个 节点线性层指定输出logits。</p>
<p><code>tf.name_scope</code>独一无二的图层 作为在该范围内创建的项目的前缀。</p>
<pre><code>with tf.name_scope(&apos;hidden1&apos;):
</code></pre><p>在规定的范围内，每一个要使用的权重和偏差 产生<code>tf.Variable</code>层 实例，与他们所需的形状：</p>
<pre><code>weights = tf.Variable(
    tf.truncated_normal([IMAGE_PIXELS, hidden1_units],
                        stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),
    name=&apos;weights&apos;)
biases = tf.Variable(tf.zeros([hidden1_units]),
                     name=&apos;biases&apos;)
</code></pre><p>例如，当它们是在<code>hidden1</code>示波器下创建的时候，独一无二的 赋予权重变量的名称是“<code>hidden1/weights</code>”。</p>
<p>每个变量被赋予初始化操作作为其构造的一部分。</p>
<p>在这个最常见的情况下，权重用初始化 <code>tf.truncated_normal</code> 并给出了它们的二维张量的形状 第一个dim表示层中的单元的数量<br>权重连接，第二个dim代表数量 权重连接的图层中的单位。对于第一层，命名 <code>hidden1</code>，外形尺寸为<code>[IMAGE_PIXELS,
hidden1_units]</code> 权重将图像输入连接到hidden1图层。该 <code>tf.truncated_normal</code>初始化器给定一个随机分布<br>均值和标准差。</p>
<p>然后偏差用<code>tf.zeros</code>初始化 确保它们以零值开始，它们的形状就是数字 他们连接的图层中的单位。</p>
<p>该图的三个主要操作 - 两个<code>tf.nn.relu</code> ops包装<code>tf.matmul</code> 为隐藏层和一个额外的<code>tf.matmul</code>为logits - 然后<br>依次创建每个<code>tf.Variable</code>实例 输入占位符或前一层的输出张量。</p>
<pre><code>hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)



hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)



logits = tf.matmul(hidden2, weights) + biases
</code></pre><p>最后，返回包含输出的<code>logits</code>张量。</p>
<h3><span id="失利">失利</span></h3><p><code>loss()</code>功能通过添加所需的损耗进一步构建图表 欢声笑语。</p>
<p>首先，将<code>labels_placeholder</code>的值转换为64位整数。然后，添加一个<code>tf.nn.sparse_softmax_cross_entropy_with_logits</code><br>op，从<code>labels_placeholder</code>自动生成单热标签，并将<code>inference()</code>功能的输出对象与这些单热标签进行比较。</p>
<pre><code>labels = tf.to_int64(labels)
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=labels, logits=logits, name=&apos;xentropy&apos;)
</code></pre><p>然后使用<code>tf.reduce_mean</code> 平均整个批量维度的交叉熵值（第一个） 维度）作为总损失。</p>
<pre><code>loss = tf.reduce_mean(cross_entropy, name=&apos;xentropy_mean&apos;)
</code></pre><p>然后将包含损失值的张量返回。</p>
<blockquote>
<p>注：交叉熵是信息理论允许我们的一个想法 描述相信神经网络的预测有多糟糕， 给出什么是真实的。欲了解更多信息，请阅读博客文章Visual<br>信息理论（<a href="http://colah.github.io/posts/2015-09-Visual-Information/）" target="_blank" rel="noopener">http://colah.github.io/posts/2015-09-Visual-Information/）</a></p>
</blockquote>
<h3><span id="训练">训练</span></h3><p><code>training()</code>功能增加了通过减少损失所需的操作 梯度下降。</p>
<p>首先，它从<code>loss()</code>功能中获取损耗张量并将其传递给a <code>tf.summary.scalar</code>， 与a一起使用时生成汇总值到事件文件中的操作<br><code>tf.summary.FileWriter</code>（见下文）。在这种情况下，它会发出快照值 每次写出摘要都会有损失。</p>
<pre><code>tf.summary.scalar(&apos;loss&apos;, loss)
</code></pre><p>接下来，我们实例化一个<code>tf.train.GradientDescentOptimizer</code> 负责按要求的学习率应用渐变。</p>
<pre><code>optimizer = tf.train.GradientDescentOptimizer(learning_rate)
</code></pre><p>然后我们生成一个变量来包含一个全局计数器 培训步骤和<code>tf.train.Optimizer.minimize</code> op用于更新系统中的可训练权重并增加<br>全球一步。按照惯例，该操作称为<code>train_op</code>，是必须的 由TensorFlow会议运行，以诱导一个完整的培训步骤 （见下文）。</p>
<pre><code>global_step = tf.Variable(0, name=&apos;global_step&apos;, trainable=False)
train_op = optimizer.minimize(loss, global_step=global_step)
</code></pre><h2><span id="训练模型">训练模型</span></h2><p>一旦图形被构建，就可以迭代地训练和评估一个循环 由<code>fully_connected_feed.py</code>中的用户代码控制。</p>
<h3><span id="图表">图表</span></h3><p>在<code>run_training()</code>功能的顶部是一个python <code>with</code>命令 表示所有构建的操作都将与默认值相关联 全球<code>tf.Graph</code> 实例。</p>
<pre><code>with tf.Graph().as_default():
</code></pre><p><code>tf.Graph</code>是一组可以一起执行的操作集合。 大多数TensorFlow使用将只需要依靠单个默认图。</p>
<p>更复杂的用途与多个图是可能的，但超出了范围 这个简单的教程。</p>
<h3><span id="会议">会议</span></h3><p>一旦所有的建设准备工作已经完成，所有必要的 生成了一个<code>tf.Session</code> 是为运行图形而创建的。</p>
<pre><code>sess = tf.Session()
</code></pre><p>或者，可以将<code>Session</code>生成为<code>with</code>模块进行作用域：</p>
<pre><code>with tf.Session() as sess:
</code></pre><p>会话的空参数表示此代码将附加到 （或创建，如果尚未创建）默认的本地会话。</p>
<p>创建会话后，立即所有的<code>tf.Variable</code> 实例通过调用<code>tf.Session.run</code>进行初始化 在他们的初始化操作。</p>
<pre><code>init = tf.global_variables_initializer()
sess.run(init)
</code></pre><p><code>tf.Session.run</code> 方法将运行图的完整子集 对应于作为参数传递的op（s）。在这第一个电话，<code>init</code> op是<code>tf.group</code><br>只包含变量的初始值设定项。没有其余的 图表在这里运行;发生在下面的训练循环中。</p>
<h3><span id="火车循环">火车循环</span></h3><p>用会话初始化变量后，可以开始培训。</p>
<p>用户代码控制每步的训练，以及最简单的循环 可以做有益的训练是：</p>
<pre><code>for step in xrange(FLAGS.max_steps):
    sess.run(train_op)
</code></pre><p>但是，本教程稍微复杂一点，因为它也必须切片 为每个步骤提供输入数据以匹配先前生成的占位符。</p>
<h4><span id="喂图表">喂图表</span></h4><p>对于每一个步骤，代码将生成一个饲料字典，将包含 由占位符键控的步骤的一组示例 ops他们代表。</p>
<p>在<code>fill_feed_dict()</code>功能中，查询给定的<code>DataSet</code>的下一个 <code>batch_size</code>套图像和标签，以及与占位符匹配的张量<br>填充包含下一个图像和标签。</p>
<pre><code>images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,
                                               FLAGS.fake_data)
</code></pre><p>然后用占位符作为键生成一个python字典对象 代表进给张量作为值。</p>
<pre><code>feed_dict = {
    images_placeholder: images_feed,
    labels_placeholder: labels_feed,
}
</code></pre><p>这被传递到<code>sess.run()</code>功能的<code>feed_dict</code>参数中以提供 这一步训练的输入例子。</p>
<h4><span id="检查状态">检查状态</span></h4><p>该代码指定了两个要在其运行调用中获取的值：<code>[train_op, loss]</code>。</p>
<pre><code>for step in xrange(FLAGS.max_steps):
    feed_dict = fill_feed_dict(data_sets.train,
                               images_placeholder,
                               labels_placeholder)
    _, loss_value = sess.run([train_op, loss],
                             feed_dict=feed_dict)
</code></pre><p>因为有两个取值，所以<code>sess.run()</code>返回一个元组 项目。在取值列表中的每个<code>Tensor</code>对应于一个numpy<br>返回的元组中的数组，在此期间充满张量的值 培训的一步。由于<code>train_op</code>是无输出值的<code>Operation</code>，<br>返回的元组中的对应元素是<code>None</code>，因此， 丢弃。但是，<code>loss</code>张量的值如果是模型，则可能成为NaN 在训练期间发散，所以我们捕获这个值用于记录。</p>
<p>假设训练运行良好，没有NaNs，训练循环也是如此 每100步打印一个简单的状态文本，让用户知道状态 训练。</p>
<pre><code>if step % 100 == 0:
    print(&apos;Step %d: loss = %.2f (%.3f sec)&apos; % (step, loss_value, duration))
</code></pre><h4><span id="可视化状态">可视化状态</span></h4><p>为了发出TensorBoard使用的事件文件， 所有的摘要（在这种情况下，只有一个）被收集到一个张量中 在图建设阶段。</p>
<pre><code>summary = tf.summary.merge_all()
</code></pre><p>然后在会话创建后，一个<code>tf.summary.FileWriter</code> 可以被实例化来写入事件文件，其中 包含图形本身和摘要的值。</p>
<pre><code>summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)
</code></pre><p>最后，事件文件将每次更新新的汇总值 对<code>summary</code>进行评估，并将其输出传递给作者的<code>add_summary()</code> 功能。</p>
<pre><code>summary_str = sess.run(summary, feed_dict=feed_dict)
summary_writer.add_summary(summary_str, step)
</code></pre><p>当事件文件被写入时，TensorBoard可能会针对训练运行 文件夹以显示摘要中的值。</p>
<p><img src="https://www.tensorflow.org/images/mnist_tensorboard.png" alt="MNIST TensorBoard"></p>
<p>注意：有关如何构建和运行Tensorboard的更多信息，请参阅附带的教程Tensorboard：可视化学习。</p>
<h4><span id="保存一个检查点">保存一个检查点</span></h4><p>为了发出可能用于以后恢复模型的检查点文件 为了进一步的培训或评估，我们实例化一个 <code>tf.train.Saver</code>。</p>
<pre><code>saver = tf.train.Saver()
</code></pre><p>在训练循环中，<code>tf.train.Saver.save</code> 方法将定期被调用来写一个检查点文件到培训 目录中包含所有可训练变量的当前值。</p>
<pre><code>saver.save(sess, FLAGS.train_dir, global_step=step)
</code></pre><p>在将来的某个时候，培训可能会通过使用 <code>tf.train.Saver.restore</code> 方法重新加载模型参数。</p>
<pre><code>saver.restore(sess, FLAGS.train_dir)
</code></pre><h2><span id="评估模型">评估模型</span></h2><p>每千步，代码将尝试评估这两个模型 训练和测试数据集。 <code>do_eval()</code>功能称为三次 培训，验证和测试数据集。</p>
<pre><code>print(&apos;Training Data Eval:&apos;)
do_eval(sess,
        eval_correct,
        images_placeholder,
        labels_placeholder,
        data_sets.train)
print(&apos;Validation Data Eval:&apos;)
do_eval(sess,
        eval_correct,
        images_placeholder,
        labels_placeholder,
        data_sets.validation)
print(&apos;Test Data Eval:&apos;)
do_eval(sess,
        eval_correct,
        images_placeholder,
        labels_placeholder,
        data_sets.test)
</code></pre><blockquote>
<p>请注意，更复杂的用法通常会隔离<code>data_sets.test</code> 只有在大量超参数调整后才能检查。对于<br>为了一个简单的小MNIST问题，然而，我们评估所有的 数据。</p>
</blockquote>
<h3><span id="构建评估图">构建评估图</span></h3><p>在进入训练循环之前，应该已经建立了Eval操作 通过调用<code>evaluation()</code>的<code>mnist.py</code>功能相同<br>作为<code>loss()</code>功能的对象/标记参数。</p>
<pre><code>eval_correct = mnist.evaluation(logits, labels_placeholder)
</code></pre><p><code>evaluation()</code>功能只是生成一个<code>tf.nn.in_top_k</code> 如果真的标签可以自动评分每个模型的输出<br>可以在K最有可能的预测中找到。在这种情况下，我们设置值 的K为1，只考虑一个预测是否正确，如果是真实的标签。</p>
<pre><code>eval_correct = tf.nn.in_top_k(logits, labels, 1)
</code></pre><h3><span id="评估输出">评估输出</span></h3><p>然后可以创建一个回路来填充<code>feed_dict</code>并调用<code>sess.run()</code> 针对<code>eval_correct</code>运算来评估给定数据集上的模型。</p>
<pre><code>for step in xrange(steps_per_epoch):
    feed_dict = fill_feed_dict(data_set,
                               images_placeholder,
                               labels_placeholder)
    true_count += sess.run(eval_correct, feed_dict=feed_dict)
</code></pre><p><code>true_count</code>变量简单地累积了所有的预测 <code>in_top_k</code> op已经确定是正确的。从那里，精度可能是 从简单除以示例的总数计算。</p>
<pre><code>precision = true_count / num_examples
print(&apos;  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f&apos; %
      (num_examples, true_count, precision))
</code></pre>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/how-to-determine-the-current-shell-im-working-on/" title="如何确定我正在工作的当前shell？" itemprop="url">如何确定我正在工作的当前shell？</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> Published 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>我如何确定我正在使用的当前shell？</p>
<p><code>ps</code>命令的输出是否足够？</p>
<p>这怎么能在不同的UNIX中完成呢？</p>
        
        
        <p class="article-more-link">
          
            <a href="/2018/01/01/how-to-determine-the-current-shell-im-working-on/#more">Read More</a>
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/bash/">bash</a><a href="/tags/unix/">unix</a><a href="/tags/shell/">shell</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  


  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/page/123/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/122/">122</a><a class="page-number" href="/page/123/">123</a><span class="page-number current">124</span><a class="page-number" href="/page/125/">125</a><a class="page-number" href="/page/126/">126</a><span class="space">&hellip;</span><a class="page-number" href="/page/157/">157</a><a class="extend next" rel="next" href="/page/125/">Next<span></span></a>
  </nav>

</div>

      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- side-bar-ad -->
<ins class="adsbygoogle"
     style="display:block; overflow:hidden;"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="2232545787"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


  


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/javascript/" title="javascript">javascript<sup>207</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>205</sup></a></li>
			
		
			
				<li><a href="/tags/html/" title="html">html<sup>203</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>199</sup></a></li>
			
		
			
				<li><a href="/tags/bash/" title="bash">bash<sup>198</sup></a></li>
			
		
			
				<li><a href="/tags/php/" title="php">php<sup>197</sup></a></li>
			
		
			
				<li><a href="/tags/css/" title="css">css<sup>88</sup></a></li>
			
		
			
				<li><a href="/tags/shell/" title="shell">shell<sup>78</sup></a></li>
			
		
			
				<li><a href="/tags/jquery/" title="jquery">jquery<sup>61</sup></a></li>
			
		
			
				<li><a href="/tags/linux/" title="linux">linux<sup>57</sup></a></li>
			
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>41</sup></a></li>
			
		
			
				<li><a href="/tags/unix/" title="unix">unix<sup>30</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/html5/" title="html5">html5<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/xml/" title="xml">xml<sup>13</sup></a></li>
			
		
			
				<li><a href="/tags/http/" title="http">http<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/区块链/" title="区块链">区块链<sup>1</sup></a></li>
			
		
			
		
			
		
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="https://tracholar.github.io" target="_blank" title="个人博客">个人博客</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>

    </div>
    <footer><div id="footer" >
	
	
	<section class="info">
		<p> To be or not to be, that is a question. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		版权所有 © 2018 本站文章未经同意，禁止转载！作者：
		
		<a href="/about" target="_blank" title="zhizi">zhizi</a>
		


		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>












<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
