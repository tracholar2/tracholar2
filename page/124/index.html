
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>智子</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="zhizi">
    

    
    <meta name="description" content="IT技术、编程、web开发以及新兴的技术翻译与总结">
<meta property="og:type" content="website">
<meta property="og:title" content="智子">
<meta property="og:url" content="https://www.tracholar.top/page/124/index.html">
<meta property="og:site_name" content="智子">
<meta property="og:description" content="IT技术、编程、web开发以及新兴的技术翻译与总结">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="智子">
<meta name="twitter:description" content="IT技术、编程、web开发以及新兴的技术翻译与总结">

    
    <link rel="alternative" href="/atom.xml" title="智子" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">

    <!-- ad start -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6300557868920774",
    enable_page_level_ads: true
  });
</script>

    <!-- ad end -->

    <!--  stat -->
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4036f580b1119e720db871571faa68cc";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-78529611-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-78529611-1');
</script>

    <!-- end stat -->
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="智子">智子</a></h1>
				<h2 class="blog-motto">智子之家</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:www.tracholar.top">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">


   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/beginners/" title="MN初学者MNIST" itemprop="url">MN初学者MNIST</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="mn初学者mnist">MN初学者MNIST</span></h1><p>本教程适用于对机器学习和机器学习都陌生的读者 TensorFlow。如果你已经知道MNIST是什么，以及什么softmax（multinomial<br>逻辑）回归是，你可能更喜欢这个 节奏更快的教程。务必 在启动之前安装TensorFlow 教程。</p>
<p>当学习如何编程时，首先要做的就是传统 打印“Hello World”。就像编程有Hello World，机器学习一样 有MNIST。</p>
<p>MNIST是一个简单的计算机视觉数据集。它由手写的图像组成 这样的数字：</p>
<p><img src="https://www.tensorflow.org/images/MNIST.png" alt=""></p>
<p>它还包括每个图像的标签，告诉我们它是哪个数字。对于 例如，上述图片的标签是5,0,4和1。</p>
<p>在本教程中，我们将训练一个模型来查看图像并进行预测 他们是什么数字。我们的目标不是训练一个真正精细的模型 达到了最先进的性能 - 尽管我们会给你这样的代码<br>后来！ - 而是倾向于使用TensorFlow的脚趾。就这样，我们走了 开始一个非常简单的模型，称为Softmax回归。</p>
<p>本教程的实际代码非常短，而且都很有趣 东西发生在三行。但是，这是非常 重要的是要理解它背后的想法：TensorFlow如何工作和<br>核心机器学习概念。因此，我们要非常小心 通过代码工作。</p>
<h2><span id="关于本教程">关于本教程</span></h2><p>这个教程是一行一行的解释 mnist_softmax.py代码。</p>
<p>您可以通过几种不同的方式使用本教程，其中包括：</p>
<p>将每个代码片段逐行复制并粘贴到Python环境中   你读通过每一行的解释。 在读取之前或之后运行整个<code>mnist_softmax.py</code> Python文件<br>通过解释，并使用本教程了解的行   代码不清楚给你。</p>
<p>我们将在本教程中完成的任务：</p>
<p>了解MNIST数据和softmax回归 创建一个函数，这个函数是一个基于数据来识别数字的模型   图像中的每个像素<br>使用TensorFlow训练模型，通过“看”来识别数字   成千上万的例子（并运行我们的第一个TensorFlow会话来这样做）<br>用我们的测试数据检查模型的准确性</p>
<h2><span id="mnist数据">MNIST数据</span></h2><p>MNIST数据托管在 Yann LeCun的网站。如果你正在复制和 从本教程的代码粘贴，从这里开始这两行代码 它将自动下载并读取数据：</p>
<pre><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)
</code></pre><p>MNIST数据分为三部分：55,000个训练数据点 数据（<code>mnist.train</code>），10,000点测试数据（<code>mnist.test</code>）和5,000<br>验证数据点（<code>mnist.validation</code>）。这个分裂是非常重要的： 在机器学习中，我们有独立的数据是不可或缺的<br>从中学习，以便我们确保我们所学到的东西 概括！</p>
<p>如前所述，每个MNIST数据点都有两部分：a 手写数字和相应的标签。我们将调用图像“x” 和标签“y”。训练集和测试集都包含图像及其图像<br>相应的标签;例如训练图像是<code>mnist.train.images</code> 培训标签为<code>mnist.train.labels</code>。</p>
<p>每个图像是28像素×28像素。我们可以把它解释为一大堆 数字：</p>
<p><img src="https://www.tensorflow.org/images/MNIST-Matrix.png" alt=""></p>
<p>我们可以把这个数组变成一个28x28 = 784的数字。它不 只要我们在图像之间保持一致，那么我们如何平铺阵列。 从这个角度来看，MNIST图像只是一堆点<br>784维矢量空间，用a 结构非常丰富 （警告：计算密集的可视化）。</p>
<p>展平数据会丢弃有关图像二维结构的信息。 那不好吗？那么，最好的计算机视觉方法是利用这一点 结构，我们将在以后的教程。但是，我们将是简单的方法<br>在这里使用softmax回归（下面定义）不会。</p>
<p>结果是<code>mnist.train.images</code>是一个张量（一个n维阵列） 形状为<code>[55000, 784]</code>。第一个维度是列表中的一个索引<br>第二维是每个图像中每个像素的索引。 张量中的每个条目都是0到1之间的像素强度，对于特定的 像素在特定的图像。</p>
<p><img src="https://www.tensorflow.org/images/mnist-train-xs.png" alt=""></p>
<p>MNIST中的每个图像都有一个相应的标签，一个介于0和9之间的数字 代表在图像中绘制的数字。</p>
<p>对于本教程的目的，我们将要我们的标签为“一热” 矢量“，单向矢量是大多数情况下为0，向量为1的矢量 单一维度。在这种情况下，\（n \）的数字将被表示为a<br>向量在\（n \）维上是1。例如，3将是 \（[0,0,0,1,0,0,0,0,0,0] \）。因此，<code>mnist.train.labels</code>是一款<br><code>[55000, 10]</code>浮标阵列。</p>
<p><img src="https://www.tensorflow.org/images/mnist-train-ys.png" alt=""></p>
<p>我们现在准备好实际制作我们的模型！</p>
<h2><span id="softmax回归">Softmax回归</span></h2><p>我们知道MNIST中的每个图像都是一个0到0之间的手写数字 九。所以一个给定的图像可能只有十个可能的东西。我们想要 能够看图像，并给出它们的概率<br>数字。例如，我们的模型可能会看到一个九的图片，并有80％的把握 这是一个九，但有一个5％的机会，作为一个八（因为顶级循环）<br>对所有其他人都有一定的概率，因为这不是100％确定的。</p>
<p>这是softmax回归是一个自然，简单模型的经典案例。 如果你想给一个对象分配几个不同的概率<br>事情，softmax是要做的事情，因为softmax给了我们一个价值清单 在0和1之间加起来就是1.甚至在以后，当我们训练更复杂时<br>型号，最后一步将是softmax的一层。</p>
<p>softmax回归有两个步骤：首先我们加上我们输入的证据 在某些类别中，然后我们将这些证据转化为概率。</p>
<p>为了统计一个给定的图像是在一个特定的类别的证据，我们做一个 像素强度的加权和。如果该像素的重量是负的 具有高强度的证据是反对该类图像的证据<br>积极的，如果它是有利的证据。</p>
<p>下图显示了一个模型为每个模型学习的权重 类。红色代表负重，蓝色代表正值 权重。</p>
<p><img src="https://www.tensorflow.org/images/softmax-weights.png" alt=""></p>
<p>我们还添加了一些额外的证据，称为偏见。基本上，我们希望能够 说有些东西更可能独立于输入。结果是 证明给定输入\（x \）的类\（i \）的证据是：</p>
<p>$$\text{evidence}_i = \sum<em>j W</em>{i,~ j} x_j + b_i$$</p>
<p>其中\（W_i \）是权重，\（b_i \）是类别\（i \）的偏差， \（j \）是对我们输入图像\（x \）中的像素求和的索引。<br>然后，我们将证据变成我们预测的概率 \（y \）使用“softmax”功能：</p>
<p>$$y = \text{softmax}(\text{evidence})$$</p>
<p>这里softmax是作为一个“激活”或“链接”功能，塑造 我们的线性函数输出成我们想要的形式 - 在这种情况下，a 概率分布10例。 你可以把它当作转换符号<br>将证据转化为我们在每个阶层投入的概率。 它被定义为：</p>
<p>$$\text{softmax}(evidence) = \text{normalize}(\exp(evidence))$$</p>
<p>如果将这个等式展开，你会得到：</p>
<p>$$\text{softmax}(evidence)_i = \frac{\exp(evidence_i)}{\sum_j<br>\exp(evidence_j)}$$</p>
<p>但是，首先想到softmax是指数式的，这通常会更有帮助 其输入，然后正常化他们。指数意味着多一个 证据单位乘以增加给予任何假设的权重。<br>相反，少一个单位的证据意味着一个假设得到一个 早期重量的一小部分。没有假设曾经有零或负面的 重量。 Softmax然后归一化这些权重，以便它们合计为一，<br>形成有效的概率分布。 （为了获得更多的直觉 softmax功能，检查出来 在它的部分 迈克尔·尼尔森（Michael<br>Nielsen）的书，完成一个交互式的可视化。</p>
<p>您可以将我们的softmax回归看成如下所示， 尽管有更多\（x \）s。对于每个输出，我们计算一个加权和 \（x<br>\）s，添加一个偏差，然后应用softmax。</p>
<p><img src="https://www.tensorflow.org/images/softmax-regression-scalargraph.png" alt=""></p>
<p>如果我们把它写成等式，我们得到：</p>
<p><img src="https://www.tensorflow.org/images/softmax-regression-scalarequation.png" alt="\[y1, y2, y3\] = softmax\(W11*x1 + W12*x2 + W13*x3 + b1,  W21*x1 + W22*x2 +
W23*x3 + b2,  W31*x1 + W32*x2 + W33*x3 +
b3\)"></p>
<p>我们可以将这个过程“矢量化”，将其转化为矩阵乘法 和矢量添加。这对于计算效率是有帮助的。 （这也是 一个有用的思考方式。）</p>
<p><img src="https://www.tensorflow.org/images/softmax-regression-
vectorequation.png" alt="\[y1, y2, y3\] = softmax\(\[\[W11, W12, W13\], \[W21, W22, W23\], \[W31,
W32, W33\]\]*\[x1, x2, x3\] + \[b1, b2,
b3\]\)"></p>
<p>更简洁，我们可以写：</p>
<p>$$y = \text{softmax}(Wx + b)$$</p>
<p>现在让我们把它转换成TensorFlow可以使用的东西。</p>
<h2><span id="实施回归">实施回归</span></h2><p>为了在Python中进行高效的数值计算，我们通常使用像 NumPy做矩阵等昂贵的操作 在Python之外进行乘法，使用在其中实现的高效代码<br>另一种语言。不幸的是，还是会有很多开销 切换回Python的每一个操作。如果你这个开销特别糟糕 想要在GPU上运行计算或以分布式方式运行，在哪里可以<br>传输数据的成本很高。</p>
<p>TensorFlow也在Python之外做了繁重的工作，但是它需要一些东西 进一步避免这种开销。而不是运行一个昂贵的<br>独立于Python的操作，TensorFlow让我们描述一个图 完全在Python之外运行的交互操作。 （像这样的方法 可以在几个机器学习库中看到）。</p>
<p>要使用TensorFlow，首先我们需要导入它。</p>
<pre><code>import tensorflow as tf
</code></pre><p>我们通过操纵符号变量来描述这些交互操作。 我们来创建一个：</p>
<pre><code>x = tf.placeholder(tf.float32, [None, 784])
</code></pre><p><code>x</code>不是一个具体的值。这是一个<code>placeholder</code>，我们将在什么时候输入 我们要求TensorFlow进行计算。我们希望能够输入任何数字<br>的MNIST图像，每个平面化成784维向量。我们代表 这是一个浮点数的二维张量，形状为<code>[None, 784]</code>。<br>（这里<code>None</code>是指尺寸可以是任意长度。）</p>
<p>我们也需要我们的模型的权重和偏见。我们可以想象治疗 这些额外的输入，但TensorFlow有一个更好的方式来处理 它：<code>Variable</code>。<br><code>Variable</code>是生活在TensorFlow中的可修改张量 互动操作图。它可以被使用甚至被修改 计算。对于机器学习应用程序，通常有一个模型<br>参数为<code>Variable</code>。</p>
<pre><code>W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
</code></pre><p>我们通过给<code>Variable</code>创建这些<code>tf.Variable</code>的初始值 <code>Variable</code>：在这种情况下，我们将<code>W</code>和<code>b</code>初始化为张量充满<br>零。由于我们要学习<code>W</code>和<code>b</code>，所以没关系 他们最初是什么。</p>
<p>请注意，<code>W</code>的形状为[784，10]，因为我们想要乘以 784维图像矢量由它产生的10维矢量 证据的差异类。 <code>b</code>具有[10]的形状，所以我们可以添加它<br>到输出。</p>
<p>我们现在可以实现我们的模型。只需要一行来定义它！</p>
<pre><code>y = tf.nn.softmax(tf.matmul(x, W) + b)
</code></pre><p>首先，用<code>x</code>将<code>W</code>乘以<code>tf.matmul(x, W)</code>。这是 从我们在我们的方程中乘以它们的时候翻转过来，在那里我们有\（Wx \），如<br>处理<code>x</code>是一个具有多个输入的2D张量的小技巧。然后我们 加<code>b</code>，最后用<code>tf.nn.softmax</code>。</p>
<p>而已。在短短几句之后，我们只用了一条线来定义我们的模型 设置线。那并不是因为TensorFlow被设计成softmax<br>回归特别容易：这只是一个非常灵活的方式来描述很多 从机器学习模型到物理学的各种数值计算 模拟。一旦定义，我们的模型可以在不同的设备上运行：<br>你的电脑的CPU，GPU甚至手机！</p>
<h2><span id="训练">训练</span></h2><p>为了训练我们的模型，我们需要定义模型的含义 好。实际上，在机器学习中，我们通常定义它的意义 一个模型是坏的。我们称之为成本或损失，代表了多远<br>关闭我们的模型是从我们期望的结果。我们尽量减少这个错误，并且 误差越小，我们的模型越好。</p>
<p>调用一个非常常见的非常好的函数来确定模型的损失 “交叉熵”。交叉熵来源于思考信息 压缩信息论中的代码，但它是一个重要的想法<br>在从赌博到机器学习的很多领域。它被定义为：</p>
<p>$$H_{y’}(y) = -\sum_i y’_i \log(y_i)$$</p>
<p>其中\（y \）是我们预测的概率分布，而\（y’\）是真实的 分配（带有数字标签的一个热点向量）。在一些粗略的意义上，<br>交叉熵是衡量我们的预测是如何低效的描述 真相。关于交叉熵的更多细节超出了范围 本教程，但它是非常值得的 理解。</p>
<p>为了实现交叉熵，我们需要先添加一个新的占位符来输入 正确答案：</p>
<pre><code>y_ = tf.placeholder(tf.float32, [None, 10])
</code></pre><p>然后我们可以实现交叉熵函数\（ - \ sum y’\ log（y）\）：</p>
<pre><code>cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
</code></pre><p>首先，<code>tf.log</code>计算<code>y</code>各个元素的对数。接下来，我们乘以 <code>y_</code>的每个元件与<code>tf.log(y)</code>的相应元件。然后<br><code>tf.reduce_sum</code>增加了y的第二维中的元素，由于<br><code>reduction_indices=[1]</code>参数。最后，<code>tf.reduce_mean</code>计算平均值 在批处理中的所有例子。</p>
<p>请注意，在源代码中，我们不使用这个公式，因为它是 数字不稳定。相反，我们申请<br><code>tf.nn.softmax_cross_entropy_with_logits</code>在非规范化的logits（例如，我们<br>请拨打<code>softmax_cross_entropy_with_logits</code>上的<code>tf.matmul(x, W) + b</code>），因为这样<br>在数值上更稳定的函数在内部计算softmax激活。在 你的代码，考虑使用<code>tf.nn.softmax_cross_entropy_with_logits</code><br>代替。</p>
<p>现在我们知道我们的模型要做什么了，那么就很容易有TensorFlow 训练它这样做。因为TensorFlow知道你的整个图表 计算，它可以自动使用的<br>反向传播算法 有效地确定你的变量如何影响你所要求的损失 最小化。然后它可以应用你选择的优化算法来修改 变量并减少损失。</p>
<pre><code>train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
</code></pre><p>在这种情况下，我们要求TensorFlow使<code>cross_entropy</code>最小化 梯度下降算法 学习率为0.5。渐变下降是一个简单的程序，在哪里<br>TensorFlow简单地将每个变量稍微向一个方向移动 降低成本。但是TensorFlow也提供了 许多其他优化算法： 使用一个就像调整一行一样简单。</p>
<p>什么TensorFlow实际上在这里，在幕后，是添加新的操作 实现向后传播和梯度下降的图形。然后呢 给你一个单一的操作，当运行时，做一个梯度的步骤<br>下降训练，稍微调整你的变量以减少损失。</p>
<p>我们现在可以在<code>InteractiveSession</code>中启动该模型：</p>
<pre><code>sess = tf.InteractiveSession()
</code></pre><p>我们首先必须创建一个操作来初始化我们创建的变量：</p>
<pre><code>tf.global_variables_initializer().run()
</code></pre><p>让我们训练 - 我们将运行1000次的训练步骤！</p>
<pre><code>for _ in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre><p>循环的每一步，我们从中获得一百个随机数据点的“批” 我们的训练集。我们运行<code>train_step</code>送料批量数据更换 <code>placeholder</code>s。</p>
<p>使用小批量的随机数据称为随机训练 情况下，随机梯度下降。理想情况下，我们想使用我们所有的数据 每一步的训练，因为这将使我们更好地了解我们的<br>应该在做，但是这很贵。所以，我们使用不同的子集 每次。这样做很便宜，而且有很多相同的好处。</p>
<h2><span id="评估我们的模型">评估我们的模型</span></h2><p>我们的模型有多好？</p>
<p>那么，首先让我们弄清楚我们预测了正确的标签。 <code>tf.argmax</code> 是一个非常有用的功能，它给你最高的条目索引<br>沿着一些轴张量。例如，<code>tf.argmax(y,1)</code>是我们的标签 型号认为是最有可能的每一个输入，而<code>tf.argmax(y_,1)</code>是<br>正确的标签。我们可以使用<code>tf.equal</code>来检查我们的预测是否符合 真相。</p>
<pre><code>correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
</code></pre><p>这给了我们一个布尔的列表。为了确定什么部分是正确的，我们 投到浮点数，然后取平均值。例如， <code>[True, False, True,
True]</code>将成为<code>[1,0,1,1]</code>，成为<code>0.75</code>。</p>
<pre><code>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
</code></pre><p>最后，我们要求您的测试数据的准确性。</p>
<pre><code>print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
</code></pre><p>这应该是大约92％。</p>
<p>那好吗？那么，不是真的。事实上，这很糟糕。这是因为我们 使用一个非常简单的模型。有一些小的变化，我们可以达到97％。最好的<br>模型可以达到超过99.7％的准确性！ （有关更多信息，请参阅 这个 结果列表）</p>
<p>重要的是我们从这个模型中学到了东西。不过，如果你有点感觉 关于这些结果，退房 下一个教程，我们做了很多<br>更好地学习如何使用TensorFlow构建更复杂的模型！</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  
      <ins class="adsbygoogle"
     style="display:block;  overflow:hidden;"
     data-ad-format="fluid"
     data-ad-layout-key="-ej+6f-q-c7+ou"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="5206371097"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/version_compat/" title="TensorFlow版本兼容性" itemprop="url">TensorFlow版本兼容性</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="tensorflow版本兼容性">TensorFlow版本兼容性</span></h1><p>本文档适用于需要向后兼容的用户 TensorFlow（用于代码或数据）的版本，以及需要的开发人员 在保持兼容性的同时修改TensorFlow。</p>
<h2><span id="语义版本20">语义版本2.0</span></h2><p>TensorFlow遵循语义版本2.0（semver） 公共API。每个发布版本的TensorFlow都有<code>MAJOR.MINOR.PATCH</code>的形式。<br>例如，TensorFlow 1.2.3版本有<code>MAJOR</code>版本1，<code>MINOR</code>版本2， 和<code>PATCH</code>版本3.每个数字的更改具有以下含义：</p>
<p>主要：潜在的后向不兼容的变化。代码和数据   与以前的主要版本一起工作不一定与新的工作   发布。但是，在某些情况下，现有的TensorFlow图和检查点<br>可能会迁移到较新的版本;看到   图形和检查点的兼容性   有关数据兼容性的细节。 MINOR：向后兼容功能，速度提升等等<br>与之前的次要版本一起工作的数据，并且仅依赖于   公共API将继续工作不变。关于什么是和是的细节   不是公共API，看看是什么涵盖。<br>补丁：向后兼容的错误修复。</p>
<p>例如，1.0.0版引入了向后不兼容的更改 释放0.12.1。但是，版本1.1.1与版本向后兼容 1.0.0。</p>
<h2><span id="什么是覆盖">什么是覆盖</span></h2><p>只有TensorFlow的公共API向后兼容minor和 补丁版本。公共API包含</p>
<p>所有记录在Python中的函数和类 <code>tensorflow</code>模块及其子模块除外 <code>tf.contrib</code>的功能和类别<br>名称以<code>_</code>开头的函数和类（因为它们是私有的）   请注意，<code>examples/</code>和<code>tools/</code>目录中的代码不是   可通过<code>tensorflow</code><br>Python模块访问，因此不在此列   兼容性保证。</p>
<p>如果一个符号可以通过<code>tensorflow</code> Python模块或者它的   子模块，但没有记录，那么它不被视为的一部分   公共API。</p>
<p>C API。 以下协议缓冲区文件： <code>attr_value</code> <code>config</code> <code>event</code> <code>graph</code> <code>op_def</code> <code>reader_base</code><br><code>summary</code> <code>tensor</code> <code>tensor_shape</code> <code>types</code></p>
<h2><span id="什么不包括">什么不包括</span></h2><p>某些API函数明确标记为“实验性”，可以更改 后向不兼容的方式在次要版本之间。这些包括：</p>
<p>实验性API：Python中的<code>tf.contrib</code>模块及其子模块     以及C API中的任何函数或协议缓冲区中的字段     明确表示是实验性的。<br>其他语言：Python和C以外的其他语言的TensorFlow API，     如： C ++（通过头文件公开的<br><code>tensorflow/cc</code>）。 Java中， 走 复合操作的细节：Python中的许多公共函数扩展为<br>图中的几个原始操作，这些细节将成为任何的一部分     以<code>GraphDef</code>的形式保存到磁盘。这些细节可能会改变<br>次要版本。尤其是，回归测试，确切地检查     甚至，图形之间的匹配可能会跨越次要版本     尽管图的行为应该不变并存在     检查站仍然有效。<br>浮点数值细节：具体的浮点值     由运营商计算可能会随时改变。用户应该只依靠     近似精度和数值稳定性，而不是特定的位<br>计算。在次要和补丁版本中对数字公式的更改应该是     导致准确性相当或提高，与机器中的警告     学习提高特定公式的准确性可能会导致下降<br>整个系统的准确性。 随机数字：由特定的随机数字计算的     随机操作可能随时改变。     用户应该只依靠大致正确的分配和<br>统计强度，而不是计算的具体位数。但是，我们会做出     随机位的变化很少（也可能永远不会）发布。我们     当然会记录所有这些变化。<br>分布式Tensorflow中的版本偏差：运行两个不同的版本     TensorFlow在单个群集中不受支持。没有保证     关于有线协议的向后兼容性。<br>错误：我们保留向后兼容行为的权利     （尽管不是API）如果当前实现明显中断，     也就是说，如果它与文档相矛盾，或者如果是知名的<br>明确的预期行为由于错误而不能正确执行。     例如，如果一个优化器宣称要实现一个众所周知的优化     算法，但由于错误而不匹配该算法，则我们将修复<br>优化器。我们的修复可能会破坏依赖于错误行为的代码     收敛。我们会在发行说明中注意到这些变化。 错误消息：我们保留更改错误文本的权利<br>消息。另外，错误的类型可能会改变，除非类型是     在文档中指定。例如，一个函数记录到     举一个<code>InvalidArgument</code>异常会继续<br>提高<code>InvalidArgument</code>，但人类可读的信息内容可以改变。</p>
<h2><span id="图形和检查点的兼容性">图形和检查点的兼容性</span></h2><p>你有时需要保存图形和检查点。 图表描述了在训练期间运行的操作的数据流 推理和检查点包含保存在变量中的张量值 图形。</p>
<p>许多TensorFlow用户将图形和训练好的模型保存到磁盘 后来的评估或额外的培训，但最终运行他们保存的图表<br>或更高版本的模型。符合semver，任何图形或检查点 用一个版本的TensorFlow写出来就可以加载和评估一个<br>更高版本的TensorFlow与主要版本相同。但是，我们会 尽力在主要版本之间保持向后兼容性 可能的，以便序列化的文件可以长时间使用。</p>
<p>图表通过<code>GraphDef</code>协议缓冲区进行序列化。为了方便（罕见） 向后不兼容的图形变化，每个<code>GraphDef</code>都有一个版本号<br>与TensorFlow版本分开。例如，<code>GraphDef</code>版本17 <code>inv</code>不赞成使用<code>reciprocal</code>。语义是：</p>
<p>TensorFlow的每个版本都支持<code>GraphDef</code>版本的间隔。这个   间隔将在不同的补丁版本之间保持不变，并且只会越来越大<br>次要版本。只能删除对<code>GraphDef</code>版本的支持   为TensorFlow的主要版本。 新创建的图表分配了最新的<code>GraphDef</code>版本号。<br>如果给定版本的TensorFlow支持图形的<code>GraphDef</code>版本，   它将加载和评估与TensorFlow版本相同的行为<br>用于生成它（除了浮点数字细节和随机   数字），而不考虑TensorFlow的主要版本。尤其是所有   检查点文件将是兼容的。<br>如果<code>GraphDef</code>上限在（次要）版本中增加到X，那么   在下限增加到X之前至少6个月   例子（我们在这里使用假设的版本号）： TensorFlow<br>1.2可能支持<code>GraphDef</code>版本4至7。 TensorFlow 1.3可以添加<code>GraphDef</code>版本8并支持版本4到8。<br>至少半年后，TensorFlow 2.0.0可能会放弃支持   版本4到7，只剩下版本8。</p>
<p>最后，在支持<code>GraphDef</code>版本的时候，我们会试着去做 提供自动将图形转换为新的支持的工具 <code>GraphDef</code>版本。</p>
<h2><span id="扩展tensorflow时的图形和检查点兼容性">扩展TensorFlow时的图形和检查点兼容性</span></h2><p>只有在对<code>GraphDef</code>进行不兼容的更改时，本节才有意义 格式，如添加操作，删除操作或更改功能 现有的操作。上一节应该足以满足大多数用户的需求。</p>
<h3><span id="向后和部分向前兼容">向后和部分向前兼容</span></h3><p>我们的版本方案有三个要求：</p>
<p>向后兼容性，支持加载图形和检查点     用旧版本的TensorFlow创建。 向前兼容性，以支持生产者的场景<br>图形或检查点之前升级到更新版本的TensorFlow     消费者。 以不兼容的方式启用进化的TensorFlow。例如，删除Ops，<br>添加属性和删除属性。</p>
<p>请注意，<code>GraphDef</code>版本机制与TensorFlow分离 版本，向后不兼容<code>GraphDef</code>格式的更改仍然存在<br>受语义版本的限制。这意味着功能只能被删除 或在<code>MAJOR</code>版本的TensorFlow（如<code>1.7</code>到<code>2.0</code>）之间进行更改。<br>另外，在补丁版本（<code>1.x.1</code>）中强制执行向前兼容性 以<code>1.x.2</code>为例）。</p>
<p>实现向前和向后兼容性，并知道何时执行更改 在格式中，图形和检查点都有描述它们的时间的元数据 被生产了。下面的章节详细介绍了TensorFlow的实现和<br>不断发展的<code>GraphDef</code>版本指南。</p>
<h3><span id="独立的数据版本计划">独立的数据版本计划</span></h3><p>图形和检查点有不同的数据版本。这两个数据 格式的演变速度不同，速度也不同 来自TensorFlow。两个版本控制系统都在<br><code>core/public/version.h</code>。 每当添加新版本时，会在标题中添加注释，详细说明内容 改变和日期。</p>
<h3><span id="数据生产者和消费者">数据，生产者和消费者</span></h3><p>我们区分以下几种数据版本信息：  生产者：生成数据的二进制文件。生产者有一个版本   （<code>producer</code>）以及兼容的最低消费版本<br>（<code>min_consumer</code>）。  消费者：消耗数据的二进制文件。消费者有一个版本   （<code>consumer</code>）和它们兼容的最低生产者版本<br>（<code>min_producer</code>）。</p>
<p>每个版本化数据都有一个VersionDef 版本 记录制作数据的<code>producer</code>，<code>min_consumer</code><br>它与<code>bad_consumers</code>版本兼容 不允许。</p>
<p>默认情况下，当生产者创建一些数据时，数据会继承生产者的数据 <code>producer</code>和<code>min_consumer</code>版本。<br><code>bad_consumers</code>可根据具体情况进行设置 已知消费者版本包含错误，必须避免。消费者可以 接受一段数据如果以下情况都是如此：</p>
<p><code>consumer</code>&gt; =数据的<code>min_consumer</code> 数据的<code>producer</code>&gt; =消费者的<code>min_producer</code><br><code>consumer</code>不在数据的<code>bad_consumers</code>中</p>
<p>由于生产者和消费者都来自同一个TensorFlow代码库， <code>core/public/version.h</code><br>包含一个主要的数据版本，被视为<code>producer</code>或 <code>consumer</code>取决于上下文，<code>min_consumer</code>和<code>min_producer</code><br>（分别需要生产者和消费者）。特别，</p>
<p>对于<code>GraphDef</code>型号，我们有<code>TF_GRAPH_DEF_VERSION</code>，<br><code>TF_GRAPH_DEF_VERSION_MIN_CONSUMER</code>，和     <code>TF_GRAPH_DEF_VERSION_MIN_PRODUCER</code>。<br>对于检查点版本，我们有<code>TF_CHECKPOINT_VERSION</code>，     <code>TF_CHECKPOINT_VERSION_MIN_CONSUMER</code>，和<br><code>TF_CHECKPOINT_VERSION_MIN_PRODUCER</code>。</p>
<h3><span id="不断发展的graphdef版本">不断发展的GraphDef版本</span></h3><p>本节介绍如何使用这个版本机制来做出不同的 <code>GraphDef</code>格式的更改类型。</p>
<h4><span id="添加操作">添加操作</span></h4><p>同时向消费者和生产者添加新的作品，而不是 改变任何<code>GraphDef</code>版本。这种改变是自动的 向后兼容，并且不影响兼容性计划<br>现有的生产者脚本不会突然使用新的功能。</p>
<h4><span id="添加一个操作并切换现有的python包装来使用它">添加一个操作并切换现有的Python包装来使用它</span></h4><p>实现新的消费者功能并增加<code>GraphDef</code>版本。 如果有可能使包装仅使用新功能     以前没有用过的情况下，包装可以现在更新。<br>更改Python包装以使用新功能。不要增加     <code>min_consumer</code>，因为不使用此操作的模型不应该中断。</p>
<h4><span id="删除或限制操作的功能">删除或限制操作的功能</span></h4><p>修复所有的生产者脚本（而不是TensorFlow本身）不使用禁止的操作或     功能。 增加<code>GraphDef</code>版本并实现新的消费者功能<br>禁止在新版本中删除GraphDefs的操作或功能     以上。如有可能，使TensorFlow停止生产<code>GraphDefs</code><br>禁止功能。为此，请添加     REGISTER_OP（…）。已过时（deprecated_at_version，     信息）。<br>等待主要版本的向后兼容性的目的。 将<code>min_producer</code>从（2）中增加到GraphDef版本，然后删除     功能完全。</p>
<h4><span id="更改操作的功能">更改操作的功能</span></h4><p>添加一个新的类似的操作命名为<code>SomethingV2</code>或类似的，并通过     将其添加并切换现有的Python包装来使用它的过程<br>如果需要向前兼容，可能需要三周的时间。 删除旧的操作（只能发生与主要版本的变化，由于     向后兼容）。<br>增加<code>min_consumer</code>排除消费者与旧的欧普，加回     旧的Op作为<code>SomethingV2</code>的别名，并经过这个过程切换<br>现有的Python包装来使用它。 通过该过程删除<code>SomethingV2</code>。</p>
<h4><span id="禁止单个不安全的消费者版本">禁止单个不安全的消费者版本</span></h4><p>碰撞<code>GraphDef</code>版本并添加坏的版本到<code>bad_consumers</code><br>所有新的GraphDefs。如有可能，只能将GraphDefs添加到<code>bad_consumers</code>     其中包含一定的操作或类似的。<br>如果现有的消费者有不好的版本，尽快将其推出     可能。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/audio_recognition/" title="简单的音频识别" itemprop="url">简单的音频识别</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="简单的音频识别">简单的音频识别</span></h1><p>本教程将向您展示如何构建一个基本的语音识别网络 承认十个不同的单词。知道真实的言论和知识是很重要的 音频识别系统要复杂得多，但像MNIST这样的图像，<br>应该让你对所涉及的技术有一个基本的了解。一旦你 完成本教程，您将有一个试图分类一秒钟的模型 音频剪辑为沉默，未知单词，“是”，“否”，“上”，“下”，<br>“左”，“右”，“上”，“关”，“停止”或“去”。你也可以把这个 模型并在Android应用程序中运行。</p>
<h2><span id="制备">制备</span></h2><p>你应该确保你已经安装了TensorFlow，并且从脚本开始 下载超过1GB的训练数据，你需要一个良好的互联网连接和<br>您的机器上有足够的可用空间。培训过程本身可能需要几个 小时，所以确保你有一台机器可用的那么久。</p>
<h2><span id="训练">训练</span></h2><p>要开始训练过程，请转到TensorFlow源代码树并运行：</p>
<pre><code>python tensorflow/examples/speech_commands/train.py
</code></pre><p>脚本将从下载语音命令开始 数据集， 它由三十五个人的六万五千个WAVE音频文件组成 话。这些数据由Google收集，并以CC BY许可证的形式发布<br>你可以通过贡献自己的五分钟帮助改善它 语音。档案是 超过1GB，所以这部分可能需要一段时间，但你应该看到进度日志，和<br>一旦它被下载一次，你将不需要再做这一步。</p>
<p>一旦下载完成，你会看到日志信息 喜欢这个：</p>
<pre><code>I0730 16:53:44.766740   55030 train.py:176] Training from step: 1
I0730 16:53:47.289078   55030 train.py:217] Step #1: rate 0.001000, accuracy 7.0%, cross entropy 2.611571
</code></pre><p>这表明初始化过程完成，训练循环已经完成 开始。你会看到它为每个训练步骤输出信息。这是一个 打破它的意思：</p>
<p><code>Step #1</code>显示我们正在进行训练循环的第一步。在这种情况下 总共有18000步，所以你可以看一下步数 以了解它是如何接近完成。</p>
<p><code>rate 0.001000</code>是控制速度的学习速率 网络的重量更新。在这个早期是一个相对较高的数字（0.001），<br>但是对于以后的训练周期来说，它会减少10倍，达到0.0001。</p>
<p><code>accuracy 7.0%</code>是多少类正确预测在这个 训练步骤。这个值往往会波动很多，但是应该增加 平均随着培训的进展。模型输出一个数字数组，一个用于<br>每个标签，每个数字是输入的预测可能性 类。预测的标签是通过选择最高的条目来挑选的 得分了。分数总是在0到1之间，数值越高越好 对结果表示更多的信心。</p>
<p><code>cross entropy 2.611571</code>是我们正在使用的丢失功能的结果 指导培训过程。这是通过比较得到的分数 从当前训练运行到正确标签的分数向量<br>在训练中应该趋于下降。</p>
<p>经过一百步后，你应该看到这样的一行：</p>
<p>I0730 16：54：41.813438 55030 train.py:252]保存到<br>“/tmp/speech_commands_train/conv.ckpt-100”</p>
<p>这节省了目前训练过的检查点文件的权重。如果你的 训练脚本被中断，你可以查找最后保存的检查点和 然后用重新启动脚本<br><code>--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-100</code>作为命令行 从这一点开始。</p>
<h2><span id="混乱矩阵">混乱矩阵</span></h2><p>经过四百步之后，这些信息将被记录下来：</p>
<pre><code>I0730 16:57:38.073667   55030 train.py:243] Confusion Matrix:
 [[258   0   0   0   0   0   0   0   0   0   0   0]
 [  7   6  26  94   7  49   1  15  40   2   0  11]
 [ 10   1 107  80  13  22   0  13  10   1   0   4]
 [  1   3  16 163   6  48   0   5  10   1   0  17]
 [ 15   1  17 114  55  13   0   9  22   5   0   9]
 [  1   1   6  97   3  87   1  12  46   0   0  10]
 [  8   6  86  84  13  24   1   9   9   1   0   6]
 [  9   3  32 112   9  26   1  36  19   0   0   9]
 [  8   2  12  94   9  52   0   6  72   0   0   2]
 [ 16   1  39  74  29  42   0   6  37   9   0   3]
 [ 15   6  17  71  50  37   0   6  32   2   1   9]
 [ 11   1   6 151   5  42   0   8  16   0   0  20]]
</code></pre><p>第一部分是一个混乱 矩阵。至 明白它的意思，你首先需要知道正在使用的标签， 这种情况是“沉默”，“不明”，“是”，“否”，“上”，“下”，“左”<br>“右”，“上”，“关”，“停”和“走”。每列代表一组样本 被预测为每个标签，所以第一列代表所有的 所有被预测为沉默的剪辑，所有那些剪辑<br>预测是未知的话，第三个“是”，等等。</p>
<p>每行代表剪辑的正确的，地面的真相标签。第一行 是所有沉默的剪辑，第二个剪辑是未知的单词， 第三个“是”等</p>
<p>这个矩阵可能比仅仅一个精度分数更有用，因为它 给出了网络正在发生的错误的一个很好的总结。在这个例子中你 可以看到，第一行中的所有条目都是零，除了<br>最初的一个。因为第一行是所有实际上是沉默的剪辑， 这意味着它们中没有一个被错误地标记为单词，所以我们没有 对沉默的负面否定。这显示网络已经变得漂亮了<br>善于区分沉默与言语。</p>
<p>如果我们往下看第一列，我们会看到很多非零值。该 列代表所有被预测为沉默的剪辑，如此积极 第一个单元格外的数字是错误的。这意味着一些真实的剪辑<br>口头上的话实际上被预言是沉默，所以我们确实有相当的一个 很少有误报。</p>
<p>一个完美的模型将产生一个混乱矩阵，其中所有条目都是 零通过中心的对角线。发现偏离 该模式可以帮助你弄清楚模型是如何最容易混淆的<br>一旦你确定了问题，你可以通过添加更多的数据或者解决它们 清理类别。</p>
<h2><span id="验证">验证</span></h2><p>混淆矩阵后，你应该看到这样一行：</p>
<p>步骤400：验证准确度= 26.3％ （N = 3093）</p>
<p>将数据集分成三类是一个很好的做法。最大的 （在这种情况下大约80％的数据）用于训练网络，a 较小的一套（这里10％，被称为“验证”）被保留用于评估<br>在训练期间的准确性，另一组（最后的10％，“测试”）被使用 在训练结束后评估一次的准确性。</p>
<p>这种分裂的原因是网络总会有危险 在训练期间开始记忆他们的输入。通过保持验证集 单独，您可以确保该模型与以前从未见过的数据一起工作。<br>测试集是一个额外的保证，以确保你不只是 一直以适合训练和训练的方式调整你的模型 验证集合，但不是更广泛的输入。</p>
<p>训练脚本自动将数据集分成这三个 类别，上面的日志行显示了运行时模型的准确性 验证集。理想情况下，这应该相当接近培训<br>准确性。如果训练的准确性增加，但验证没有，那就是 一个标志，过度配合正在发生，你的模型只是学习的东西 关于培训片段，而不是泛泛的模式。</p>
<h2><span id="tensorboard">Tensorboard</span></h2><p>想象如何使用Tensorboard进行培训的一个好方法就是使用Tensorboard。通过 默认情况下，脚本将事件保存到/ tmp /<br>retrain_logs，并且可以加载 这些通过运行：</p>
<p><code>tensorboard --logdir /tmp/retrain_logs</code></p>
<p>然后在浏览器中导航到http：// localhost：6006， 你会看到图表和图表显示你的模型的进展。</p>
<p><img src="https://storage.googleapis.com/download.tensorflow.org/example_images/speech_commands_tensorflow.png" alt=""></p>
<h2><span id="培训完成">培训完成</span></h2><p>经过几个小时的训练（取决于你的机器的速度），脚本 应该已经完成​​了所有18000个步骤。这将打印出最后的混乱 矩阵，以及准确性分数，都在测试集上运行。随着<br>默认设置，你应该看到在85％和90％之间的准确性。</p>
<p>因为音频识别功能在移动设备上特别有用，接下来我们将会 将其导出为便于在这些平台上工作的紧凑格式。去做 那就运行这个命令行：</p>
<pre><code>python tensorflow/examples/speech_commands/freeze.py \
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \
--output_file=/tmp/my_frozen_graph.pb
</code></pre><p>冷冻模型创建完成后，您可以使用<code>label_wav.py</code>进行测试 脚本，如下所示：</p>
<pre><code>python tensorflow/examples/speech_commands/label_wav.py \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav
</code></pre><p>这应该打印出三个标签：</p>
<pre><code>left (score = 0.81477)
right (score = 0.14139)
_unknown_ (score = 0.03808)
</code></pre><p>希望“离开”是最高分，因为这是正确的标签，但自从 训练是随机的，它可能不会为您尝试的第一个文件。尝试一些 在同一个文件夹中的其他.wav文件，看看它有多好。</p>
<p>分数在0到1之间，数值越高意味着模型越多 对预测充满信心。</p>
<h2><span id="在android应用程序中运行模型">在Android应用程序中运行模型</span></h2><p>了解这个模型在真实应用程序中的最简单方法是下载 预构建的Android演示 应用 并将它们安装在您的手机上。你会看到’TF<br>Speech’出现在你的应用程序列表中， 打开它就会显示出我们刚刚培训过的动作词语列表 我们的模型，从“是”和“否”开始。一旦你给了应用程序的权限<br>要使用麦克风，你应该可以尝试说出这些话，看看他们 当模型识别其中的一个时，在用户界面中突出显示。</p>
<p>你也可以自己构建这个应用程序，因为它是开源的 作为TensorFlow存储库的一部分提供 github上。 默认情况下，它从一个预训练模型下载<br>tensorflow.org， 但是您可以轻松地将其替换为您已训练的模型 你自己。 如果你这样做，你需要确保主要的常量 SpeechActivity<br>Java源码 文件 像<code>SAMPLE_RATE</code>和<code>SAMPLE_DURATION</code>匹配您所做的任何更改 在训练时默认。你也会看到有一个Java版本的<br>RecognizeCommands 模 这与本教程中的C ++版本非常相似。如果你调整了<br>参数，您也可以在SpeechActivity中更新它们以获得相同的结果 结果与您的服务器测试一样。</p>
<p>演示应用程序根据标签自动更新其UI结果列表 您可以将文本文件复制到您的冻结图形的旁边，这意味着您可以 轻松地尝试不同的模型，而无需做任何代码更改。您<br>将需要更新<code>LABEL_FILENAME</code>和<code>MODEL_FILENAME</code>指向文件 你已经添加如果你改变路径。</p>
<h2><span id="这个模型如何工作">这个模型如何工作？</span></h2><p>本教程中使用的体系结构基于本文中描述的一些体系结构 用于小尺寸关键字的卷积神经网络 斑点。 选择它是因为它比较简单，快速训练，而且容易<br>理解，而不是最先进的技术。有很多不同的 方法来建立神经网络模型来处理音频，包括 经常性网络或扩张 （atrous） 卷积。<br>本教程是基于那种感觉非常好的卷积网络 熟悉任何使用图像识别的人员。这似乎令人惊讶 首先，因为音频本质上是一维连续信号 随着时间的推移，不是一个2D空间问题。</p>
<p>我们通过定义一个时间窗口来相信我们所说的话语来解决这个问题 应该适合，并将该窗口中的音频信号转换成图像。 这是通过将传入的音频样本分成短片段来完成的，<br>几毫秒长，并计算跨越a的频率的强度 一组乐队。来自一个段的每组频率强度被视为一个 数字矢量，而这些矢量按时间顺序排列组成<br>二维数组。这个值的数组可以被视为一个 单通道图像，并被称为一个 谱图。如果你想查看<br>音频采样产生了什么样的图像，你可以运行`wav_to_spectrogram 工具：</p>
<pre><code>bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- \
--input_wav=/tmp/speech_dataset/happy/ab00c4b2_nohash_0.wav \
--output_png=/tmp/spectrogram.png
</code></pre><p>如果你打开<code>/tmp/spectrogram.png</code>你应该看到这样的东西：</p>
<p><img src="https://storage.googleapis.com/download.tensorflow.org/example_images/spectrogram.png" alt=""></p>
<p>由于TensorFlow的记忆顺序，此图像中的时间从顶部增加 到底，频率从左到右，不像平常 时间从左到右的谱图的约定。你应该能够<br>看到几个不同的部分，与第一个音节“哈”不同 “PPY”。</p>
<p>由于人耳对某些频率比其他频率更敏感， 传统的语音识别方法对此做了进一步的处理 代表把它变成一套梅尔倒频谱 系数或MFCC<br>简而言之。这也是一个二维的单通道表示法 也被当作一个形象来对待。如果你的目标是一般的声音，而不是 你可能会发现你可以跳过这一步，直接操作 谱图。</p>
<p>这些处理步骤产生的图像然后被输入到一个 多层卷积神经网络，具有完整的连接层 由softmax结束。你可以看到这部分的定义 tensorflow /示例/<br>speech_commands / models.py。</p>
<h2><span id="流媒体的准确性">流媒体的准确性</span></h2><p>大多数音频识别应用程序需要在连续的音频流上运行， 而不是单独的剪辑。在此使用模型的典型方法 环境是在不同的偏移量和时间平均重复应用<br>结果在一个短的窗口产生一个平滑的预测。如果你认为 的输入作为图像，它不断沿着时间轴滚动。该 我们想要认识的话可以随时开始，所以我们需要采取一系列的措施<br>快照有机会获得大部分的对齐 在时间窗口中的话语我们进入模型。如果我们抽高一些 足够的速度，那么我们有一个很好的机会来捕捉这个词在多个<br>窗户，所以平均的结果提高了整体的信心 预测。</p>
<p>有关如何在流式数据上使用模型的示例，可以查看 test_streaming_accuracy.cc。 这使用了 RecognizeCommands<br>类通过一个长形式的输入音频来运行，尝试发现单词，并进行比较 这些预测是针对标签和时间的基本事实清单。这使得它 一个将模型应用于音频信号流的好例子。</p>
<p>您将需要一个长音频文件来测试它，并显示标签 在哪里说每个字。如果你不想自己录制一个，你可以<br>使用<code>generate_streaming_test_wav</code>生成一些综合测试数据 效用。默认情况下，这将创建一个大约10分钟的.wav文件<br>每三秒钟一个文本文件，每个文件包含每个时间的实际情况 说话。这些单词是从你当前的测试部分中提取的 数据集，与背景噪音混合在一起。运行它，使用：</p>
<pre><code>bazel run tensorflow/examples/speech_commands:generate_streaming_test_wav
</code></pre><p>这将保存一个.wav文件到<code>/tmp/speech_commands_train/streaming_test.wav</code>， 和一个列出标签的文本文件<br><code>/tmp/speech_commands_train/streaming_test_labels.txt</code>。你可以运行 准确性测试：</p>
<pre><code>bazel run tensorflow/examples/speech_commands:test_streaming_accuracy -- \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_commands_train/streaming_test.wav \
--ground_truth=/tmp/speech_commands_train/streaming_test_labels.txt \
--verbose
</code></pre><p>这将输出正确匹配的单词数量的信息，如何 许多人被给出了错误的标签，以及当模型触发多少次 没有真正的言语。有各种各样的参数控制如何<br>信号均值工作，包括设定长度的<code>--average_window_ms</code> 时间平均结果超过，<code>--clip_stride_ms</code>之间的时间<br><code>--suppression_ms</code>型号的应用，停止后续的单词 在发现初始的一段时间后触发一段时间的检测<br><code>--detection_threshold</code>，控制平均得分必须达到多高 在它被认为是一个坚实的结果之前。</p>
<p>您会看到，流式精确度会输出三个数字，而不仅仅是 在训练中使用的一个度量。这是因为不同的应用程序 不同的要求，有些能够容忍频繁的不正确<br>结果只要找到真正的单词（高回忆），而另一些非常集中 确保预测的标签即使有些也很可能是正确的 没有被检测到（高精度）。工具中的数字给你一个想法<br>你的模型将如何在应用程序中执行，你可以尝试调整 信号平均参数调整它给你想要的性能。 要了解什么是适合您的应用程序的正确参数，您可以查看 在创造一个中华民国<br>曲线帮助 你明白这个权衡。</p>
<h2><span id="识别命令">识别命令</span></h2><p>流媒体精确度工具使用一个简单的解码器包含在一个小的C ++类 叫 RecognizeCommands。<br>这个类是随着时间的推移运行TensorFlow模型的输出 对信号进行平均，并在足够的时候返回关于标签的信息 证据认为已经发现了一个公认的词。执行是<br>相当小，只是跟踪最后的几个预测和平均他们， 所以很容易根据需要移植到其他平台和语言。例如，<br>在Android或Python上的Java级别上做类似的事情是很方便的 在树莓派。只要这些实现共享相同的逻辑，你 可以使用流测试来调整控制平均的参数<br>工具，然后将它们转移到您的应用程序以获得类似的结果。</p>
<h2><span id="高级培训">高级培训</span></h2><p>训练脚本的默认设计是为了产生良好的端对端 导致一个相对较小的文件，但有很多选项，你可以 根据自己的要求更改以自定义结果。</p>
<h3><span id="自定义培训数据">自定义培训数据</span></h3><p>默认情况下，脚本将下载语音命令 数据集，但是 你也可以提供你自己的训练数据。为了训练你自己的数据，你 应该确保每个声音至少有数百个录音<br>你想认识，并按课程安排成文件夹。对于 例如，如果你试图从猫咪中识别出狗吠，那么你会的 创建一个名为<code>animal_sounds</code>的根文件夹，然后在那两个之内<br>子文件夹称为<code>bark</code>和<code>miaow</code>。你会然后组织你的音频文件 进入适当的文件夹。</p>
<p>要将脚本指向新的音频文件，您需要将<code>--data_url=</code>设置为 禁用语音命令数据集的下载，以及<br><code>--data_dir=/your/data/folder/</code>来查找刚创建的文件。</p>
<p>文件本身应该是16位小端PCM编码的WAVE格式。该 采样率默认为16,000，但只要所有音频一致 相同的速度（脚本不支持重新采样），你可以改变这一点<br><code>--sample_rate</code>的说法。剪辑也应该大致相同 持续时间。默认的预期持续时间是一秒钟，但您可以使用此设置<br><code>--clip_duration_ms</code>标志。如果你有可变金额的剪辑 在开始的沉默，你可以看看字对齐工具来标准化他们<br>（这里是一个快速和肮脏的方法，你可以使用 太）。</p>
<p>要注意的一个问题是，你可能会有非常类似的重复 在你的数据集中有相同的声音，如果是这样的话，这些可能会产生误导性的指标<br>分布在您的培训，验证和测试集。例如，演讲 设置的命令有人多次重复相同的单词。每一个 这些重复可能是非常接近其他人，所以如果训练是<br>过度拟合和记忆，它可能表现得不切实际 在测试集中看到一个非常相似的副本。为了避免这种危险，语音指令 要确保所有剪辑都包含一个人说的同一个词<br>被放入同一个分区。剪辑分配给培训，测试或 验证集基于它们的文件名的散列，以确保 即使添加了新的剪辑，作业也保持稳定，并避免任何培训<br>样本迁移到其他集合。确保所有给定的发言者 单词在同一个桶中，散列 功能 计算时忽略“nohash”后的文件名中的任何内容<br>分配。这意味着如果你有文件名如<code>pete_nohash_0.wav</code>和 <code>pete_nohash_1.wav</code>，他们保证在同一个集合。</p>
<h3><span id="未知的类">未知的类</span></h3><p>您的应用程序很可能会听到不在您的培训中的声音 设置，你会希望模型表明它不能识别噪音 在这些情况下。为了帮助网络学习什么可以忽略，你需要<br>提供一些既不是你的课堂音频片段。要做到这一点，你会的 创建<code>quack</code>，<code>oink</code>和<code>moo</code>子文件夹， 你的用户可能遇到的其他动物。<br><code>--wanted_words</code>的论点 脚本定义你关心哪些类，所有其他提到的类 在训练期间，子文件夹名称将用于填充<code>_unknown_</code>类。<br>语音命令数据集在其未知的类中包含二十个词，包括 数字从零到九，随机名称如“Sheila”。</p>
<p>默认情况下，10％的训练样例是从未知类中挑选出来的，但是 你可以用<code>--unknown_percentage</code>标志来控制它。增加这个意志<br>使得模型不太可能将不知名的单词误认为想要的单词，而是使其成为可能 它太大，可能会适得其反，因为模型可能会认为这是最安全的分类 所有的话都是未知的！</p>
<h3><span id="背景噪音">背景噪音</span></h3><p>即使有其他不相关的情况，真正的应用程序也必须识别音频 发生在环境中的声音。要建立一个这种强大的模型 的干扰，我们需要训练与录制的音频相似<br>属性。语音命令数据集中的文件被捕获在一个变种上 用户在许多不同的环境中，而不是在工作室，这样的设备 有助于增加一些现实主义的培训。要添加更多，你可以随机混合<br>环境音频段到训练输入。在语音命令中 设置有一个名为<code>_background_noise_</code>的特殊文件夹 一分钟长的WAVE文件，白噪声，机器和日常录音<br>家庭活动。</p>
<p>随机选择这些文件的小片段，并以低量混合 在训练期间进入剪辑。响度也是随机选择的，并受到控制<br>由<code>--background_volume</code>的论点作为一个比例，其中0是沉默，1 是完整的数量。不是所有的剪辑都添加了背景，所以<br><code>--background_frequency</code>标志控制着它们混合的比例。</p>
<p>您自己的应用程序可能在不同的环境中运行 背景噪音模式比这些默认，所以你可以提供自己的音频<br>剪辑在<code>_background_noise_</code>文件夹中。这些应该是相同的采样率 作为你的主要数据集，但是持续时间要长得多，这样一套好的随机性<br>段可以从中选择。</p>
<h3><span id="安静">安静</span></h3><p>在大多数情况下，你所关心的声音将是间歇性的，所以它是 重要的是要知道什么时候没有匹配的音频。为了支持这个，有一个<br>特殊的<code>_silence_</code>标签，指示模型什么时候什么都没有检测到 有趣。因为在真实环境中从来没有完全沉默，我们<br>实际上不得不提供安静和不相关的音频的例子。为此，我们 重新使用<code>_background_noise_</code>文件夹，这个文件夹也混合到了真正的剪辑中，<br>拖动音频数据的短小部分，并将其与地面进行馈送 <code>_silence_</code>的真理级。默认提供10％的训练数据<br>这个，但<code>--silence_percentage</code>可以用来控制比例。如 与未知的话，设置这个更高可以减轻模型的结果有利于<br>真正的保持沉默，牺牲了文字的否定，但也是如此 很大一部分会导致它陷入一直猜测的陷阱 安静。</p>
<h3><span id="时间转移">时间转移</span></h3><p>增加背景噪音是扭曲a中训练数据的一种方式 现实的方式来有效地增加数据集的大小，等等增加 整体的准确性和时间的转移是另一回事。这涉及到一个随机偏移量<br>训练样本数据的时间，使得开始或结束的一小部分是 切断，对面的部分用零填充。这模仿了自然 训练数据中的起始时间的变化，并且由… …控制<br><code>--time_shift_ms</code>标志，默认为100ms。增加这个值将会 提供更多的变化，但有切断重要部分的风险<br>音频。用现实的扭曲来增加数据的一个相关的方法是通过 使用时间拉伸和俯仰 缩放， 但这超出了本教程的范围。</p>
<h2><span id="定制模型">定制模型</span></h2><p>这个脚本使用的默认模型是相当大的，超过8亿 FLOPs为每个推断和使用940,000重量参数。这运行在 台式机或现代手机可用的速度，但涉及太多<br>计算在具有更多限制的设备上以交互式速度运行 资源。为了支持这些用例，有几个选择 可供选择：</p>
<p>low_latency_conv 基于卷积中描述的’cnn-one-fstride4’拓扑 用于小型关键字点击的神经网络 纸。<br>精度略低于“conv”，但重量参数的数量 大致相同，只需要1100万FLOP就能进行一次预测， 使其更快。</p>
<p>要使用此型号，请指定<code>--model_architecture=low_latency_conv</code> 命令行。您还需要更新培训费率和数量<br>的步骤，所以完整的命令将如下所示：</p>
<pre><code>python tensorflow/examples/speech_commands/train \
--model_architecture=low_latency_conv \
--how_many_training_steps=20000,6000 \
--learning_rate=0.01,0.001
</code></pre><p>这就要求脚本以20000步的学习速率进行训练 然后以小10倍的速度微调6000步。</p>
<p>low_latency_svdf 基于压缩深度神经网络的拓扑结构， Rank-Constrained Topology论文。<br>准确度也低于’conv’，但只用了大约75万 参数，最重要的是，它允许在一个优化的执行 测试时间（即，当你真的在你的应用程序中使用它），结果<br>在750万FLOPs。</p>
<p>要使用此型号，请指定<code>--model_architecture=low_latency_svdf</code> 命令行，并更新培训率和数量<br>的步骤，所以完整的命令将如下所示：</p>
<pre><code>python tensorflow/examples/speech_commands/train \
--model_architecture=low_latency_svdf \
--how_many_training_steps=100000,35000 \
--learning_rate=0.01,0.005
</code></pre><p>请注意，尽管需要比前两个更多的步骤 拓扑结构，减少计算的数量意味着训练应该采取 大约在同一时间，最后达到85％左右的精度。<br>你还可以进一步调整拓扑相当容易计算和 通过改变SVDF层中的这些参数来提高精度：</p>
<p>等级 - 近似的等级（更高通常更好，但结果是          更多的计算）。 num_units - 类似于其他图层类型，指定中的节点数<br>层（更多的节点更好的质量，更多的计算）。</p>
<p>关于运行时，由于该层允许缓存一些优化 内部神经网络激活，你需要确保使用一致的 （例如’clip_stride_ms’标志），当你冻结图表，以及什么时候<br>以流模式执行模型（例如test_streaming_accuracy.cc）。</p>
<p>其他参数要定制 如果你想尝试定制模型，一个好的开始是通过 调整光谱图创建参数。这具有改变的效果 输入图像到模型的大小，以及创建代码 models.py<br>将自动调整计算和权重的数量以适应 不同的尺寸。如果您将输入变小，模型将需要更少 计算来处理它，所以它是一个折衷一些准确性的好方法 以改善延迟。<br><code>--window_stride_ms</code>控制每个相隔多远 频率分析样本是从以前的。如果你增加这个值，那么 在给定的持续时间内采样的数量越少，输入的时间轴越少<br>会缩小。 <code>--dct_coefficient_count</code>标志控制着多少桶 用于频率计数，所以减少这个会缩小输入 其他维度。<br><code>--window_size_ms</code>参数不影响大小，但是 确实控制了用于计算频率的区域的宽度 样品。减少训练样本的持续时间，由…控制<br><code>--clip_duration_ms</code>，如果你要找的声音很短， 因为这也减少了输入的时间维度。你需要做的 确保所有的训练数据在开始部分都包含正确的音频<br>虽然剪辑。</p>
<p>如果你对于你的问题有一个完全不同的模型，你可能会发现 你可以插入它 models.py 并让脚本的其余部分处理所有的预处理和训练<br>力学。你会添加一个新的条款<code>create_model</code>，寻找的名字 你的架构，然后调用模型创建功能。这个功能是 给定光谱图输入的大小以及其他模型信息<br>预计会创建TensorFlow操作来读取并生成输出 预测向量和占位符来控制丢失率。剩下的 脚本将把这个模型整合到一个更大的图中去做<br>输入计算并应用softmax和损失函数来训练它。</p>
<p>当你调整模型和训练超参数时，一个常见的问题是 由于数值精度问题，非数值可能会蔓延。在 一般来说，你可以通过减少像学习这样的东西来解决这些问题<br>费率和重量初始化函数，但如果它们是持久的，你可以 启用<code>--check_nans</code>标志来追踪错误的来源。这会<br>在TensorFlow中的大多数常规操作之间插入检查操作，然后中止 培训过程中遇到一个有用的错误消息。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/tensorboard_histograms/" title="TensorBoard直方图仪表板" itemprop="url">TensorBoard直方图仪表板</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="tensorboard直方图仪表板">TensorBoard直方图仪表板</span></h1><p>TensorBoard直方图仪表板显示一些分布的方式 TensorFlow图表中的<code>Tensor</code>随时间变化。它通过显示来实现<br>很多直方图可视化您的张量在不同的时间点。</p>
<h2><span id="一个基本的例子">一个基本的例子</span></h2><p>让我们从一个简单的例子开始：一个正态分布的变量，其中的平均值 随时间变化。 TensorFlow有一个操作 <code>tf.random_normal</code><br>这是完美的这个目的。和TensorBoard通常一样，我们 将使用摘要操作来摄取数据;在这种情况下， ‘tf.summary.histogram’。<br>有关总结如何工作的初步介绍，请参阅总结 TensorBoard教程。</p>
<p>这是一个代码片段，将生成一些直方图摘要包含 正态分布的数据，其中分布的均值增加 时间。</p>
<pre><code>import tensorflow as tf

k = tf.placeholder(tf.float32)

# Make a normal distribution, with a shifting mean
mean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)
# Record that distribution into a histogram summary
tf.summary.histogram(&quot;normal/moving_mean&quot;, mean_moving_normal)

# Setup a session and summary writer
sess = tf.Session()
writer = tf.summary.FileWriter(&quot;/tmp/histogram_example&quot;)

summaries = tf.summary.merge_all()

# Setup a loop and write the summaries to disk
N = 400
for step in range(N):
  k_val = step/float(N)
  summ = sess.run(summaries, feed_dict={k: k_val})
  writer.add_summary(summ, global_step=step)
</code></pre><p>一旦代码运行，我们可以通过命令行将数据加载到TensorBoard中：</p>
<pre><code>tensorboard --logdir=/tmp/histogram_example
</code></pre><p>一旦TensorBoard正在运行，将其加载到Chrome或Firefox并导航到 直方图仪表板。然后我们可以看到我们通常的直方图可视化 分布式数据。</p>
<p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/1_moving_mean.png" alt=""></p>
<p><code>tf.summary.histogram</code>采取任意大小和形状的张量，并且 将其压缩成由许多分箱组成的直方图数据结构<br>宽度和数量。例如，假设我们要组织这些数字 <code>[0.5, 1.1, 1.3, 2.2, 2.9, 2.99]</code>分为箱子。我们可以做三个箱子：  一个箱子<br>包含从0到1的所有内容（它将包含一个元素，0.5），  一个箱子 包含1-2的所有内容（包含1.1和1.3两个元素），<br>*包含2-3的所有内容（它将包含三个元素：2.2， 2.9和2.99）。</p>
<p>TensorFlow使用类似的方法来创建垃圾箱，但不像我们的例子那样 不会创建整数箱。对于大型稀疏数据集，可能会导致 数千个垃圾箱。<br>相反，箱是指数分布的，许多箱接近0和 相当数量的箱子比较少。 然而，可视化指数分布的垃圾箱是棘手的;如果使用高度<br>对数字进行编码，然后更宽的箱子占用更多的空间，即使它们具有相同的空间 元素的数量。相反，在该地区编码计数使高度 比较不可能。相反，直方图重新采样数据<br>进入统一的箱子。这在某些情况下会导致不幸的文物。</p>
<p>直方图可视化器中的每个切片显示单个直方图。 切片按步骤组织; 较旧的切片（例如，步骤0）进一步“后退”而较暗，而较新的切片<br>（例如步骤400）靠近前景，并且颜色较浅。 右边的y轴显示步骤编号。</p>
<p>您可以将鼠标悬停在柱状图上，查看更详细的工具提示 信息。例如，在下图中我们可以看到直方图 在时间步骤176具有以2.25为中心的仓，在该仓中有177个元素。</p>
<p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/2_moving_mean_tooltip.png" alt=""></p>
<p>此外，您可能会注意到，直方图切片并不总是均匀分布在中 步数或时间。这是因为TensorBoard使用 水库取样保持一个<br>所有直方图的子集，以节省内存。油藏采样保证 每个样本都有相同的被包含的可能性，但是因为它是 一个随机的算法，选择的样本不会在偶数阶段出现。</p>
<h2><span id="覆盖模式">覆盖模式</span></h2><p>仪表板左侧有一个控件，可以切换 直方图模式从“偏移”到“覆盖”：</p>
<p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/3_overlay_offset.png" alt=""></p>
<p>在“偏移”模式下，可视化旋转45度，使个人 直方图切片不再分散，而是全部绘制 在同一个Y轴上。</p>
<p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/4_overlay.png" alt=""><br>现在，每个切片都是图表上的一个单独的行，y轴显示该项目 在每个桶内计数。较深的线条较旧，较早的步骤较轻<br>线条是更新的，后面的步骤。再次，您可以将鼠标悬停在图表上 看到一些额外的信息。</p>
<p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/5_overlay_tooltips.png" alt=""></p>
<p>一般来说，如果要直接进行比较，覆盖可视化是有用的 不同直方图的计数。</p>
<h2><span id="多模式分配">多模式分配</span></h2><p>直方图仪表板非常适合可视化多模式 分布。让我们通过连接构造一个简单的双峰分布 来自两个不同正态分布的输出。代码看起来像 这个：</p>
<pre><code>import tensorflow as tf

k = tf.placeholder(tf.float32)

# Make a normal distribution, with a shifting mean
mean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)
# Record that distribution into a histogram summary
tf.summary.histogram(&quot;normal/moving_mean&quot;, mean_moving_normal)

# Make a normal distribution with shrinking variance
variance_shrinking_normal = tf.random_normal(shape=[1000], mean=0, stddev=1-(k))
# Record that distribution too
tf.summary.histogram(&quot;normal/shrinking_variance&quot;, variance_shrinking_normal)

# Let&apos;s combine both of those distributions into one dataset
normal_combined = tf.concat([mean_moving_normal, variance_shrinking_normal], 0)
# We add another histogram summary to record the combined distribution
tf.summary.histogram(&quot;normal/bimodal&quot;, normal_combined)

summaries = tf.summary.merge_all()

# Setup a session and summary writer
sess = tf.Session()
writer = tf.summary.FileWriter(&quot;/tmp/histogram_example&quot;)

# Setup a loop and write the summaries to disk
N = 400
for step in range(N):
  k_val = step/float(N)
  summ = sess.run(summaries, feed_dict={k: k_val})
  writer.add_summary(summ, global_step=step)
</code></pre><p>你已经记得我们的例子中的“移动均值”正态分布 以上。现在我们也有一个“收缩差异”的分布。他们并排 看起来像这样：<br><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/6_two_distributions.png" alt=""></p>
<p>当我们把它们连接起来的时候，我们得到一张清楚地显示出不同的图表， 双峰结构：<br><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/7_bimodal.png" alt=""></p>
<h2><span id="更多的分布">更多的分布</span></h2><p>为了好玩，让我们生成并可视化一些更多的发行版，然后 将它们组合成一个图表。以下是我们将使用的代码：</p>
<pre><code>import tensorflow as tf

k = tf.placeholder(tf.float32)

# Make a normal distribution, with a shifting mean
mean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)
# Record that distribution into a histogram summary
tf.summary.histogram(&quot;normal/moving_mean&quot;, mean_moving_normal)

# Make a normal distribution with shrinking variance
variance_shrinking_normal = tf.random_normal(shape=[1000], mean=0, stddev=1-(k))
# Record that distribution too
tf.summary.histogram(&quot;normal/shrinking_variance&quot;, variance_shrinking_normal)

# Let&apos;s combine both of those distributions into one dataset
normal_combined = tf.concat([mean_moving_normal, variance_shrinking_normal], 0)
# We add another histogram summary to record the combined distribution
tf.summary.histogram(&quot;normal/bimodal&quot;, normal_combined)

# Add a gamma distribution
gamma = tf.random_gamma(shape=[1000], alpha=k)
tf.summary.histogram(&quot;gamma&quot;, gamma)

# And a poisson distribution
poisson = tf.random_poisson(shape=[1000], lam=k)
tf.summary.histogram(&quot;poisson&quot;, poisson)

# And a uniform distribution
uniform = tf.random_uniform(shape=[1000], maxval=k*10)
tf.summary.histogram(&quot;uniform&quot;, uniform)

# Finally, combine everything together!
all_distributions = [mean_moving_normal, variance_shrinking_normal,
                     gamma, poisson, uniform]
all_combined = tf.concat(all_distributions, 0)
tf.summary.histogram(&quot;all_combined&quot;, all_combined)

summaries = tf.summary.merge_all()

# Setup a session and summary writer
sess = tf.Session()
writer = tf.summary.FileWriter(&quot;/tmp/histogram_example&quot;)

# Setup a loop and write the summaries to disk
N = 400
for step in range(N):
  k_val = step/float(N)
  summ = sess.run(summaries, feed_dict={k: k_val})
  writer.add_summary(summ, global_step=step)
</code></pre><h3><span id="伽玛分布">伽玛分布</span></h3><p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/8_gamma.png" alt=""></p>
<h3><span id="统一分配">统一分配</span></h3><p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/9_uniform.png" alt=""></p>
<h3><span id="泊松分布">泊松分布</span></h3><p><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/10_poisson.png" alt=""><br>泊松分布是在整数上定义的。所以，所有的价值 被生成是完美的整数。直方图压缩移动数据 进入浮点箱，导致可视化显示很少 颠覆整数值而不是完美的尖峰。</p>
<h3><span id="现在都在一起了">现在都在一起了</span></h3><p>最后，我们可以将所有的数据连接成一个有趣的曲线。<br><img src="https://www.tensorflow.org/images/tensorboard/histogram_dashboard/11_all_combined.png" alt=""></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/faq/" title="经常问的问题" itemprop="url">经常问的问题</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="经常问的问题">经常问的问题</span></h1><p>本文档提供了有关某些常见问题的答案 TensorFlow。如果你有一个这里没有涉及的问题，你可能会找到一个 回答一个TensorFlow社区资源。</p>
<h2><span id="功能和兼容性">功能和兼容性</span></h2><h4><span id="我可以在多台电脑上运行分布式培训吗">我可以在多台电脑上运行分布式培训吗？</span></h4><p>是! TensorFlow获得了 支持分布式计算 版本0.8。 TensorFlow现在支持一个或多个设备（CPU和GPU） 更多的电脑。</p>
<h4><span id="tensorflow与python-3一起工作吗">TensorFlow与Python 3一起工作吗？</span></h4><p>从0.6.0发布时间（2015年12月初）开始，我们支持Python 3.3+。</p>
<h2><span id="建立张量流图">建立张量流图</span></h2><p>另见 关于建筑图的API文档。</p>
<h4><span id="为什么c-tfmatmula-b不立即执行矩阵乘法">为什么<code>c = tf.matmul(a, b)</code>不立即执行矩阵乘法？</span></h4><p>在TensorFlow Python API中，<code>a</code>，<code>b</code>和<code>c</code>分别是 <code>tf.Tensor</code>物体。一个<code>Tensor</code>的对象是<br>一个操作结果的符号句柄，但实际上并不包含该句柄 操作输出的值。相反，TensorFlow鼓励用户构建 把复杂的表达式（比如整个神经网络和它的梯度）表示为<br>一个数据流图。然后卸载整个数据流图的计算 （或其子图）添加到TensorFlow中 <code>tf.Session</code>，它能够执行 整个计算比执行操作更有效率<br>一个接一个。</p>
<h4><span id="如何命名设备">如何命名设备？</span></h4><p>支持的设备名称为CPU的<code>&quot;/device:CPU:0&quot;</code>（或<code>&quot;/cpu:0&quot;</code>）<br>设备以及用于第i个GPU设备的<code>&quot;/device:GPU:i&quot;</code>（或<code>&quot;/gpu:i&quot;</code>）。</p>
<h4><span id="如何在特定设备上进行操作">如何在特定设备上进行操作？</span></h4><p>要在设备上放置一组操作，请在一个设备中创建它们 <code>with tf.device(name):</code>上下文。看到 如何在文件上<br>使用GPU与TensorFlow的细节如何 TensorFlow将操作分配给设备， CIFAR-10教程中的示例模型 使用多个GPU。</p>
<h2><span id="运行一个tensorflow计算">运行一个TensorFlow计算</span></h2><p>另见 关于运行图形的API文档。</p>
<h4><span id="什么是喂养和占位符交易">什么是喂养和占位符交易？</span></h4><p>Feeding是TensorFlow Session API中的一种机制，可以让您 在运行时替换一个或多个张量的不同值。 <code>feed_dict</code><br>对<code>tf.Session.run</code>的说法是一个 将<code>tf.Tensor</code>对象映射到的字典 numpy数组（和一些其他类型），将被用作这些值<br>张量在执行一个步骤。</p>
<p>通常情况下，你有一定的张量，如投入，总是会被喂食。该 <code>tf.placeholder</code> op允许你 定义必须被馈送的张量，并且可选地允许你约束<br>他们的形状也是如此。看到了 初学者的MNIST教程 举例说明如何使用占位符和喂食来提供训练数据 为神经网络。</p>
<h4><span id="sessionrun和tensoreval有何区别"><code>Session.run()</code>和<code>Tensor.eval()</code>有何区别？</span></h4><p>如果<code>t</code>是<code>tf.Tensor</code>的对象， <code>tf.Tensor.eval</code>是速记 <code>tf.Session.run</code>（其中<code>sess</code>是<br>目前<code>tf.get_default_session</code>。该 以下两段代码片段是等价的：</p>
<pre><code># Using `Session.run()`.
sess = tf.Session()
c = tf.constant(5.0)
print(sess.run(c))

# Using `Tensor.eval()`.
c = tf.constant(5.0)
with tf.Session():
  print(c.eval())
</code></pre><p>在第二个例子中，会话充当一个 上下文经理， 它具有将其安装为默认会话的效果 <code>with</code>模块。上下文管理器的方法可以导致更简洁的代码<br>简单用例（如单元测试）;如果你的代码处理多个图表和 会话，明确的呼叫可能更直接 <code>Session.run()</code>。</p>
<h4><span id="会议有一生吗中间张量怎么样">会议有一生吗？中间张量怎么样？</span></h4><p>会话可以拥有自己的资源，比如 <code>tf.Variable</code>， <code>tf.QueueBase</code>，和 <code>tf.ReaderBase</code>;而这些资源可以使用<br>大量的记忆。这些资源（和相关的内存）是 会议结束后通过电话发布 <code>tf.Session.close</code>。</p>
<p>作为呼叫的一部分创建的中间张量 <code>Session.run()</code>将在或之前被释放 通话结束。</p>
<h4><span id="运行时并行化图的执行部分">运行时并行化图的执行部分？</span></h4><p>TensorFlow运行时间跨多种不同的并行图表执行 尺寸：</p>
<p>单独的操作具有并行的实现，使用a中的多个核心   CPU或GPU中的多个线程。 TensorFlow图中的独立节点可以并行运行多个<br>设备，这使得加快成为可能   CIFAR-10使用多个GPU进行培训。 会话API允许多个并发步骤（即呼叫   <code>tf.Session.run</code>并联。这个<br>如果单个步骤不使用，则可以使运行时获得更高的吞吐量   计算机中的所有资源。</p>
<h4><span id="tensorflow支持哪些客户端语言">TensorFlow支持哪些客户端语言？</span></h4><p>TensorFlow旨在支持多种客户端语言。 目前，支持最好的客户端语言是Python。的实验接口 执行和构建图也可用于 C ++，Java和Go。</p>
<p>TensorFlow也有一个 基于C的客户端API 帮助构建对更多客户端语言的支持。我们邀请新的贡献 语言绑定。</p>
<p>由TensorFlow维护人员支持的C API之上的开源社区创建和支持的各种其他语言（如C＃，Julia，Ruby和Scala）的绑定。</p>
<h4><span id="tensorflow是否使用我的机器上可用的所有设备gpu和cpu">TensorFlow是否使用我的机器上可用的所有设备（GPU和CPU）？</span></h4><p>TensorFlow支持多个GPU和CPU。请参阅关于如何使用的文档 使用GPU与TensorFlow的细节如何 TensorFlow将操作分配给设备，<br>CIFAR-10教程中的示例模型 使用多个GPU。</p>
<p>请注意，TensorFlow只使用计算能力更强的GPU设备 比3.5。</p>
<h4><span id="为什么sessionrun在使用阅读器或队列时会挂起">为什么<code>Session.run()</code>在使用阅读器或队列时会挂起？</span></h4><p><code>tf.ReaderBase</code>和 <code>tf.QueueBase</code>类提供了特殊的操作 可以阻塞，直到输入（或有界队列中的空闲空间）变成为止<br>可用。这些操作允许您构建复杂的 输入管道，在做的成本 TensorFlow计算有点复杂。请参阅how-to文档 对于 运用<br><code>QueueRunner</code>对象驱动队列和阅读器 有关如何使用它们的更多信息。</p>
<h2><span id="变量">变量</span></h2><p>另请参阅有关变量和方法的how-to文档 变量的API文档。</p>
<h4><span id="什么是一个变量的生命周期">什么是一个变量的生命周期？</span></h4><p>首次运行时会创建一个变量 <code>tf.Variable.initializer</code> 在会话中为该变量操作。那个时候被破坏了 <code>tf.Session.close</code>。</p>
<h4><span id="变量如何被同时访问">变量如何被同时访问？</span></h4><p>变量允许并发的读写操作。从a中读取的值 变量可能会改变，如果它同时更新。默认情况下，并发 对变量的赋值操作允许运行而不互斥。<br>将<code>use_locking=True</code>分配给变量时要获取锁定 <code>tf.Variable.assign</code>。</p>
<h2><span id="张量形状">张量形状</span></h2><p>另见 <code>tf.TensorShape</code>。</p>
<h4><span id="我如何确定python中张量的形状">我如何确定Python中张量的形状？</span></h4><p>在TensorFlow中，张量具有静态（推断）形状和动态（真实） 形状。静态形状可以使用 <code>tf.Tensor.get_shape</code><br>方法：这个形状是从用来创建的操作中推断出来的 张量，可能是 部分完成。如果是静态的 <code>Tensor</code> <code>t</code>的动态形状可以是<br>通过评估<code>tf.shape(t)</code>来确定。</p>
<h4><span id="xset_shape和x-tfreshapex有何区别"><code>x.set_shape()</code>和<code>x = tf.reshape(x)</code>有何区别？</span></h4><p><code>tf.Tensor.set_shape</code>方法更新 <code>Tensor</code>物体的静态形状，通常用于提供 当不能直接推断出附加的形状信息。它不是<br>改变张量的动态形状。</p>
<p><code>tf.reshape</code>操作创建 一个具有不同动态形状的新张量。</p>
<h4><span id="我如何构建一个可用于批量变量的图形">我如何构建一个可用于批量变量的图形？</span></h4><p>建立一个可变批处理大小的图形常常是有用的 例如，以便相同的代码可以用于（小型）批量训练 单实例推断。结果图可以 保存为协议缓冲区 和 导入另一个程序。</p>
<p>在构建可变大小的图时，最重要的是记住不是 将批量大小编码为Python常量，而是使用符号 <code>Tensor</code>来代表它。以下提示可能会有用：</p>
<p>使用<code>batch_size = tf.shape(input)[0]</code>   从称为<code>Tensor</code>的<code>input</code>中提取批量维度，并将其存储<br>称为<code>Tensor</code>的<code>batch_size</code>。 改用<code>tf.reduce_mean</code>   <code>tf.reduce_sum(...) /
batch_size</code>。</p>
<h2><span id="tensorboard">TensorBoard</span></h2><h4><span id="我如何可视化张量流图">我如何可视化张量流图？</span></h4><p>看图可视化教程。</p>
<h4><span id="向tensorboard发送数据最简单的方法是什么">向TensorBoard发送数据最简单的方法是什么？</span></h4><p>将总结操作添加到您的TensorFlow图表中，然后编写 这些摘要到一个日志目录。然后，启动TensorBoard使用</p>
<pre><code>python tensorflow/tensorboard/tensorboard.py --logdir=path/to/log-directory
</code></pre><p>有关更多详细信息，请参阅 摘要和TensorBoard教程。</p>
<h4><span id="每次启动tensorboard我都会得到一个网络安全弹出窗口">每次启动TensorBoard，我都会得到一个网络安全弹出窗口！</span></h4><p>您可以将TensorBoard更改为本地主机而不是“0.0.0.0” 标志–host = localhost。这应该消除任何安全警告。</p>
<h2><span id="扩展tensorflow">扩展TensorFlow</span></h2><p>请参阅如何使用文档 添加一个新的操作到TensorFlow。</p>
<h4><span id="我的数据是自定义格式-我如何使用tensorflow读取它">我的数据是自定义格式。我如何使用TensorFlow读取它？</span></h4><p>以自定义格式处理数据有三个主要选项。</p>
<p>最简单的选择是编写用Python转换数据的解析代码 成一个numpy数组。然后用<code>tf.data.Dataset.from_tensor_slices</code>来<br>从内存数据创建输入管道。</p>
<p>如果您的数据不适合内存，请尝试在数据集中进行解析 管道。从适当的文件阅读器开始，像<br><code>tf.data.TextLineDataset</code>。然后通过映射转换数据集 映射适当的操作。<br>优先使用预定义的TensorFlow操作，如<code>tf.decode_raw</code>，<br><code>tf.decode_csv</code>，<code>tf.parse_example</code>或<code>tf.image.decode_png</code>。</p>
<p>如果您的数据不能通过内置的TensorFlow操作轻松解析， 考虑将其脱机转换为易于解析的格式 作为$<br>{tf.python_io.TFRecordWriter $ <code>TFRecord</code>}格式。</p>
<p>定制解析行为的更有效的方法是 添加一个用C ++编写的解析你的 数据格式。处理新数据格式的指南已经 有关执行此操作的步骤的更多信息。</p>
<h2><span id="杂">杂</span></h2><h4><span id="什么是tensorflow的编码风格约定">什么是TensorFlow的编码风格约定？</span></h4><p>TensorFlow Python API遵循 PEP8约定 特别是，我们使用<code>CamelCase</code>名称作为类别，<code>snake_case</code>名称<br>功能，方法和属性。我们也坚持了 Google Python风格指南。</p>
<p>TensorFlow C ++代码库遵守 Google C ++风格指南</p>
<p>（*有一个例外：我们使用2空格缩进而不是4空格 缩进。）</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/pros/" title="专家的深度MNIST" itemprop="url">专家的深度MNIST</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="专家的深度mnist">专家的深度MNIST</span></h1><p>TensorFlow是进行大规模数值计算的强大库。 它擅长的任务之一是实施和训练深层神经 网络。在本教程中，我们将学习a的基本构建块<br>TensorFlow模型，同时构建一个深度卷积MNIST分类器。</p>
<p>这个介绍假定熟悉神经网络和MNIST 数据集。如果你没有 与他们的背景，检查出 初学者介绍。务必 在开始之前安装TensorFlow。</p>
<h2><span id="关于本教程">关于本教程</span></h2><p>本教程的第一部分解释了正在发生的事情 mnist_softmax.py 代码，这是一个Tensorflow模型的基本实现。第二部分<br>显示了一些提高准确性的方法。</p>
<p>您可以将本教程中的每个代码片段复制并粘贴到Python中 环境跟随，或者你可以下载完全实施的深网 来自mnist_deep.py 。</p>
<p>我们将在本教程中完成的任务：</p>
<p>创建一个softmax回归函数，该函数是识别MNIST的模型   数字，基于查看图像中的每个像素 使用Tensorflow来训练模型，通过“看”来识别数字<br>数以千计的例子（并运行我们的第一个Tensorflow会话来这样做） 用我们的测试数据检查模型的准确性 建立，训练和测试一个多层卷积神经网络来改善   结果</p>
<h2><span id="建立">建立</span></h2><p>在我们创建模型之前，我们将首先加载MNIST数据集，然后启动一个 TensorFlow会议。</p>
<h3><span id="加载mnist数据">加载MNIST数据</span></h3><p>如果您正在复制和粘贴本教程中的代码，请从此处开始 这两行代码将自动下载并读取数据：</p>
<pre><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True)
</code></pre><p>这里<code>mnist</code>是一个轻量级的存储培训，验证和 测试设置为NumPy数组。它还提供了一个遍历的函数 数据minibatches，我们将在下面使用。</p>
<h3><span id="启动tensorflow-interactivesession">启动TensorFlow InteractiveSession</span></h3><p>TensorFlow依靠高效的C ++后端来完成它的计算。该 连接到这个后端被称为一个会话。 TensorFlow的常见用法<br>程序是先创建一个图形，然后在会话中启动它。</p>
<p>在这里，我们使用方便的<code>InteractiveSession</code>类，这使得 TensorFlow对于如何构建代码更灵活。它允许你 交错操作，建立一个 计算图<br>与那些运行图表。这在工作时特别方便 交互式上下文如IPython。如果你不使用一个 <code>InteractiveSession</code>，那么你应该建立之前的整个计算图<br>开始会议和 启动图表。</p>
<pre><code>import tensorflow as tf
sess = tf.InteractiveSession()
</code></pre><h4><span id="计算图">计算图</span></h4><p>为了在Python中进行高效的数值计算，我们通常使用像 NumPy做矩阵等昂贵的操作 在Python之外进行乘法，使用在其中实现的高效代码<br>另一种语言。不幸的是，还是会有很多开销 切换回Python的每一个操作。如果你这个开销特别糟糕 想要在GPU上运行计算或以分布式方式运行，在哪里可以<br>传输数据的成本很高。</p>
<p>TensorFlow也在Python之外做了繁重的工作，但是它需要一些东西 进一步避免这种开销。而不是运行一个昂贵的<br>独立于Python的操作，TensorFlow让我们描述一个图 完全在Python之外运行的交互操作。这种方法是 类似于Theano或火炬中使用的。</p>
<p>Python代码的作用是建立这个外部计算 图表，并规定应该运行计算图的哪个部分。看到 计算图 有关TensorFlow入门的更多详细信息。</p>
<h2><span id="建立一个softmax回归模型">建立一个Softmax回归模型</span></h2><p>在本节中，我们将建立一个单线性的softmax回归模型 层。在下一节中，我们将扩展到softmax的情况 回归多层卷积网络。</p>
<h3><span id="占位符">占位符</span></h3><p>我们通过创建节点来开始构建计算图 输入图像和目标输出类。</p>
<pre><code>x = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])
</code></pre><p>这里<code>x</code>和<code>y_</code>不是特定的值。相反，它们都是<code>placeholder</code> - 我们要求TensorFlow运行计算时输入的值。</p>
<p>输入图像<code>x</code>将由浮点数的二维张量组成。 <code>shape</code> <code>[None, 784]</code>是<code>784</code>的维度<br>单个28×28像素的MNIST图像，而<code>None</code>则表明了这一点 第一维度，对应于批量大小，可以是任何大小。该<br>目标输出类别<code>y_</code>也将包含一个二维张量，其中每一行是一个 指示哪个数字类（零到九）的一个热点10维向量， 对应的MNIST图像属于。</p>
<p><code>shape</code>的<code>placeholder</code>参数是可选的，但它允许TensorFlow 自动捕捉源自不一致张量形状的错误。</p>
<h3><span id="变量">变量</span></h3><p>我们现在定义的重量<code>W</code>和<code>b</code>偏向于我们的模型。我们可以想象 对待这些额外的投入，但TensorFlow有一个更好的办法 处理它们：<code>Variable</code>。<br><code>Variable</code>是一个生活在TensorFlow中的价值 计算图。它可以被使用，甚至被计算修改。在 机器学习应用程序，一般具有模型参数<br><code>Variable</code>s。</p>
<pre><code>W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
</code></pre><p>我们将呼叫中每个参数的初始值传递给<code>tf.Variable</code>。在 在这种情况下，我们将<code>W</code>和<code>b</code>初始化为满零的张量。 <code>W</code>是一款<br>784x10矩阵（因为我们有784个输入功能和10个输出）和<code>b</code>是一个 10维矢量（因为我们有10个类）。</p>
<p>在会话中可以使用<code>Variable</code>之前，必须使用初始化 会议。这一步取初始值（在这种情况下，张量满了 零）已经被指定，并分配给每个 <code>Variable</code>。<br><code>Variables</code>可以一次完成：</p>
<pre><code>sess.run(tf.global_variables_initializer())
</code></pre><h3><span id="预测类和损失函数">预测类和损失函数</span></h3><p>我们现在可以实现我们的回归模型。只需要一行！我们乘 向量化的输入图像<code>x</code>由权重矩阵<code>W</code>添加偏置<code>b</code>。</p>
<pre><code>y = tf.matmul(x,W) + b
</code></pre><p>我们可以很容易地指定一个损失函数。损失表明有多糟糕 模型的预测就是一个例子。我们尽量减少这一点 培训所有的例子。在这里，我们的损失函数是交叉熵<br>目标和应用于模型的softmax激活函数之间 预测。正如在初学者教程中，我们使用稳定的公式：</p>
<pre><code>cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
</code></pre><p>请注意<code>tf.nn.softmax_cross_entropy_with_logits</code>内部应用 softmax模型的非标准模型预测和总和<br><code>tf.reduce_mean</code>取平均值。</p>
<h2><span id="训练模型">训练模型</span></h2><p>现在我们已经定义了我们的模型和训练损失函数了 直接训练使用TensorFlow。因为TensorFlow知道整个 计算图，它可以使用自动分化来找到梯度<br>相对于每个变量的损失。 TensorFlow有多种 内置的优化算法。 对于这个例子，我们将使用最陡的梯度下降，步长为 0.5，下降交叉熵。</p>
<pre><code>train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
</code></pre><p>TensorFlow在这一行中实际上做了什么是增加新的操作 计算图。这些操作包括计算梯度的操作， 计算参数更新步骤，并将更新步骤应用于参数。</p>
<p>运行的返回操作<code>train_step</code>将应用梯度下降 更新参数。训练模型可以通过 反复运行<code>train_step</code>。</p>
<pre><code>for _ in range(1000):
  batch = mnist.train.next_batch(100)
  train_step.run(feed_dict={x: batch[0], y_: batch[1]})
</code></pre><p>我们在每次训练迭代中加载100个训练样例。我们然后运行 <code>train_step</code>操作，用<code>feed_dict</code>代替<code>placeholder</code>张力器<br><code>x</code>和<code>y_</code>。请注意，您可以替换任何张量 在使用<code>feed_dict</code>的计算图表中 - 并不仅限于此 <code>placeholder</code>s。</p>
<h3><span id="评估模型">评估模型</span></h3><p>我们的模型有多好？</p>
<p>首先我们要弄清楚我们在哪里预测了正确的标签。 <code>tf.argmax</code>是一款 非常有用的功能，它给你一个最高的条目索引<br>张量沿一些轴。例如，<code>tf.argmax(y,1)</code>是我们的型号的标签 认为是最有可能的每个输入，而<code>tf.argmax(y_,1)</code>是真实的<br>标签。我们可以使用<code>tf.equal</code>来检查我们的预测是否符合事实。</p>
<pre><code>correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
</code></pre><p>这给了我们一个布尔的列表。为了确定什么部分是正确的，我们 投到浮点数，然后取平均值。例如， <code>[True, False, True,
True]</code>将成为<code>[1,0,1,1]</code>，成为<code>0.75</code>。</p>
<pre><code>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
</code></pre><p>最后，我们可以评估我们的测试数据的准确性。这应该是关于 92％正确。</p>
<pre><code>print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
</code></pre><h2><span id="构建一个多层卷积网络">构建一个多层卷积网络</span></h2><p>在MNIST上获得92％的准确性是不好的。这几乎是令人尴尬的坏事。在这 部分，我们将解决这个问题，从一个非常简单的模型跳到某个东西<br>中等复杂度：一个小的卷积神经网络。这将得到我们 达到99.2％左​​右的精确度 - 而不是技术水平，但相当可观。</p>
<p>下面是一个用TensorBoard创建的关于我们将要构建的模型的图表：</p>
<p><img src="https://www.tensorflow.org/images/mnist_deep.png" alt=""></p>
<h3><span id="重量初始化">重量初始化</span></h3><p>要创建这个模型，我们需要创建很多权重和偏见。 一般应该用少量的噪音初始化权重 对称性破坏，并防止0梯度。因为我们正在使用 ReLU神经元，它是<br>也是一个良好的做法初始化他们有一个稍微积极的初始偏见 避免“死神经元”。在我们建立模型的时候，而不是重复地做这个， 让我们创建两个方便的函数来为我们做。</p>
<pre><code>def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)
</code></pre><h3><span id="卷积和汇集">卷积和汇集</span></h3><p>TensorFlow也为我们提供了很多卷积和合并的灵活性 操作。我们如何处理边界？我们的步幅是多少？ 在这个例子中，我们总是选择香草版本。<br>我们的卷积使用了一个步幅，并且是零填充的 输出与输入大小相同。我们的池是老式的最大池 超过2×2块。为了保持我们的代码更清晰，我们也抽象这些操作 进入功能。</p>
<pre><code>def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)
</code></pre><h3><span id="第一卷积层">第一卷积层</span></h3><p>我们现在可以实现我们的第一层。它将包括卷积，其次 通过最大池。卷积将为每个5x5补丁计算32个特征。 其重量张量将具有<code>[5, 5, 1,
32]</code>的形状。前两个 尺寸是补丁大小，下一个是输入通道的数量， 最后是输出通道的数量。我们也将有一个偏向量 每个输出通道的组件。</p>
<pre><code>W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])
</code></pre><p>为了应用该层，我们首先将<code>x</code>重塑为4d张量，第二张和 第三个尺寸对应的图像宽度和高度，和最后 尺寸对应于颜色通道的数量。</p>
<pre><code>x_image = tf.reshape(x, [-1, 28, 28, 1])
</code></pre><p>然后我们将<code>x_image</code>与重量张量进行叠加，加上 偏差，应用ReLU功能，最后是最大池。 <code>max_pool_2x2</code>方法将会<br>将图像尺寸缩小到14x14。</p>
<pre><code>h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
</code></pre><h3><span id="第二卷积层">第二卷积层</span></h3><p>为了建立一个深层网络，我们堆叠了这种类型的几个层。该 第二层将为每个5x5补丁有64个功能。</p>
<pre><code>W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)
</code></pre><h3><span id="密集连接层">密集连接层</span></h3><p>现在图像大小已经减小到7x7，我们添加一个完全连接的图层 有1024个神经元允许在整个图像上进行处理。我们重塑张量 从池层到一批载体，<br>乘以权重矩阵，添加偏差，并应用ReLU。</p>
<pre><code>W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
</code></pre><h4><span id="退出">退出</span></h4><p>为了减少过拟合，我们将在读出层之前应用丢失。 我们创建一个<code>placeholder</code>，用于保持神经元输出的概率<br>在辍学。这可以让我们在训练过程中变成辍学的，并将其转变 在测试期间关闭。 TensorFlow的<code>tf.nn.dropout</code>可自动处理缩放神经元输出<br>除了掩盖他们，所以辍学只是没有任何额外的工作 scaling.1</p>
<pre><code>keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
</code></pre><h3><span id="读出层">读出层</span></h3><p>最后，我们添加一个图层，就像一个图层softmax回归一样 以上。</p>
<pre><code>W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
</code></pre><h3><span id="训练和评估模型">训练和评估模型</span></h3><p>这个模型有多好？为了训练和评估，我们将使用代码 几乎与上面简单的一层SoftMax网络相同。</p>
<p>不同之处在于：</p>
<p>我们将更换最陡的梯度下降优化器   复杂的ADAM优化器。 我们将在<code>keep_prob</code>中增加额外的参数<code>feed_dict</code>进行控制   辍学率。<br>我们将在训练过程中每100次迭代添加一次记录。</p>
<p>我们也将使用tf.Session而不是tf.InteractiveSession。这个更好 分离创建图形（模型说明）的过程和<br>评估图的过程（模型拟合）。它通常使更清洁 码。 tf.Session是在<code>with</code>模块中创建的 所以一旦块被退出，它就会被自动销毁。</p>
<p>随意运行这个代码。请注意，它会进行20,000次训练迭代 并可能需要一段时间（可能长达半小时），这取决于您的处理器。</p>
<pre><code>cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for i in range(20000):
    batch = mnist.train.next_batch(50)
    if i % 100 == 0:
      train_accuracy = accuracy.eval(feed_dict={
          x: batch[0], y_: batch[1], keep_prob: 1.0})
      print(&apos;step %d, training accuracy %g&apos; % (i, train_accuracy))
    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

  print(&apos;test accuracy %g&apos; % accuracy.eval(feed_dict={
      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))
</code></pre><p>运行此代码后的最终测试集精度应该约为99.2％。</p>
<p>我们已经学会了如何快速，轻松地构建，培训和评估一个 使用TensorFlow的相当复杂的深度学习模型。</p>
<p>1：对于这个小卷积网络，性能实际上几乎是相同的，没有丢失。辍学对于减少过度劳累通常是非常有效的，但是在训练非常大的神经网络时它是最有用的。 ↩</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/how-to-determine-the-current-shell-im-working-on/" title="如何确定我正在工作的当前shell？" itemprop="url">如何确定我正在工作的当前shell？</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>我如何确定我正在使用的当前shell？</p>
<p><code>ps</code>命令的输出是否足够？</p>
<p>这怎么能在不同的UNIX中完成呢？</p>
        
        
        <p class="article-more-link">
          
            <a href="/2018/01/01/how-to-determine-the-current-shell-im-working-on/#more">Read More</a>
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/bash/">bash</a><a href="/tags/unix/">unix</a><a href="/tags/shell/">shell</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/summaries_and_tensorboard/" title="TensorBoard：可视化学习" itemprop="url">TensorBoard：可视化学习</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="tensorboard可视化学习">TensorBoard：可视化学习</span></h1><p>你将使用TensorFlow进行计算 - 就像训练一个巨大的 深度神经网络 - 可能是复杂和混乱的。为了更容易<br>理解，调试和优化TensorFlow程序，我们已经包含了一套 可视化工具称为TensorBoard。您可以使用TensorBoard进行可视化<br>您的TensorFlow图表，绘制关于您的执行的量化指标 图表，并显示其他数据，如通过它的图像。什么时候 TensorBoard完全配置，看起来像这样：</p>
<p><img src="https://www.tensorflow.org/images/mnist_tensorboard.png" alt="MNIST TensorBoard"></p>
<p>本教程旨在让您开始使用简单的TensorBoard。 还有其他资源可用！ TensorBoard的GitHub<br>有更多关于TensorBoard使用情况的信息，包括提示和技巧 调试信息。</p>
<h2><span id="序列化数据">序列化数据</span></h2><p>TensorBoard通过读取包含摘要的TensorFlow事件文件来操作 运行TensorFlow时可以生成的数据。一般来说<br>TensorBoard中的摘要数据的生命周期。</p>
<p>首先，创建您想要收集摘要的TensorFlow图表 数据，并决定你想注释哪个节点 总结操作。</p>
<p>例如，假设你正在训练一个卷积神经网络 识别MNIST数字。你想记录如何学习率 随时间变化，目标函数如何变化。通过收集这些<br>附加<code>tf.summary.scalar</code>操作 到分别输出学习率和损失的节点。然后，给<br>每个<code>scalar_summary</code>一个有意义的<code>tag</code>，就像<code>&#39;learning rate&#39;</code>或“损失 功能’。</p>
<p>也许你也想看到即将到来的激活分布 关闭特定层，或者分布梯度或权重。搜集 这个数据通过附加 <code>tf.summary.histogram</code>选择<br>梯度输出和分别保存你的权重的变量。</p>
<p>有关所有可用摘要操作的详细信息，请查看文档 总结操作。</p>
<p>在运行之前，TensorFlow中的操作不会执行任何操作 取决于他们的输出。我们刚刚创建的摘要节点是 你的图形的外围设备：你当前运行的操作系统都不依赖于它<br>他们。所以，为了生成摘要，我们需要运行所有这些汇总节点。 用手来管理它们将是乏味的，所以使用 <code>tf.summary.merge_all</code><br>将它们组合成一个单独的操作来生成所有的汇总数据。</p>
<p>然后，您可以运行合并的摘要操作，这将生成一个序列化 <code>Summary</code> protobuf对象与给定步骤中的所有摘要数据。<br>最后，要将这个汇总数据写入磁盘，请将summary protobuf传递给一个 <code>tf.summary.FileWriter</code>。</p>
<p><code>FileWriter</code>在其构造函数中使用了一个logdir - 这个logdir是相当的 重要的是，这是所有事件将被写出的目录。<br>而且，<code>FileWriter</code>可以选择在其构造器中使用<code>Graph</code>。 如果它收到一个<code>Graph</code>对象，那么TensorBoard将显示您的图形<br>连同张量形状信息。这会给你一个更好的感觉 图中流动的东西：看 张量形状信息。</p>
<p>现在你已经修改了你的图表，并有一个<code>FileWriter</code>，你准备好了 开始运行你的网络！如果你愿意，你可以运行合并的摘要操作<br>每一步，并记录大量的训练数据。这可能更多 数据比你需要，虽然。相反，请考虑运行合并的摘要操作 每个<code>n</code>步骤。</p>
<p>下面的代码示例是对其的修改 简单的MNIST教程， 我们在其中添加了一些摘要操作，并且每十步执行一次。如果你 运行这个，然后启动<code>tensorboard
--logdir=/tmp/tensorflow/mnist</code>，你就可以 统计数据的可视化，例如权重或准确度如何变化 训练。下面的代码是摘录;完整的来源是<br>这里。</p>
<pre><code>def variable_summaries(var):
  &quot;&quot;&quot;Attach a lot of summaries to a Tensor (for TensorBoard visualization).&quot;&quot;&quot;
  with tf.name_scope(&apos;summaries&apos;):
    mean = tf.reduce_mean(var)
    tf.summary.scalar(&apos;mean&apos;, mean)
    with tf.name_scope(&apos;stddev&apos;):
      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
    tf.summary.scalar(&apos;stddev&apos;, stddev)
    tf.summary.scalar(&apos;max&apos;, tf.reduce_max(var))
    tf.summary.scalar(&apos;min&apos;, tf.reduce_min(var))
    tf.summary.histogram(&apos;histogram&apos;, var)

def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
  &quot;&quot;&quot;Reusable code for making a simple neural net layer.

  It does a matrix multiply, bias add, and then uses relu to nonlinearize.
  It also sets up name scoping so that the resultant graph is easy to read,
  and adds a number of summary ops.
  &quot;&quot;&quot;
  # Adding a name scope ensures logical grouping of the layers in the graph.
  with tf.name_scope(layer_name):
    # This Variable will hold the state of the weights for the layer
    with tf.name_scope(&apos;weights&apos;):
      weights = weight_variable([input_dim, output_dim])
      variable_summaries(weights)
    with tf.name_scope(&apos;biases&apos;):
      biases = bias_variable([output_dim])
      variable_summaries(biases)
    with tf.name_scope(&apos;Wx_plus_b&apos;):
      preactivate = tf.matmul(input_tensor, weights) + biases
      tf.summary.histogram(&apos;pre_activations&apos;, preactivate)
    activations = act(preactivate, name=&apos;activation&apos;)
    tf.summary.histogram(&apos;activations&apos;, activations)
    return activations

hidden1 = nn_layer(x, 784, 500, &apos;layer1&apos;)

with tf.name_scope(&apos;dropout&apos;):
  keep_prob = tf.placeholder(tf.float32)
  tf.summary.scalar(&apos;dropout_keep_probability&apos;, keep_prob)
  dropped = tf.nn.dropout(hidden1, keep_prob)

# Do not apply softmax activation yet, see below.
y = nn_layer(dropped, 500, 10, &apos;layer2&apos;, act=tf.identity)

with tf.name_scope(&apos;cross_entropy&apos;):
  # The raw formulation of cross-entropy,
  #
  # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),
  #                               reduction_indices=[1]))
  #
  # can be numerically unstable.
  #
  # So here we use tf.nn.softmax_cross_entropy_with_logits on the
  # raw outputs of the nn_layer above, and then average across
  # the batch.
  diff = tf.nn.softmax_cross_entropy_with_logits(targets=y_, logits=y)
  with tf.name_scope(&apos;total&apos;):
    cross_entropy = tf.reduce_mean(diff)
tf.summary.scalar(&apos;cross_entropy&apos;, cross_entropy)

with tf.name_scope(&apos;train&apos;):
  train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(
      cross_entropy)

with tf.name_scope(&apos;accuracy&apos;):
  with tf.name_scope(&apos;correct_prediction&apos;):
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
  with tf.name_scope(&apos;accuracy&apos;):
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
tf.summary.scalar(&apos;accuracy&apos;, accuracy)

# Merge all the summaries and write them out to /tmp/mnist_logs (by default)
merged = tf.summary.merge_all()
train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + &apos;/train&apos;,
                                      sess.graph)
test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + &apos;/test&apos;)
tf.global_variables_initializer().run()
</code></pre><p>在我们初始化了<code>FileWriters</code>之后，我们必须添加总结 <code>FileWriters</code>在我们训练和测试模型。</p>
<pre><code># Train the model, and also write summaries.
# Every 10th step, measure test-set accuracy, and write test summaries
# All other steps, run train_step on training data, &amp; add training summaries

def feed_dict(train):
  &quot;&quot;&quot;Make a TensorFlow feed_dict: maps data onto Tensor placeholders.&quot;&quot;&quot;
  if train or FLAGS.fake_data:
    xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
    k = FLAGS.dropout
  else:
    xs, ys = mnist.test.images, mnist.test.labels
    k = 1.0
  return {x: xs, y_: ys, keep_prob: k}

for i in range(FLAGS.max_steps):
  if i % 10 == 0:  # Record summaries and test-set accuracy
    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))
    test_writer.add_summary(summary, i)
    print(&apos;Accuracy at step %s: %s&apos; % (i, acc))
  else:  # Record train set summaries, and train
    summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))
    train_writer.add_summary(summary, i)
</code></pre><p>您现在已经开始使用TensorBoard将这些数据可视化了。</p>
<h2><span id="启动tensorboard">启动TensorBoard</span></h2><p>要运行TensorBoard，请使用以下命令（或者使用python -m tensorboard.main）</p>
<pre><code>tensorboard --logdir=path/to/log-directory
</code></pre><p><code>logdir</code>指向<code>FileWriter</code>串行化的目录 数据。如果这个<code>logdir</code>目录包含包含子目录<br>序列化的数据从单独的运行，然后TensorBoard将可视化的数据 从所有这些运行。一旦TensorBoard正在运行，浏览您的网页浏览器<br>以<code>localhost:6006</code>来查看张量板。</p>
<p>在看TensorBoard时，你会看到右上角的导航标签 角。每个选项卡代表一组可以可视化的序列化数据。</p>
<p>有关如何使用图形选项卡可视化图形的深入信息， 参见TensorBoard：Graph Visualization。</p>
<p>有关TensorBoard的更多使用信息，请参阅TensorBoard的GitHub。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/optimizing/" title="针对移动设备进行优化" itemprop="url">针对移动设备进行优化</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="针对移动设备进行优化">针对移动设备进行优化</span></h1><p>有一些特殊的问题需要在你尝试时处理 在手机或嵌入式设备上发货，你需要考虑这些 你正在开发你的模型。</p>
<p>这些问题是：</p>
<p>模型和二进制大小 应用速度和模型加载速度 性能和线程</p>
<p>我们将在下面讨论其中的一些。</p>
<h2><span id="tensorflow的最低设备要求是什么">TensorFlow的最低设备要求是什么？</span></h2><p>您至少需要一兆字节的程序存储器和几兆字节的RAM 运行基本的TensorFlow运行时，所以它不适合DSP或者 微控制器。除此之外，最大的限制通常是<br>设备的计算速度，以及是否可以运行所需的模型 您的应用程序的延迟时间足够短。您可以使用基准测试工具 在如何剖析你的模型，以了解如何<br>模型需要许多FLOP，然后使用它来制定经验法则 估计他们在不同设备上运行的速度。例如，一个现代的 智能手机可能会运行10<br>GFLOPs每秒，所以最好的，你可以希望 从5 GFLOP模型是每秒两帧，但你可能会做得更差 取决于确切的计算模式是什么。</p>
<p>这种依赖模式意味着甚至可以运行TensorFlow 旧的或受限的手机，只要你优化你的网络，以适应内 延迟预算，也可能在有限的RAM内。对于内存使用，你<br>大多需要确定TensorFlow创建的中间缓冲区 不是太大，你也可以在基准输出中查看。</p>
<h2><span id="速度">速度</span></h2><p>大多数模型部署的最高优先级之一是弄清楚如何 运行推理足够快，以提供良好的用户体验。第一个地方 开始是通过查看浮点操作的总数<br>执行该图所需的。你可以通过一个非常粗略的估计 使用<code>benchmark_model</code>工具：</p>
<pre><code>bazel build -c opt tensorflow/tools/benchmark:benchmark_model &amp;&amp; \
bazel-bin/tensorflow/tools/benchmark/benchmark_model \
--graph=/tmp/inception_graph.pb --input_layer=&quot;Mul:0&quot; \
--input_layer_shape=&quot;1,299,299,3&quot; --input_layer_type=&quot;float&quot; \
--output_layer=&quot;softmax:0&quot; --show_run_order=false --show_time=false \
--show_memory=false --show_summary=true --show_flops=true --logtostderr
</code></pre><p>这应该显示您需要运行多少个操作的估计 图形。然后，您可以使用该信息来确定模型的可行性 将在您定位的设备上运行。举个例子，来自高端手机<br>2016年可能能够做到200亿FLOPs每秒，所以最好的速度 可能希望从一个需要100亿FLOPs的模型是500毫秒左右。在一个 像Raspberry Pi<br>3那样可以做大约50亿FLOP的设备，你可能只有 每两秒得一个推理。</p>
<p>有了这个估计值可以帮助你计划你将能够实际的 在设备上实现。如果模型使用太多的操作，那么有很多 优化体系结构以减少数量的机会。</p>
<p>先进的技术包括SqueezeNet 和MobileNet，这是架构 旨在生产移动模型 - 精益和快速，但具有一个小的准确性<br>成本。你也可以看看替代模型，甚至更旧的，可能 更小。例如，Inception v1只有大约700万个参数， 与Inception<br>v3的2400万相比，只需要30亿FLOP v3超过90亿。</p>
<h2><span id="模型大小">模型大小</span></h2><p>在设备上运行的模型需要存储在设备上的某个地方 大的神经网络可能是几百兆字节。大多数用户不愿意 从应用程序商店下载非常大的应用程序包，所以你想让你的模型<br>尽可能小。此外，更小的神经网络可以坚持和 移动设备的内存更快。</p>
<p>要了解您的网络将在磁盘上有多大，请先看看 在运行<code>GraphDef</code>后<code>freeze_graph</code>文件的磁盘大小和<br><code>strip_unused_nodes</code>（请参阅准备型号 关于这些工具的更多细节），因为它应该只包含 推理相关节点。要仔细检查您的结果是否符合预期，请运行<br><code>summarize_graph</code>工具查看常量中有多少个参数：</p>
<pre><code>bazel build tensorflow/tools/graph_transforms:summarize_graph &amp;&amp; \
bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \
--in_graph=/tmp/tensorflow_inception_graph.pb
</code></pre><p>该命令应该给你看起来像这样的输出：</p>
<pre><code>No inputs spotted.
Found 1 possible outputs: (name=softmax, op=Softmax)
Found 23885411 (23.89M) const parameters, 0 (0) variable parameters,
and 99 control_edges
Op types used: 489 Const, 99 CheckNumerics, 99 Identity, 94
BatchNormWithGlobalNormalization, 94 Conv2D, 94 Relu, 11 Concat, 9 AvgPool,
5 MaxPool, 1 Sub, 1 Softmax, 1 ResizeBilinear, 1 Reshape, 1 Mul, 1 MatMul,
1 ExpandDims, 1 DecodeJpeg, 1 Cast, 1 BiasAdd
</code></pre><p>我们目前的目的的重要部分是const数 参数。在大多数模型中，这些将被存储为32位浮点数，所以如果 你把const参数的数量乘以四，你应该得到一些东西<br>这接近于磁盘上文件的大小。你只能经常逃脱 每个参数8比特，最终结果的精度损失很小， 所以如果你的文件太大，你可以尝试使用<br>quantize_weights将参数向下变换。</p>
<pre><code>bazel build tensorflow/tools/graph_transforms:transform_graph &amp;&amp; \
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=/tmp/tensorflow_inception_optimized.pb \
--out_graph=/tmp/tensorflow_inception_quantized.pb \
--inputs=&apos;Mul:0&apos; --outputs=&apos;softmax:0&apos; --transforms=&apos;quantize_weights&apos;
</code></pre><p>如果你看看所得到的文件大小，你应该看到它大约是四分之一 的原始在23MB。</p>
<p>另一个转换是<code>round_weights</code>，它不会使文件变小，但它 使文件压缩到与<code>quantize_weights</code>相同的尺寸<br>用过的。这对于移动开发来说特别有用，利用了 应用程序包在被消费者下载之前被压缩。</p>
<p>原始文件不能用标准算法压缩，因为 甚至非常相似的数字的位模式可能会非常不同。该 <code>round_weights</code>变换保持重量参数存储为浮动，但<br>将它们四舍五入成一定数量的步进值。这意味着还有更多 在存储的模型中重复字节模式，所以经常会带来压缩 尺寸急剧下降，在许多情况下接近它们的尺寸 被存储为八位。</p>
<p><code>round_weights</code>的另一个优点是框架不必 分配一个临时缓冲区来解压参数，就像我们到的时候一样<br>我们只使用<code>quantize_weights</code>。这节省了一点点的延迟（虽然 结果应该被缓存，所以它只是在第一次运行成本高昂），并使其成为可能<br>如后面所述，可以使用内存映射。</p>
<h2><span id="二进制大小">二进制大小</span></h2><p>移动和服务器开发最大的区别之一是 二进制大小的重要性。在台式机上，并不罕见 数百兆字节的磁盘上的可执行文件，但移动和嵌入式<br>应用程序是至关重要的，保持二进制尽可能小，以便用户下载 很容易。如上所述，TensorFlow只包含op的一个子集<br>默认情况下是实现，但是这仍然导致12MB的最终结果 可执行文件。为了减少这一点，你可以设置库只包括 实际需要的操作的实现，基于自动 分析你的模型。要使用它：</p>
<p>在您的<code>tools/print_required_ops/print_selective_registration_header.py</code>上运行<br>模型生成一个头文件，只启用它使用的操作。 把<code>ops_to_register.h</code>文件放在编译器可以找到的地方<br>它。这可以在您的TensorFlow源文件夹的根目录下。 使用定义的<code>SELECTIVE_REGISTRATION</code>构建TensorFlow，例如通过<br>在<code>--copts=&quot;-DSELECTIVE_REGISTRATION&quot;</code>到您的Bazel生成命令。</p>
<p>这个过程重新编译库，以便只有需要的操作和类型 包括，这可以大大减少可执行文件的大小。例如，用 入侵v3，新的大小只有1.5MB。</p>
<h2><span id="如何配置你的模型">如何配置你的模型</span></h2><p>一旦您了解了您的设备的最佳性能范围，那就是 值得关注其当前的实际表现。使用独立的TensorFlow 基准，而不是在一个更大的应用程序内运行，有助于隔离只是<br>Tensorflow对贡献的贡献 潜伏。该 张量流量/工具/基准工具 旨在帮助你做到这一点。在桌面上的Inception v3上运行它<br>机器，建立这个基准模型：</p>
<pre><code>bazel build -c opt tensorflow/tools/benchmark:benchmark_model &amp;&amp; \
bazel-bin/tensorflow/tools/benchmark/benchmark_model \
--graph=/tmp/tensorflow_inception_graph.pb --input_layer=&quot;Mul&quot; \
--input_layer_shape=&quot;1,299,299,3&quot; --input_layer_type=&quot;float&quot; \
--output_layer=&quot;softmax:0&quot; --show_run_order=false --show_time=false \
--show_memory=false --show_summary=true --show_flops=true --logtostderr
</code></pre><p>你应该看到如下所示的输出：</p>
<pre><code>============================== Top by Computation Time ==============================
[node
 type]  [start]  [first] [avg ms]     [%]  [cdf%]  [mem KB]  [Name]
Conv2D   22.859   14.212   13.700  4.972%  4.972%  3871.488  conv_4/Conv2D
Conv2D    8.116    8.964   11.315  4.106%  9.078%  5531.904  conv_2/Conv2D
Conv2D   62.066   16.504    7.274  2.640% 11.717%   443.904  mixed_3/conv/Conv2D
Conv2D    2.530    6.226    4.939  1.792% 13.510%  2765.952  conv_1/Conv2D
Conv2D   55.585    4.605    4.665  1.693% 15.203%   313.600  mixed_2/tower/conv_1/Conv2D
Conv2D  127.114    5.469    4.630  1.680% 16.883%    81.920  mixed_10/conv/Conv2D
Conv2D   47.391    6.994    4.588  1.665% 18.548%   313.600  mixed_1/tower/conv_1/Conv2D
Conv2D   39.463    7.878    4.336  1.574% 20.122%   313.600  mixed/tower/conv_1/Conv2D
Conv2D  127.113    4.192    3.894  1.413% 21.535%   114.688  mixed_10/tower_1/conv/Conv2D
Conv2D   70.188    5.205    3.626  1.316% 22.850%   221.952  mixed_4/conv/Conv2D

============================== Summary by node type ==============================
[Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB]
Conv2D            94   244.899    88.952%    88.952% 35869.953
BiasAdd           95     9.664     3.510%    92.462% 35873.984
AvgPool            9     7.990     2.902%    95.364%  7493.504
Relu              94     5.727     2.080%    97.444% 35869.953
MaxPool            5     3.485     1.266%    98.710%  3358.848
Const            192     1.727     0.627%    99.337%     0.000
Concat            11     1.081     0.393%    99.730%  9892.096
MatMul             1     0.665     0.242%    99.971%     4.032
Softmax            1     0.040     0.015%    99.986%     4.032
&lt;&gt;                 1     0.032     0.012%    99.997%     0.000
Reshape            1     0.007     0.003%   100.000%     0.000

Timings (microseconds): count=50 first=330849 curr=274803 min=232354 max=415352 avg=275563 std=44193
Memory (bytes): count=50 curr=128366400(all same)
514 nodes defined 504 nodes observed
</code></pre><p>这是由show_summary标志启用的摘要视图。至 解释它，第一个表格是花费最多时间的节点的列表， 订购他们花了多长时间。从左到右，列是：</p>
<p>节点类型，这是什么样的操作。 操作的开始时间，显示操作顺序的落点。 第一次以毫秒为单位。这是第一次手术的时间<br>基准的运行，因为默认情况下执行20次运行得到更多   可靠的统计。第一次是有用的发现哪些操作正在做   昂贵的计算在第一次运行，然后缓存结果。<br>所有运行的平均操作时间，以毫秒为单位。 一次运行所花费的总时间的百分比。这是有用的   了解热点在哪里。 本表和前面的操作的累积总时间。这是<br>方便了解各层面的工作分配情况   看看是否只有几个节点占用大部分时间。 节点的名称。</p>
<p>第二个表是类似的，但不是按时间分解 特定的命名节点，它将它们按操作类型分组。这是非常有用的 了解您可能想要优化或消除哪些操作<br>你的图。这张桌子在开始的时候是安排成本最高的， 并且只显示前十个条目，并且具有其他节点的占位符。该 从左到右的列是：</p>
<p>正在分析的节点的类型。 此类型的所有节点所花费的平均时间（以毫秒为单位）。 这种类型的操作占总时间的百分比。 这个和op类型在表中的累积时间较高，所以你可以<br>了解工作量的分布。 这个op类型的输出占用了多少内存。</p>
<p>这两个表都设置好，以便您可以轻松地复制和粘贴它们 结果导入电子表格文档，因为它们与标签一起输出 列之间的分隔符。节点类型的摘要可能是最有用的<br>当寻找优化机会时，因为它是一个指向代码的指针 这是花费最多的时间。在这种情况下，您可以看到Conv2D操作符是<br>几乎90％的执行时间。这是一个图表很漂亮的标志 最佳的，因为卷积和矩阵乘法预计是大部分 一个神经网络的计算工作量。</p>
<p>作为一个经验法则，如果你看到很多其他的操作，则更加令人担忧 占用了一小部分时间。对于神经网络，操作 不涉及大矩阵乘法的情况通常应该被相对矮化<br>那些做，所以如果你看到很多时间进入这些，这是一个迹象 要么你的网络是非最优构造的，要么是实现这些的代码 ops没有尽可能优化 是。性能错误或<br>如果遇到这种情况，补丁总是受欢迎的，特别是如果 它们包括一个展示此行为和命令行的附加模型 用来运行基准测试工具。</p>
<p>上面的运行是在您的桌面上，但该工具也适用于Android，这是 它对移动开发最有用。这里是一个示例命令行 在64位ARM设备上运行它：</p>
<pre><code>bazel build -c opt --config=android_arm64 \ 
tensorflow/tools/benchmark:benchmark_model
adb push bazel-bin/tensorflow/tools/benchmark/benchmark_model /data/local/tmp
adb push /tmp/tensorflow_inception_graph.pb /data/local/tmp/
adb shell &apos;/data/local/tmp/benchmark_model \
--graph=/data/local/tmp/tensorflow_inception_graph.pb --input_layer=&quot;Mul&quot; \
--input_layer_shape=&quot;1,299,299,3&quot; --input_layer_type=&quot;float&quot; \
--output_layer=&quot;softmax:0&quot; --show_run_order=false --show_time=false \
--show_memory=false --show_summary=true&apos;
</code></pre><p>你可以用与桌面版本完全相同的方式来解释结果 以上。如果你有什么困难搞清楚什么是正确的输入和输出 名字和类型是，看看准备<br>模型页面的细节检测这些为您的模型，并看看 <code>summarize_graph</code>工具可能会给你 有用的信息。</p>
<p>对于iOS上的命令行工具没有很好的支持，所以没有 单独的例子 在 tensorflow / examples / ios / benchmark<br>在独立的应用程序中打包相同的功能。这输出 统计到设备的屏幕和调试日志。如果你想 Android示例应用程序的屏幕统计信息，您可以打开它们 按下音量增加按钮。</p>
<h2><span id="在自己的应用程序中进行分析">在自己的应用程序中进行分析</span></h2><p>您从基准测试工具看到的输出是从模块生成的 作为标准TensorFlow运行时的一部分，这意味着您有权访问<br>在你自己的应用程序中也是如此。你可以看到一个如何做的例子 在这里。</p>
<p>基本步骤是：</p>
<p>创建一个StatSummarizer对象： tensorflow :: StatSummarizer<br>stat_summarizer（tensorflow_graph）; 设置选项： tensorflow :: RunOptions run_options;<br>run_options.set_trace_level（tensorflow :: RunOptions :: FULL_TRACE）;<br>tensorflow :: RunMetadata run_metadata; 运行图： run_status = session-&gt;<br>Run（run_options，inputs，output_layer_names，{}，<br>output_layers，＆run_metadata）; 计算结果并打印出来： 断言（run_metadata.has_step_stats（））;<br>const tensorflow :: StepStats＆step_stats = run_metadata.step_stats（）;<br>stat_summarizer-&gt; ProcessStepStats（step_stats）; stat_summarizer-&gt;<br>PrintStepStats（）;</p>
<h2><span id="可视化模型">可视化模型</span></h2><p>加速你的代码最有效的方法就是改变你的模型 做更少的工作。要做到这一点，你需要了解你的模型在做什么，以及 想象这是一个好的第一步。要高度概括您的图表，<br>使用TensorBoard。</p>
<h2><span id="穿线">穿线</span></h2><p>TensorFlow的桌面版本有一个复杂的线程模型，并将 如果可以的话，尝试并行运行多个操作。在我们的术语中是这样的<br>所谓的“内部操作并行”（尽管为了避免与“内部操作”混淆，你 可以把它看作是“之间的”），并可以通过指定来设置<br><code>inter_op_parallelism_threads</code>中的会话选项。</p>
<p>默认情况下，移动设备连续运行操作;那是， <code>inter_op_parallelism_threads</code>设置为1.移动处理器通常很少<br>核心和一个小缓存，所以运行多个访问不相交部分的操作 内存通常不利于性能。 “内部操作并行”（或 “内部操作”）可以是非常有用的，特别是对于计算限制<br>像不同的线程可以馈送同样小的卷积操作 内存集。</p>
<p>在移动设备上，一个操作系统将使用多少个线程被设置为核心数量 默认值，或者当核心数量不能确定时为2。你可以覆盖 默认的ops正在使用的线程数量<br><code>intra_op_parallelism_threads</code>中的会话选项。这是一个好主意 如果你的应用程序有自己的线程做重处理，那么减少默认值<br>他们不互相干扰。</p>
<p>要查看会话选项的更多细节，请查看ConfigProto。</p>
<h2><span id="用移动数据重新调整">用移动数据重新调整</span></h2><p>在移动应用上运行模型时，精度问题的最大原因是 非代表性的训练数据。例如，大多数Imagenet照片都是 精心设计，使对象在图片的中心，光线充足，<br>用普通镜头拍摄。来自移动设备的照片通常很差， 灯光不好，可能会有鱼眼变形，特别是自拍。</p>
<p>解决方案是用实际捕获的数据扩展您的训练集 你的申请。这一步可能需要额外的工作，因为你必须标注 你自己的例子，但即使你只是用它来扩大你的原创<br>训练数据，可以大大帮助训练。改善培训 设置这样做，并通过修复其他质量问题，如重复或严重 标记示例是提高准确性的最佳方法。通常是一个<br>更大的帮助比改变你的模型架构或使用不同的技术。</p>
<h2><span id="减少模型加载时间和或内存占用">减少模型加载时间和/或内存占用</span></h2><p>大多数操作系统允许您使用内存映射加载文件 比通过通常的I / O API。而不是分配一个内存区域 在堆上，然后从磁盘复制字节到它，你只需告诉<br>操作系统使文件的全部内容直接出现在 记忆。这有几个好处：</p>
<p>加载速度 减少分页（提高性能） 不计入您的应用程序的RAM预算</p>
<p>TensorFlow支持内存映射构成大部分的权重 模型文件。由于<code>ProtoBuf</code>系列化格式的限制，我们 必须对我们的模型加载和处理代码进行一些更改。该<br>双向内存映射的工作原理是我们有一个单独的文件，其中第一部分是 正常<code>GraphDef</code>串行化成协议缓冲线的格式，不过接下来就是了<br>权重以可以直接映射的形式附加。</p>
<p>要创建这个文件，运行 <code>tensorflow/contrib/util:convert_graphdef_memmapped_format</code>工具。这需要<br>通过<code>GraphDef</code>运行的<code>freeze_graph</code>文件并将其转换为 格式，在最后附加权重。由于该文件不再是一个 标准<code>GraphDef</code><br>protobuf，则需要对加载进行一些更改 码。你可以看到这个例子 该 iOS相机演示程序， 在<code>LoadMemoryMappedModel()</code>功能中。</p>
<p>相同的代码（用Objective C调用替换文件名） 也可以在其他平台上使用。因为我们正在使用内存映射，所以我们需要<br>首先创建一个特殊的TensorFlow环境对象 我们将使用的文件：</p>
<pre><code>std::unique_ptr&lt;tensorflow::MemmappedEnv&gt; memmapped_env;
memmapped_env-&gt;reset(
      new tensorflow::MemmappedEnv(tensorflow::Env::Default()));
tensorflow::Status mmap_status =
      (memmapped_env-&gt;get())-&gt;InitializeFromFile(file_path);
</code></pre><p>然后，您需要将此环境传递给随后的调用，如下所示 加载图表：</p>
<pre><code>tensorflow::GraphDef tensorflow_graph;
tensorflow::Status load_graph_status = ReadBinaryProto(
    memmapped_env-&gt;get(),
    tensorflow::MemmappedFileSystem::kMemmappedPackageDefaultGraphDef,
    &amp;tensorflow_graph);
</code></pre><p>您还需要使用指向您所在环境的指针来创建会话 创建：</p>
<pre><code>tensorflow::SessionOptions options;
options.config.mutable_graph_options()
    -&gt;mutable_optimizer_options()
    -&gt;set_opt_level(::tensorflow::OptimizerOptions::L0);
options.env = memmapped_env-&gt;get();

tensorflow::Session* session_pointer = nullptr;
tensorflow::Status session_status =
    tensorflow::NewSession(options, &amp;session_pointer);
</code></pre><p>有一点要注意的是，我们也禁用自动优化， 因为在某些情况下，这些将折叠不变的子树，并创建副本 张量值，我们不想要和用尽更多的RAM。</p>
<p>一旦你完成了这些步骤，你可以使用会话和图表 正常，你会看到加载时间和内存使用量的减少。</p>
<h2><span id="保护模型文件免于复制">保护模型文件免于复制</span></h2><p>默认情况下，你的模型将被存储在标准的序列化protobuf中 磁盘格式。理论上这意味着任何人都可以复制你的模型，你自己<br>可能不想要。但实际上，大多数模型都是特定于应用程序的 被优化所迷惑，风险与竞争对手相似 反汇编和重复使用你的代码，但是如果你确实想让它变得更加困难的话<br>临时用户访问您的文件是可以采取一些基本的步骤。</p>
<p>我们大多数的例子都使用 该 ReadBinaryProto（）方便 调用从磁盘加载<code>GraphDef</code>。这确实需要一个未加密的protobuf<br>磁盘。幸运的是，调用的实现非常简单 编写一个可以在内存中解密的等价物应该很容易。这里的 一些代码展示了如何使用你自己的方式读取和解密protobuf<br>解密程序：</p>
<pre><code>Status ReadEncryptedProto(Env* env, const string&amp; fname,
                          ::tensorflow::protobuf::MessageLite* proto) {
  string data;
  TF_RETURN_IF_ERROR(ReadFileToString(env, fname, &amp;data));

  DecryptData(&amp;data);  // Your own function here.

  if (!proto-&gt;ParseFromString(&amp;data)) {
    TF_RETURN_IF_ERROR(stream-&gt;status());
    return errors::DataLoss(&quot;Can&apos;t parse &quot;, fname, &quot; as binary proto&quot;);
  }
  return Status::OK();
}
</code></pre><p>要使用这个，你需要自己定义DecryptData（）函数。它可以 就像这样简单：</p>
<pre><code>void DecryptData(string* data) {
  for (int i = 0; i &lt; data.size(); ++i) {
    data[i] = data[i] ^ 0x23;
  }
}
</code></pre><p>你可能想要一些更复杂的东西，但是你需要的东西就在外面 当前范围在这里。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/01/01/debugger/" title="调试TensorFlow程序" itemprop="url">调试TensorFlow程序</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="zhizi" target="_blank" itemprop="author">zhizi</a>
		
  <p class="article-time">
    <time datetime="2018-01-01T02:00:00.000Z" itemprop="datePublished"> 发表于 2018-01-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1><span id="调试tensorflow程序">调试TensorFlow程序</span></h1><p>TensorFlow调试器（tfdbg）是TensorFlow的专用调试器。它 让您查看运行TensorFlow图的内部结构和状态<br>在训练和推理过程中，这是很难与通用调试 调试器，例如由于TensorFlow的计算图范例而导致的Python <code>pdb</code>。</p>
<blockquote>
<p>注意：支持的外部平台上的tfdbg的系统要求包括 下列。在Mac OS X上，<code>ncurses</code>库是必需的。有可能 与<code>brew install
homebrew/dupes/ncurses</code>一起安装。在Windows上，<code>pyreadline</code><br>是必须的。如果您使用Anaconda3，则可以使用命令进行安装 如<code>&quot;C:\Program Files\Anaconda3\Scripts\pip.exe&quot;
install pyreadline</code>。</p>
</blockquote>
<p>本教程演示如何使用tfdbg命令行界面 （CLI）来调试<code>nan</code>的外观 和<code>inf</code>，这是一个经常遇到的问题 TensorFlow模型开发中的错误类型。<br>以下示例适用于使用低级别的用户 <code>Session</code> API TensorFlow。本文档的后面部分介绍了如何使用tfdbg 采用更高级别的API，即tf-<br>learn <code>Estimator</code>和<code>Experiment</code>。 要观察这样的问题，运行下面的命令没有调试器（ 源代码可以找到 这里）：</p>
<pre><code>python -m tensorflow.python.debug.examples.debug_mnist
</code></pre><p>此代码为MNIST数字图像识别训练一个简单的神经网络。 请注意，在第一个训练步骤之后，准确度略有增加，但是 然后卡在一个很低的（几率）水平：</p>
<pre><code>Accuracy at step 0: 0.1113
Accuracy at step 1: 0.3183
Accuracy at step 2: 0.098
Accuracy at step 3: 0.098
Accuracy at step 4: 0.098
</code></pre><p>想知道什么可能会出错，你怀疑在某些节点 训练图生成错误的数值，如<code>inf</code>和<code>nan</code>，因为 这是这种训练失败的常见原因。<br>我们使用tfdbg来调试这个问题，并精确定位这个节点 数字问题首先浮出水面。</p>
<h2><span id="用tfdbg包装tensorflow会话">用tfdbg包装TensorFlow会话</span></h2><p>在我们的例子中添加对tfdbg的支持，只需要添加 遵循以下代码行并用调试器包装器包装Session对象。 这个代码已经被添加进去了<br>debug_mnist.py， 因此您可以在命令行中使用<code>--debug</code>标志激活tfdbg CLI。</p>
<pre><code># Let your BUILD target depend on &quot;//tensorflow/python/debug:debug_py&quot;
# (You don&apos;t need to worry about the BUILD dependency if you are using a pip
#  install of open-source TensorFlow.)
from tensorflow.python import debug as tf_debug

sess = tf_debug.LocalCLIDebugWrapperSession(sess)
</code></pre><p>这个包装与Session具有相同的接口，因此启用调试需要 没有其他更改的代码。包装提供了额外的功能， 包含：</p>
<p>在<code>Session.run()</code>呼叫前后拨打CLI，让你 控制执行并检查图形的内部状态。 允许您为张量值注册特殊的<code>filters</code>，以方便使用<br>问题的诊断。</p>
<p>在这个例子中，我们已经注册了一个叫做张量过滤器 <code>tfdbg.has_inf_or_nan</code>， 它只是确定是否有任何<code>nan</code>或<code>inf</code>值<br>中间张量（既不是输入也不是输出的张量） <code>Session.run()</code>的调用，但是都在从输入到输出的路径上<br>输出）。该滤波器适用于<code>nan</code>，而<code>inf</code>则是一种常见的用例 我们用它运送它 <code>debug_data</code> 模块。</p>
<p>注意：您也可以编写自己的自定义过滤器。看到 API文档 <code>DebugDumpDir.find()</code>的更多信息。</p>
<h2><span id="用tfdbg调试模型训练">用tfdbg调试模型训练</span></h2><p>让我们再次尝试训练模型，但这次添加了<code>--debug</code>标志：</p>
<pre><code>python -m tensorflow.python.debug.examples.debug_mnist --debug
</code></pre><p>调试包装会话将提示您何时执行第一个 <code>Session.run()</code>调用，提供有关取出张量和进给的信息 字典显示在屏幕上。</p>
<p><img src="https://www.tensorflow.org/images/tfdbg_screenshot_run_start.png" alt="tfdbg run-start
UI"></p>
<p>这就是我们所说的运行启动CLI。它列出了提要和提取 到当前的<code>Session.run</code>调用，在执行任何事情之前。</p>
<p>如果屏幕尺寸太小，则无法在其中显示消息的内容 完整，你可以调整它。</p>
<p>使用PageUp / PageDown / Home / End键导航 屏幕输出。在大多数键盘上缺少这些键Fn + Up / Fn +向下/ Fn +向右/<br>Fn +向左将工作。</p>
<p>在命令提示符处输入<code>run</code>命令（或者只是<code>r</code>）：</p>
<pre><code>tfdbg&gt; run
</code></pre><p><code>run</code>命令导致tfdbg执行，直到下一个结束 <code>Session.run()</code>调用，它使用测试数据计算模型的准确性 组。<br>tfdbg扩展运行时图形以转储所有中间张量。 运行结束后，tfdbg将显示所有转储的张量值 运行结束CLI。例如：</p>
<p><img src="https://www.tensorflow.org/images/tfdbg_screenshot_run_end_accuracy.png" alt="tfdbg run-end UI:
accuracy"></p>
<p>之后可以通过运行命令<code>lt</code>来获得张量列表 执行<code>run</code>。</p>
<h3><span id="tfdbg-cli经常使用的命令">tfdbg CLI经常使用的命令</span></h3><p>请在<code>tfdbg&gt;</code>提示符处尝试以下命令（参考中的代码）<br><code>tensorflow/python/debug/examples/debug_mnist.py</code>）：</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Syntax or Option</th>
<th>Explanation</th>
<th>Example  </th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>lt</code></strong></td>
<td></td>
<td><strong>List dumped tensors.</strong></td>
<td><code>lt</code>  </td>
</tr>
<tr>
<td></td>
<td><code>-n &lt;name_pattern&gt;</code></td>
<td>List dumped tensors with names matching given</td>
</tr>
<tr>
<td>regular-expression pattern.</td>
<td><code>lt -n Softmax.*</code>  </td>
</tr>
<tr>
<td></td>
<td><code>-t &lt;op_pattern&gt;</code></td>
<td>List dumped tensors with op types matching given</td>
</tr>
<tr>
<td>regular-expression pattern.</td>
<td><code>lt -t MatMul</code>  </td>
</tr>
<tr>
<td></td>
<td><code>s &lt;sort_key&gt;</code></td>
<td>Sort the output by given <code>sort_key</code>, whose possible values</td>
</tr>
<tr>
<td>are <code>timestamp</code> (default), <code>dump_size</code>, <code>op_type</code> and <code>tensor_name</code>.</td>
<td>`lt -s</td>
</tr>
</tbody>
</table>
<p>dump_size<code>|</code>-r<code>| Sort in reverse order. |</code>lt -r -s dump_size<code>**</code>pt<code>** |  | **Print value of a dumped tensor.** |  
|</code>pt <tensor><code>|  Print tensor value. |</code>pt hidden/Relu:0<code>|</code>pt <tensor>[slicing]<code>| Print a subarray of tensor, using
[numpy](http://www.numpy.org/)-style array slicing. |</code>pt<br>hidden/Relu:0[0:50,:]<code>|</code>-a<code>| Print the entirety of a large tensor, without using ellipses. (May
take a long time for large tensors.) |</code>pt -a hidden/Relu:0[0:50,:]<code>|</code>-r <range><code>| Highlight elements falling into specified numerical range.
Multiple ranges can be used in conjunction. |</code>pt hidden/Relu:0 -a -r<br>[[-inf,-1],[1,inf]]<code>|</code>-s<code>| Include a summary of the numeric values of the tensor (applicable
only to non-empty tensors with Boolean and numeric types such as</code>int<em><code>and</code>float</em><code>.) |</code>pt -s hidden/Relu:0[0:50,:]<code>**</code>@[coordinates]<code>** |  |  Navigate to specified element in</code>pt<code>output. |</code>@[10,0]<code>or</code>@10,0<code>**</code>/regex<code>** |  | [less](https://linux.die.net/man/1/less)-style search for
given regular expression. |</code>/inf<code>**</code>/<code>** |  |  Scroll to the next line with matches to the searched regex (if
any). |</code>/<code>**</code>pf<code>** |  | **Print a value in the feed_dict to</code>Session.run<code>.** |  
|</code>pf <feed_tensor_name><code>|  Print the value of the feed. Also note that the</code>pf<code>command has the</code>-a<code>,</code>-r<code>and</code>-s<code>flags (not listed below), which have
the same syntax and semantics as the identically-named flags of</code>pt<code>. |</code>pf<br>input_xs:0<code>**eval** |  | **Evaluate arbitrary Python and numpy expression.** |  
|</code>eval <expression><code>|  Evaluate a Python / numpy expression, with numpy
available as</code>np<code>and debug tensor names enclosed in backticks. |</code>eval<br>“np.matmul((<code>output/Identity:0</code> / <code>Softmax:0</code>).T, <code>Softmax:0</code>)”<code>|</code>-a<code>| Print a large-sized evaluation result in its entirety, i.e., without
using ellipses. |</code>eval -a ‘np.sum(<code>Softmax:0</code>, axis=1)’<code>**</code>ni<code>** |  | **Display node information.** |  
|</code>-a<code>|  Include node attributes in the output. |</code>ni -a hidden/Relu<code>|</code>-d<code>| List the debug dumps available from the node. |</code>ni -d hidden/Relu<code>|</code>-t<code>| Display the Python stack trace of the node&#39;s creation. |</code>ni -t<br>hidden/Relu<code>**</code>li<code>** |  | **List inputs to node** |  
|</code>-r<code>|  List the inputs to node, recursively (the input tree.) |</code>li -r<br>hidden/Relu:0<code>|</code>-d <max_depth><code>| Limit recursion depth under the</code>-r<code>mode. |</code>li -r -d 3<br>hidden/Relu:0<code>|</code>-c<code>| Include control inputs. |</code>li -c -r hidden/Relu:0<code>**</code>lo<code>** |  | **List output recipients of node** |  
|</code>-r<code>|  List the output recipients of node, recursively (the output tree.) |</code>lo -r hidden/Relu:0<code>|</code>-d <max_depth><code>| Limit recursion depth under the</code>-r<code>mode. |</code>lo -r -d 3<br>hidden/Relu:0<code>|</code>-c<code>| Include recipients via control edges. |</code>lo -c -r hidden/Relu:0<code>**</code>ls<code>** |  | **List Python source files involved in node creation.** |  
|</code>-p <path_pattern><code>|  Limit output to source files matching given regular-
expression path pattern. |</code>ls -p .<em>debug_mnist.</em><code>|</code>-n<code>| Limit output to node names matching given regular-expression pattern.
|</code>ls -n Softmax.<em><code>**</code>ps<code>** |  | **Print Python source file.** |  
|</code>ps <file_path><code>|  Print given Python source file source.py, with the lines
annotated with the nodes created at each of them (if any). |</code>ps<br>/path/to/source.py<code>|</code>-t<code>| Perform annotation with respect to Tensors, instead of the default,
nodes. |</code>ps -t /path/to/source.py<code>|</code>-b <line_number><code>| Annotate source.py beginning at given line. |</code>ps -b 30<br>/path/to/source.py<code>|</code>-m <max_elements><code>| Limit the number of elements in the annotation for
each line. |</code>ps -m 100 /path/to/source.py<code>**</code>run<code>** |  | **Proceed to the next Session.run()** |</code>run<code>|</code>-n<code>|  Execute through the next</code>Session.run<code>without debugging, and drop
to CLI right before the run after that. |</code>run -n<code>|</code>-t <t><code>| Execute</code>Session.run<code></code>T - 1<code>times without debugging, followed
by a run with debugging. Then drop to CLI right after the debugged run. |</code>run<br>-t 10<code>|</code>-f <filter_name><code>| Continue executing</code>Session.run<code>until any intermediate
tensor triggers the specified Tensor filter (causes the filter to return</code>True<code>). |</code>run -f has_inf_or_nan<code>|</code>–node_name_filter <pattern><code>| Execute the next</code>Session.run<code>, watching
only nodes with names matching the given regular-expression pattern. |</code>run<br>–node_name_filter Softmax.</pattern></filter_name></t></max_elements></line_number></file_path></em><code>|</code>–op_type_filter <pattern><code>| Execute the next</code>Session.run<code>, watching only
nodes with op types matching the given regular-expression pattern. |</code>run<br>–op_type_filter Variable.<em><code>|</code>–tensor_dtype_filter <pattern><code>| Execute the next</code>Session.run<code>, dumping
only Tensors with data types (</code>dtype<code>s) matching the given regular-expression
pattern. |</code>run –tensor_dtype_filter int.</pattern></em><code>|</code>-p<code>| Execute the next</code>Session.run<code>call in profiling mode. |</code>run -p<code>**</code>ri<code>** |  | **Display information about the run the current run, including
fetches and feeds.** |</code>ri<code>**</code>help<code>** |  | **Print general help information** |</code>help<code>|</code>help <command><code>|  Print help for given command. |</code>help lt`  </pattern></path_pattern></max_depth></max_depth></expression></feed_tensor_name></range></tensor></tensor></p>
<p>请注意，每当您输入一个命令，一个新的屏幕输出 会出现。这有点类似于浏览器中的网页。您可以 点击<code>&lt;--</code>和 CLI左上角附近的<code>--&gt;</code>文本箭头。</p>
<h3><span id="tfdbg-cli的其他功能">tfdbg CLI的其他功能</span></h3><p>除了上面列出的命令外，tfdbg CLI还提供了以下内容 附加功能：</p>
<p>要浏览以前的tfdbg命令，请输入几个字符     接着是向上或向下箭头键。 tfdbg会告诉你的历史     以这些字符开始的命令。<br>要浏览屏幕输出的历史记录，请执行以下任一操作     以下： 使用<code>prev</code>和<code>next</code>命令。 点击下划线旁边的<code>&lt;--</code>和<code>--&gt;</code>链接左上角<br>屏幕。 Tab完成命令和一些命令参数。 要将屏幕输出重定向到文件而不是屏幕，请结束     命令与bash风格的重定向。例如，下面的命令<br>将pt命令的输出重定向到<code>/tmp/xent_value_slices.txt</code>     文件：</p>
<blockquote>
<p>tfdbg&gt; pt cross_entropy / Log：0 [:, 0:10]&gt; /tmp/xent_value_slices.txt</p>
</blockquote>
<h3><span id="寻找nan和inf">寻找<code>nan</code>和<code>inf</code></span></h3><p>在这首<code>Session.run()</code>调用中，碰巧没有问题的数字 值。您可以使用命令<code>run</code>或其命令进行下一次运行 速记<code>r</code>。</p>
<blockquote>
<p>提示：如果您重复输入<code>run</code>或<code>r</code>，您将可以移动 <code>Session.run()</code>按顺序调用。<br>您也可以使用<code>-t</code>标志前进一些<code>Session.run()</code>呼叫 一次，例如： tfdbg&gt; run -t 10</p>
</blockquote>
<p>而不是反复输入<code>run</code>并手动搜索<code>nan</code> 每次<code>inf</code>调用后的<code>Session.run()</code>在运行结束UI（例如，通过使用<br><code>pt</code>的命令如上表所示），可以使用以下命令 命令让调试器无需重复执行<code>Session.run()</code>调用<br>停止在运行开始或运行结束提示符，直到第一台<code>nan</code>或<code>inf</code> 值在图中显示。这类似于中的条件断点 一些程序语言调试器：</p>
<pre><code>tfdbg&gt; run -f has_inf_or_nan
</code></pre><blockquote>
<p>注：上述命令正常工作，因为张量过滤器被称为 <code>has_inf_or_nan</code>已经在包装会话时注册 创建。该滤波器检测<code>nan</code>和<code>inf</code>（如前所述）。<br>如果你已经注册了其他的过滤器，你可以 使用“运行-f”使tfdbg运行，直到任何张量触发该过滤器（原因 过滤器返回True）。 def<br>my_filter_callable（数据，张量）：   ＃一个检测零值标量的过滤器。   返回len（tensor.shape）== 0和张量== 0.0<br>sess.add_tensor_filter（’my_filter’，my_filter_callable）<br>然后在tfdbg运行启动提示符运行，直到您的过滤器被触发： tfdbg&gt;运行-f my_filter</p>
</blockquote>
<p>看到这个API文档 以获取更多关于预期签名和返回值的信息 <code>Callable</code>与<code>add_tensor_filter()</code>一起使用。</p>
<p><img src="https://www.tensorflow.org/images/tfdbg_screenshot_run_end_inf_nan.png" alt="tfdbg run-end UI: infs and
nans"></p>
<p>当屏幕显示在第一行时，首先触发<code>has_inf_or_nan</code>滤波器 在第四次<code>Session.run()</code>呼叫期间：一个 亚当优化器<br>图上前进后退训练传球。在这个运行中，36（总数中 95）中间张量包含<code>nan</code>或<code>inf</code>值。这些张力被列出 按照时间顺序，左边显示时间戳。在顶部<br>的列表中，可以看到第一个张量中的数值不好 首先浮出水面：<code>cross_entropy/Log:0</code>。</p>
<p>要查看张量的值，请单击带下划线的张量名称 <code>cross_entropy/Log:0</code>或输入等效命令：</p>
<pre><code>tfdbg&gt; pt cross_entropy/Log:0
</code></pre><p>向下滚动一下，你会注意到一些零散的<code>inf</code>值。如果 <code>inf</code>和<code>nan</code>的实例难以一目了然，您可以使用 以下命令执行正则表达式搜索并突出显示输出：</p>
<pre><code>tfdbg&gt; /inf
</code></pre><p>或者，或者：</p>
<pre><code>tfdbg&gt; /(inf|nan)
</code></pre><p>您也可以使用<code>-s</code>或<code>--numeric_summary</code>命令快速总结 张量中的数值类型：</p>
<pre><code>tfdbg&gt; pt -s cross_entropy/Log:0
</code></pre><p>从总结中，可以看到1000个元素中的几个 <code>cross_entropy/Log:0</code>张量器是<code>-inf</code>s（负无穷）。</p>
<p>为什么出现这些无穷大？为了进一步调试，显示更多信息 通过单击带下划线的<code>cross_entropy/Log</code>菜单关于节点<code>node_info</code><br>项目顶部或输入相应的node_info（<code>ni</code>）命令：</p>
<pre><code>tfdbg&gt; ni cross_entropy/Log
</code></pre><p><img src="https://www.tensorflow.org/images/tfdbg_screenshot_run_end_node_info.png" alt="tfdbg run-end UI: infs and
nans"></p>
<p>你可以看到这个节点的操作类型是<code>Log</code> 其输入是节点<code>softmax/Softmax</code>。运行以下命令 仔细看一下输入张量：</p>
<pre><code>tfdbg&gt; pt softmax/Softmax:0
</code></pre><p>检查输入张量中的值，搜索零：</p>
<pre><code>tfdbg&gt; /0\.000
</code></pre><p>确实有零。现在很清楚，数字的来源是不好的 值为节点<code>cross_entropy/Log</code>记录零。为了找出答案<br>Python源代码中的罪魁祸首，使用<code>-t</code>命令的<code>ni</code>标志 显示节点构造的回溯：</p>
<pre><code>tfdbg&gt; ni -t cross_entropy/Log
</code></pre><p>如果您点击屏幕顶部的“node_info”，tfdbg会自动显示 回溯节点的构造。</p>
<p>从追溯，你可以看到，操作是在以下构建 线： <code>debug_mnist.py</code>：</p>
<pre><code>diff = y_ * tf.log(y)
</code></pre><p>tfdbg有一个功能，可以很容易地跟踪张量和操作回来 Python源文件中的行。它可以注释一个Python文件的行 他们创造的ops或张量。要使用此功能，<br>只需单击堆栈跟踪输出中的带下划线的行号即可 <code>ni -t &lt;op_name&gt;</code>命令，或者使用<code>ps</code>（或<code>print_source</code>）命令，例如： <code>ps
/path/to/source.py</code>。例如，以下屏幕截图显示了输出 <code>ps</code>命令。</p>
<p><img src="https://www.tensorflow.org/images/tfdbg_screenshot_run_end_annotated_source.png" alt="tfdbg run-end UI: annotated Python source
file"></p>
<h3><span id="解决问题">解决问题</span></h3><p>要解决这个问题，编辑<code>debug_mnist.py</code>，改变原来的行：</p>
<pre><code>diff = -(y_ * tf.log(y))
</code></pre><p>到softmax交叉熵的内置数值稳定实现：</p>
<pre><code>diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)
</code></pre><p>重新运行<code>--debug</code>标志如下：</p>
<pre><code>python -m tensorflow.python.debug.examples.debug_mnist --debug
</code></pre><p>在<code>tfdbg&gt;</code>提示符处，输入以下命令：</p>
<pre><code>run -f has_inf_or_nan`
</code></pre><p>确认没有张量被标记为包含<code>nan</code>或<code>inf</code>值，并且 精度现在继续上升而不是卡住。成功！</p>
<h2><span id="调试tf学习估计器和实验">调试tf学习估计器和实验</span></h2><p>本节介绍如何调试使用<code>Estimator</code>的TensorFlow程序 和<code>Experiment</code> API。这些API所提供的部分便利是<br>他们在内部管理<code>Session</code>。这使得<code>LocalCLIDebugWrapperSession</code> 在前面的章节中描述不适用。幸运的是，你仍然可以<br>使用<code>hook</code>提供的<code>tfdbg</code>进行调试。</p>
<h3><span id="调试tfcontriblearn估计器">调试tf.contrib.learn估计器</span></h3><p>目前，<code>tfdbg</code>可以进行调试 <code>fit()</code> <code>evaluate()</code> 学习<code>Estimator</code>s的方法。为了调试<code>Estimator.fit()</code>，<br>创建一个<code>LocalCLIDebugHook</code>并在<code>monitors</code>参数中提供。例如：</p>
<pre><code># First, let your BUILD target depend on &quot;//tensorflow/python/debug:debug_py&quot;
# (You don&apos;t need to worry about the BUILD dependency if you are using a pip
#  install of open-source TensorFlow.)
from tensorflow.python import debug as tf_debug

# Create a LocalCLIDebugHook and use it as a monitor when calling fit().
hooks = [tf_debug.LocalCLIDebugHook()]

classifier.fit(x=training_set.data,
               y=training_set.target,
               steps=1000,
               monitors=hooks)
</code></pre><p>要调试<code>Estimator.evaluate()</code>，请将钩子分配给<code>hooks</code>参数，如 下面的例子：</p>
<pre><code>accuracy_score = classifier.evaluate(x=test_set.data,
                                     y=test_set.target,
                                     hooks=hooks)[&quot;accuracy&quot;]
</code></pre><p>debug_tflearn_iris.py， 基于{$ tflearn $ tf-learn的虹膜教程}，包含了一个完整的例子<br>与<code>Estimator</code>一起使用tfdbg。要运行这个例子，请执行：</p>
<pre><code>python -m tensorflow.python.debug.examples.debug_tflearn_iris --debug
</code></pre><h3><span id="调试tfcontriblearn实验">调试tf.contrib.learn实验</span></h3><p><code>Experiment</code>是<code>tf.contrib.learn</code>中的一种构建体 <code>Estimator</code>。 它提供了一个单一的界面来训练和评估一个模型。调试<br><code>train()</code>和<code>evaluate()</code>调用<code>Experiment</code>对象，您可以<br>当分别使用关键字参数<code>train_monitors</code>和<code>eval_hooks</code>时 调用它的构造函数。例如：</p>
<pre><code># First, let your BUILD target depend on &quot;//tensorflow/python/debug:debug_py&quot;
# (You don&apos;t need to worry about the BUILD dependency if you are using a pip
#  install of open-source TensorFlow.)
from tensorflow.python import debug as tf_debug

hooks = [tf_debug.LocalCLIDebugHook()]

ex = experiment.Experiment(classifier,
                           train_input_fn=iris_input_fn,
                           eval_input_fn=iris_input_fn,
                           train_steps=FLAGS.train_steps,
                           eval_delay_secs=0,
                           eval_steps=1,
                           train_monitors=hooks,
                           eval_hooks=hooks)

ex.train()
accuracy_score = ex.evaluate()[&quot;accuracy&quot;]
</code></pre><p>要在<code>debug_tflearn_iris</code>模式下构建并运行<code>Experiment</code>示例，请执行以下操作：</p>
<pre><code>python -m tensorflow.python.debug.examples.debug_tflearn_iris \
    --use_experiment --debug
</code></pre><p><code>LocalCLIDebugHook</code>还允许您配置<code>watch_fn</code>，该<code>Tensor</code>可以<br>用于灵活指定在不同的<code>Session.run()</code>上观看的<code>fetches</code><br>呼叫，作为<code>feed_dict</code>和<code>LocalCLIDebugWrapperSession</code>等状态的功能。看到 这个API文档 更多细节。</p>
<h2><span id="用tfdbg调试keras模型">用TFDBG调试Keras模型</span></h2><p>要使用Keras的TFDBG，请让Keras后端使用 一个TFDBG包装的Session对象。例如，要使用CLI包装器：</p>
<pre><code>import tensorflow as tf
from keras import backend as keras_backend
from tensorflow.python import debug as tf_debug

keras_backend.set_session(tf_debug.LocalCLIDebugWrapperSession(tf.Session()))

# Define your keras model, called &quot;model&quot;.
model.fit(...)  # This will break into the TFDBG CLI.
</code></pre><h2><span id="用tfdbg调试tf-slim">用TFDBG调试tf-slim</span></h2><p>TFDBG目前只支持训练 TF-渺茫。 为了调试培训过程，请提供<code>session_wrapper</code><br><code>slim.learning.train()</code>的<code>offline_analyzer</code>参数。例如：</p>
<pre><code>import tensorflow as tf
from tensorflow.python import debug as tf_debug

# ... Code that creates the graph and the train_op ...
tf.contrib.slim.learning_train(
    train_op,
    logdir,
    number_of_steps=10,
    session_wrapper=tf_debug.LocalCLIDebugWrapperSession)
</code></pre><h2><span id="脱机调试远程运行的会话">脱机调试远程运行的会话</span></h2><p>通常情况下，您的模型运行在远程机器上或者您没有的进程中 有终端访问权限。要在这种情况下执行模型调试，可以使用<br><code>tfdbg</code>的二元<code>Session</code>（下面描述）。它运行 转储的数据目录。这可以对较低级别的<code>Estimator</code> API完成<br>以及更高级别的<code>Experiment</code>和<code>tf.Session</code> API。</p>
<h3><span id="调试远程tfsessions">调试远程tf.Sessions</span></h3><p>如果您直接与<code>python</code>中的<code>RunOptions</code> API交互，则可以<br>配置您称为<code>Session.run()</code>方法的<code>tfdbg.watch_graph</code>原型 用<code>Session.run()</code>方法进行。<br>这将导致中间张量和运行时图被转储到一个 发生<code>offline_analyzer</code>呼叫时，您选择的共享存储位置 （以较慢的性能为代价）。例如：</p>
<pre><code>from tensorflow.python import debug as tf_debug

# ... Code where your session and graph are set up...

run_options = tf.RunOptions()
tf_debug.watch_graph(
      run_options,
      session.graph,
      debug_urls=[&quot;file:///shared/storage/location/tfdbg_dumps_1&quot;])
# Be sure to specify different directories for different run() calls.

session.run(fetches, feed_dict=feeds, options=run_options)
</code></pre><p>之后，在您有终端访问权的环境中（例如，本地 可以访问代码中指定的共享存储位置的计算机 上面），您可以加载和检查共享上的转储目录中的数据<br>通过使用<code>tfdbg</code>的<code>Session</code>二进制文件进行存储。例如：</p>
<pre><code>python -m tensorflow.python.debug.cli.offline_analyzer \
    --dump_dir=/shared/storage/location/tfdbg_dumps_1
</code></pre><p><code>DumpingDebugWrapperSession</code>包装机<code>tf_debug.DumpingDebugWrapperSession</code>提供了一个更容易，更多<br>灵活的方式来生成可离线分析的文件系统转储。 要使用它，只需将您的会话包装在<code>watch_fn</code>中即可。 例如：</p>
<pre><code># Let your BUILD target depend on &quot;//tensorflow/python/debug:debug_py
# (You don&apos;t need to worry about the BUILD dependency if you are using a pip
#  install of open-source TensorFlow.)
from tensorflow.python import debug as tf_debug

sess = tf_debug.DumpingDebugWrapperSession(
    sess, &quot;/shared/storage/location/tfdbg_dumps_1/&quot;, watch_fn=my_watch_fn)
</code></pre><p><code>Callable</code>参数接受<code>tensor</code>，允许您配置 <code>Session.run()</code>可以在不同的<code>fetches</code>呼叫上观看，作为<br><code>feed_dict</code>和<code>run()</code>到<code>debug_options</code>的呼叫等状态。</p>
<h3><span id="c-和其他语言">C ++和其他语言</span></h3><p>如果你的模型代码是用C ++或其他语言编写的，你也可以 修改<code>RunOptions</code>的<code>Estimator</code>字段以生成调试转储 可以离线检查。看到<br>原始的定义 更多细节。</p>
<h3><span id="调试远程运行tf学习估计器和实验">调试远程运行tf学习估计器和实验</span></h3><p>如果您的远程TensorFlow服务器运行<code>DumpingDebugHook</code>， 您可以使用非交互式<code>hook</code>。例如：</p>
<pre><code># Let your BUILD target depend on &quot;//tensorflow/python/debug:debug_py
# (You don&apos;t need to worry about the BUILD dependency if you are using a pip
#  install of open-source TensorFlow.)
from tensorflow.python import debug as tf_debug

hooks = [tf_debug.DumpingDebugHook(&quot;/shared/storage/location/tfdbg_dumps_1&quot;)]
</code></pre><p>那么这个<code>LocalCLIDebugHook</code>可以像<code>Estimator</code>一样使用 如前所述。<br>作为<code>Experiment</code>或<code>/shared/storage/location/tfdbg_dumps_1/run_&lt;epoch_timestamp_microsec&gt;_&lt;uuid&gt;</code>的培训和/或评估<br>发生，tfdbg创建具有以下名称模式的目录： <code>Session.run()</code>。 每个目录对应于底层的<code>fit()</code>呼叫<br><code>evaluate()</code>或<code>offline_analyzer</code>呼叫。你可以加载这些目录并检查 他们在一个命令行界面中以离线方式使用<br>由tfdbg提供的<code>lt</code>。例如：</p>
<pre><code>python -m tensorflow.python.debug.cli.offline_analyzer \
    --dump_dir=&quot;/shared/storage/location/tfdbg_dumps_1/run_&lt;epoch_timestamp_microsec&gt;_&lt;uuid&gt;&quot;
</code></pre><h2><span id="经常问的问题">经常问的问题</span></h2><p>问：<code>tfdbg&gt; run -p</code>输出左侧的时间戳是否反映了实际情况        性能在非调试会话？</p>
<p>答：不可以。调试器将额外的专用调试节点插入到        图表来记录中间张量的值。这些节点        减慢图形执行速度。如果你有兴趣分析你的<br>模型，检查出来</p>
<p>tfdbg的分析模式：<code>Session</code>。 tfprof       和其他TensorFlow分析工具。</p>
<p>问：如何将tfdbg与Bazel中的<code>&quot;//tensorflow:tensorflow_py&quot;</code>链接起来？我为什么看到一个<br>错误，如“ImportError：无法导入名称调试”？</p>
<p>答：在您的BUILD规则中，声明依赖关系：        <code>&quot;//tensorflow/python/debug:debug_py&quot;</code>和<code>Session</code>。<br>第一个是你甚至包括使用TensorFlow的依赖        没有调试器的支持;第二个启用调试器。        然后，在你的Python文件中，添加：</p>
<pre><code>from tensorflow.python import debug as tf_debug

# Then wrap your TensorFlow Session with the local-CLI wrapper.
sess = tf_debug.LocalCLIDebugWrapperSession(sess)
</code></pre><p>问：tfdbg是否帮助调试运行时错误，如形状不匹配？</p>
<p>答：是的。 tfdbg截取运行时产生的错误并呈现        在CLI中给用户一些调试指令的错误。        看例子：</p>
<pre><code># Debugging shape mismatch during matrix multiplication.
python -m tensorflow.python.debug.examples.debug_errors \
    --error shape_mismatch --debug

# Debugging uninitialized variable.
python -m tensorflow.python.debug.examples.debug_errors \
    --error uninitialized_variable --debug
</code></pre><p>问：如何让我的tfdbg包装的会话或挂钩运行调试模式 只从主线程？</p>
<p>A： 这是一个常见的用例，其中<code>thread_name_filter</code>对象是从多个使用的 线程并发。通常，子线程负责后台任务<br>如运行入队操作。通常情况下，你只想调试主 线程（或不太频繁，只有一个子线程）。你可以使用<br><code>LocalCLIDebugWrapperSession</code>的<code>Session</code>关键字参数 实现这种类型的线程选择性调试。例如，要从中调试<br>主线程，构建包装<code>MainThread</code>如下：</p>
<pre><code>sess = tf_debug.LocalCLIDebugWrapperSession(sess, thread_name_filter=&quot;MainThread$&quot;)
</code></pre><p>上面的例子依赖于Python中的主线程有这个事实 默认名称<code>tf.while_loop</code>。</p>
<p>问：我正在调试的模型非常大。数据由tfdbg转储 填补我的磁盘的可用空间。我能做什么？</p>
<p>A： 在以下任何情况下，您可能会遇到此问题：</p>
<p>具有许多中间张量的模型 非常大的中间张量 许多<code>LocalCLIDebugWrapperSession</code>迭代</p>
<p>有三种可能的解决方法或解决方案：</p>
<p><code>LocalCLIDebugHook</code>和<code>dump_root</code>的制造商    提供关键字参数<code>.*hidden.*</code>来指定路径<br>tfdbg将转储调试数据。你可以用它来让tfdbg转储    调试具有较大可用空间的磁盘上的数据。例如：</p>
<p>```python    ＃对于LocalCLIDebugWrapperSession    sess =<br>tf_debug.LocalCLIDebugWrapperSession（dump_root =“/ with / lots / of / space”）</p>
<p>＃对于LocalCLIDebugHook    hooks = [tf_debug.LocalCLIDebugHook（dump_root =“/ with<br>/ lots / of / space”）]    <code>`    确保dump_root指向的目录是空的或不存在的。    tfdbg在退出之前清理转储目录。
*减少运行过程中使用的批量。 *使用tfdbg&#39;srun</code>命令的过滤选项只能观察特定的    图中的节点。例如：</p>
<p>tfdbg&gt; run –node_name_filter。<em> hidden。</em>    tfdbg&gt;运行–op_type_filter变量。<em><br>tfdbg&gt; run –tensor_dtype_filter int。</em></p>
<p>上面的第一个命令只监视名字匹配的节点    正则表达模式<code>Variable.*</code>。第二个命令只监视    名称与<code>int.*</code>型号匹配的操作。第三个手表<br>只有与<code>int32</code>型（例如<code>mouse off</code>）匹配的张量。</p>
<p>问：为什么我不能在tfdbg CLI中选择文本？</p>
<p>答：这是因为tfdbg CLI通过在终端中启用鼠标事件        默认。这个鼠标掩码模式        覆盖默认的终端交互，包括文本选择。您<br>可以使用命令<code>m off</code>或者重新启用文本选择        <code>a</code>。</p>
<p>问：为什么在调试下面的代码时，tfdbg CLI显示没有转储张量？</p>
<pre><code>a = tf.ones([10], name=&quot;a&quot;)
b = tf.add(a, a, name=&quot;b&quot;)
sess = tf.Session()
sess = tf_debug.LocalCLIDebugWrapperSession(sess)
sess.run(b)
</code></pre><p>- 答：你看到没有数据倾倒的原因是因为在每个节点        执行的TensorFlow图形由TensorFlow运行时间不断折叠。<br>在这个层面上，<code>b</code>是一个常数张量;因此，取而代之        张量<code>a</code>也是一个常量张量。 TensorFlow的图表<br>优化将包含<code>b</code>和<code>tfdbg</code>的图形折叠成一个        以加速图形的未来运行，这也是<code>a</code>的原因<br>不会产生任何中间张量转储。但是，如果<code>tf.Variable</code>是一个        <code>tfdbg</code>，如下例所示：</p>
<pre><code>import numpy as np

a = tf.Variable(np.ones[10], name=&quot;a&quot;)
b = tf.add(a, a, name=&quot;b&quot;)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess = tf_debug.LocalCLIDebugWrapperSession(sess)
sess.run(b)
</code></pre><p>CXJ743-HDK-53L将不会出现恒定折叠的情况 张量转储。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





  


  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/page/123/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/122/">122</a><a class="page-number" href="/page/123/">123</a><span class="page-number current">124</span><a class="page-number" href="/page/125/">125</a><a class="page-number" href="/page/126/">126</a><span class="space">&hellip;</span><a class="page-number" href="/page/157/">157</a><a class="extend next" rel="next" href="/page/125/">Next<span></span></a>
  </nav>

</div>

      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- side-bar-ad -->
<ins class="adsbygoogle"
     style="display:block; overflow:hidden;"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="2232545787"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


  


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/javascript/" title="javascript">javascript<sup>207</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>205</sup></a></li>
			
		
			
				<li><a href="/tags/html/" title="html">html<sup>203</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>199</sup></a></li>
			
		
			
				<li><a href="/tags/bash/" title="bash">bash<sup>198</sup></a></li>
			
		
			
				<li><a href="/tags/php/" title="php">php<sup>197</sup></a></li>
			
		
			
				<li><a href="/tags/css/" title="css">css<sup>88</sup></a></li>
			
		
			
				<li><a href="/tags/shell/" title="shell">shell<sup>78</sup></a></li>
			
		
			
				<li><a href="/tags/jquery/" title="jquery">jquery<sup>61</sup></a></li>
			
		
			
				<li><a href="/tags/linux/" title="linux">linux<sup>57</sup></a></li>
			
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>41</sup></a></li>
			
		
			
				<li><a href="/tags/unix/" title="unix">unix<sup>30</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/html5/" title="html5">html5<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/xml/" title="xml">xml<sup>13</sup></a></li>
			
		
			
				<li><a href="/tags/http/" title="http">http<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/区块链/" title="区块链">区块链<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://tracholar.github.io" target="_blank" title="个人博客">个人博客</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>

    </div>
    <footer><div id="footer" >
	
	
	<section class="info">
		<p> To be or not to be, that is a question. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		版权所有 © 2018 本站文章未经同意，禁止转载！作者：
		
		<a href="/about" target="_blank" title="zhizi">zhizi</a>
		


		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>












<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
